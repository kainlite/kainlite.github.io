{"categories":[{"title":"continuous-integration","uri":"https://techsquad.rocks/categories/continuous-integration/"},{"title":"deployment-tools","uri":"https://techsquad.rocks/categories/deployment-tools/"},{"title":"kubernetes","uri":"https://techsquad.rocks/categories/kubernetes/"},{"title":"linux","uri":"https://techsquad.rocks/categories/linux/"},{"title":"serverless","uri":"https://techsquad.rocks/categories/serverless/"},{"title":"service-mesh","uri":"https://techsquad.rocks/categories/service-mesh/"},{"title":"terraform","uri":"https://techsquad.rocks/categories/terraform/"}],"posts":[{"content":"Introduction In this article we will continue where we left off the forward project last time, in this article we will use gitlab-ci to test, build and push the image of our operator to dockerhub.\nGitlab offers a pretty complete solution, but we will only sync our repo from github and set a basic pipeline to test, build and push our docker image to the registry, note that I do not have any kind of affiliation with gitlab, but I like their platform. Also this article demonstrates that you can use github and gitlab in a straight forward manner using the free tier in both sides, we rely in the free shared runners to make our custom CI system.\nIf you want to check the previous article go here, that way you will know what the project is all about.\nPrerequisites  A project in github in this case A gitlab.com account A dockerhub account  Create the project Once you have your accounts configured, let\u0026rsquo;s create a project, the page should look something like this   We want to create a repo or sync a repo in this case, so we select Create a project and continue\nProject type In this step we have a few options and since we have our code in Github and we want to work there, we only want to sync it, so we need to choose CI/CD for external repo   Note that if the repo is public you can fetch/clone using the repo URL, but since I want to check also private repos I went for the github token alternative. Once you hit github it will ask you for the token then it will show you the full list of repos in your account\nGithub Token I picked to use a personal token to fetch the repos to be able to grab private repos, etc, so you will need to go to your github account, Settings-\u0026gt;Developer settings and then create a new token or click here   Now you only need to give it access to repo, and hit save or create new personal token   Make sure you don\u0026rsquo;t expose or publish that token in any way, otherwise someone could gain access to your account\n(Back to gitlab) Select the repository to sync Here we need to select the repo that we want to sync and hit connect, it will automatically fetch everything periodically from github.   Dockerhub token Now we will need to create a token for dockerhub so we can push our image from the build runner, go to your dockerhub account and create a token   Basically you have to go to Account settings-\u0026gt;Security-\u0026gt;New Access Token or click here.\nThen we need to save that token as DOCKERHUB_TOKEN in this case as an environment variable in the gitlab project, Settings-\u0026gt;CI/CD-\u0026gt;Variables   make sure masked is marked but not protected, protected is only used when you want to use that secret in specific branches\nGitlab-CI config After that we only need to add the code to the repo and that will trigger a build, the file needs to be called .gitlab-ci.yml  basically we just install everything we need run the tests if everything goes well, then the build and push process. There is a lot of room for improvement in that initial config, but for now we only care in having some sort of CI system\nThen we will see something like this in the CI/CD-\u0026gt;Pipelines tab, after each commit it will trigger a test, build and push   Checking the results And we can validate that the images are in dockerhub   Useful links Some useful links:\n Variables and Predefined variables Using docker images Build docker images  Closing notes I hope you enjoyed it and hope to see you on twitter or github!\nErrata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":0,"section":"blog","summary":"Introduction In this article we will continue where we left off the forward project last time, in this article we will use gitlab-ci to test, build and push the image of our operator to dockerhub.\nGitlab offers a pretty complete solution, but we will only sync our repo from github and set a basic pipeline to test, build and push our docker image to the registry, note that I do not have any kind of affiliation with gitlab, but I like their platform.","tags":["go","golang","kubernetes","linux","docker","kubebuilder","gitlab","continuous-integration","continuous-delivery"],"title":"Gitlab-CI Basics","uri":"https://techsquad.rocks/blog/gitlab_ci_basics/","year":"2020"},{"content":"Introduction In this article we will see how to use cat, netcat and socat at least some basic examples and why do we have so many cats\u0026hellip;\nAlso sorry for the awful recordings, but couldn\u0026rsquo;t figure out why it looks so bad with tmux.\ncat Cat as you might have guessed or know already is to con-cat-enate things, when used in conjunction with the shell redirections it can do a lot of powerful things but it\u0026rsquo;s often used when it\u0026rsquo;s not needed due to that, let\u0026rsquo;s see some examples.\nSo what happened there? Basically when you want to end the file or the input you send the keyword Ctrl+D, when typed at the start of a line on a terminal, signifies the end of the input. This is not a signal in the unix sense: when an application is reading from the terminal and the user presses Ctrl+D, the application is notified that the end of the file has been reached (just like if it was reading from a file and had passed the last byte). This can be used also to terminate ssh sessions or just log you out from a terminal.\nIf you want to copy and paste something there you go:  While cat is overly simplified here, it can do a lot of interesting things and it is usually misused see here\nMore info:\n Cat examples Bash redirections Zsh redirections  netcat Netcat is a bit more interesting since it can use the network and it\u0026rsquo;s really simple also, so it let us use network sockets without too much complication, let\u0026rsquo;s see a couple of examples, first we spin up a server (listener), then connect from the other side and send some data, be aware that connections are bi-directional then Ctrl-C to finish the process. Then in the second example we spin up a server and wait for a compressed file to be sent from the client.\nThere are many more things that you can do with netcat and is usually really helpful to debug networking issues or to do a quick copy of files over the network.\nIf you want to copy and paste something there you go:  Netcat is pretty good at it\u0026rsquo;s job and it\u0026rsquo;s always a good tool to have at hand, but there are other more complex tasks with sockets and for that we have socat.\nMore info:\n Many uses for netcat (with a cheatsheet) Several examples  socat Socat is a command line based utility that establishes two bidirectional byte streams and transfers data between them. Because the streams can be constructed from a large set of different types of data sinks and sources (see address types), and because lots of address options may be applied to the streams, socat can be used for many different purposes. That bit was extracted from the man page, socat stands for SOcket CAT and it\u0026rsquo;s a multipurpose relay, we will see a few examples to clarify on what that means and some cool stuff that you can use socat for, at first it might look a bit intimidating, but trust me it worth learning to use it.\nSomething to have in mind when using socat it\u0026rsquo;s that it needs two addresses, sometimes you can skip them with a -. While socat has a gazillion more use cases than cat or netcat, I will just show you a few, but hand you a few links in case you are interested in learning more, what I find particularly useful it\u0026rsquo;s the ability to do a port-forward in just one line.\nBasically with socat your imagination is the limit in what you can do.\nIf you want to copy and paste something there you go:  More info:\n Socat Examples (great resource) More socat examples Linux unix TCP Port Forwarder  Closing notes Be sure to check the links if you want to learn more about each different tool and I hope you enjoyed it, see you on twitter or github!\nErrata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":1,"section":"blog","summary":"Introduction In this article we will see how to use cat, netcat and socat at least some basic examples and why do we have so many cats\u0026hellip;\nAlso sorry for the awful recordings, but couldn\u0026rsquo;t figure out why it looks so bad with tmux.\ncat Cat as you might have guessed or know already is to con-cat-enate things, when used in conjunction with the shell redirections it can do a lot of powerful things but it\u0026rsquo;s often used when it\u0026rsquo;s not needed due to that, let\u0026rsquo;s see some examples.","tags":["networking","linux"],"title":"Cat and friends (Netcat and Socat)","uri":"https://techsquad.rocks/blog/cat_and_friends_netcat_socat/","year":"2020"},{"content":"Introduction In this article we will see how to use kubebuilder and Kind to create a local test cluster and an operator, then deploy that operator in the cluster and test it, the repository with the files can be found here, also if you want to learn more about the idea and the project go: forward.\nBasically what the code does is create an alpine/socat pod and you can specify the host, port and protocol and it will make a tunnel for you, so then you can use port-forward or a service or ingress or whatever to expose things that are in another private subnet, while this might not sound like a good idea it has some use cases, so check your security constraints before doing any of that in a normal scenario it should be safe, it can be useful for testing or for reaching a DB while doing some debugging or test, but well, that is for another discussion, the tools used here is what makes this so interesting, this is a cloud native application, since it native to kubernetes and that\u0026rsquo;s what we will explore here.\nWhile Kind is not actually a requirement I used that for testing and really liked it, it\u0026rsquo;s faster and simpler than minikube.\nAlso if you are interested how I got the idea to make this operator check this github issue.\nPrerequisites  kubebuilder kustomize Go 1.13 Kind Docker  Create the project In this step we need to create the kubebuilder project, so in an empty folder we run:  Create the API Next let\u0026rsquo;s create an API, something for us to have control of (our controller).  Right until here we only have some boilerplate and basic or empty project with defaults, if you test it now it will work, but it won\u0026rsquo;t do anything interesting, but it covers a lot of ground and we should be grateful that such a tool exists.\nAdd our code to the mix First we will add it to api/v1beta1/map_types.go, which will add our fields to our type.  Basically we just edited the MapSpec and the MapStatus struct.\nNow we need to add the code to our controller in controllers/map_controller.go  In this controller we added two functions one to create a pod and modified basically the entire Reconcile function (this one takes care of checking the status and make the transitions in other words makes a controller work like a controller), also notice the kubebuilder annotations which will generate the rbac config for us, pretty handy! right?\nStarting the cluster Now we will use Kind to create a local cluster to test  it could be that easy!?!?! yes, it is!\nRunning our operator locally For testing you can run your operator locally like this:  Testing it First we spin up a pod, and launch nc -l -p 8000  Then we edit our manifest and apply it, check that everything is in place, and do the port-forward and launch another nc localhost 8000 to test if everything went well. First the manifest  Then the port-forward and test  Making it publicly ready Here we just build and push the docker image to dockerhub or our favorite public registry.  Then you can install it with make deploy IMG=kainlite/forward:0.0.1 and uninstall it with make uninstall\nClosing notes Be sure to check the kubebuilder book if you want to learn more and the kind docs, I hope you enjoyed it and hope to see you on twitter or github!\nErrata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":2,"section":"blog","summary":"Introduction In this article we will see how to use kubebuilder and Kind to create a local test cluster and an operator, then deploy that operator in the cluster and test it, the repository with the files can be found here, also if you want to learn more about the idea and the project go: forward.\nBasically what the code does is create an alpine/socat pod and you can specify the host, port and protocol and it will make a tunnel for you, so then you can use port-forward or a service or ingress or whatever to expose things that are in another private subnet, while this might not sound like a good idea it has some use cases, so check your security constraints before doing any of that in a normal scenario it should be safe, it can be useful for testing or for reaching a DB while doing some debugging or test, but well, that is for another discussion, the tools used here is what makes this so interesting, this is a cloud native application, since it native to kubernetes and that\u0026rsquo;s what we will explore here.","tags":["go","golang","kubernetes","linux","security","docker","kustomize","kubebuilder","kind"],"title":"Cloud native applications with kubebuilder and kind aka kubernetes operators","uri":"https://techsquad.rocks/blog/cloud_native_applications_with_kubebuilder_and_kind_aka_kubernetes_operators/","year":"2020"},{"content":"Introduction Easy method to report spam to SpamCop.net using GMail, this helps to reduce the true Spam from unknown sources, since for some reason I started to get like 40 emails per day (all went to spam), but it is still somewhat annoying, so I started reporting it to spamcop, this alternative method doesn\u0026rsquo;t need a script and it\u0026rsquo;s really easy to do as well, same result as with the script from the previous post.\nPre-requisites:\n GMail account Setup a spamcop account which you will be using to send your reports, you can do that here  Forwarding as attachment First of all you need to select all emails and then click on the three dots and select \u0026ldquo;Forward as attachment\u0026rdquo;   Sending it to your spamcop email In this step the only thing that you need to do is put your Spamcop email (it gives you this address to report spam when you create the account and in the report spam tab), you do not need to put anything in the body or the subject, just send it as is.   Confirming each one Then you will get an email with a link to each spam message to submit the report.   Sending the reports This is a sample report, you can add additional notes if needed and then confirm to send it to the abuse addresses of the owners of the IPs and links found in the email.   Additional notes This method is pretty easy for someone who doesn\u0026rsquo;t want to run a script or whatever and is still able to report the spam to the sources, however if you want something a bit less manual you can try with the script or just create a filter to delete everything in the spam folder.\nErrata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":3,"section":"blog","summary":"Introduction Easy method to report spam to SpamCop.net using GMail, this helps to reduce the true Spam from unknown sources, since for some reason I started to get like 40 emails per day (all went to spam), but it is still somewhat annoying, so I started reporting it to spamcop, this alternative method doesn\u0026rsquo;t need a script and it\u0026rsquo;s really easy to do as well, same result as with the script from the previous post.","tags":["linux"],"title":"How to report spam to spamcop from gmail","uri":"https://techsquad.rocks/blog/how_to_report_spam_to_spamcop_from_gmail/","year":"2020"},{"content":"Introduction This post is a bit different from the others in the sense that it\u0026rsquo;s a small \u0026ldquo;tool\u0026rdquo; I did to ease spam reporting to SpamCop.net, this helps to reduce the true Spam from unknown sources, since for some reason I started to get like 40 emails per day (all went to spam), but it is still somewhat annoying, so I started reporting it to spamcop, but the process was kind of slow and I got tired of that quickly, so I created this \u0026ldquo;script\u0026rdquo; to make things easier. Basically what it does is list all messages in the spam folders fetches them and then forwards each one as an attachment to spamcop, then you get an email with a link to confirm the submission and that\u0026rsquo;s it.\nThere are a few pre-requisites, like enabling the GMail API for your account, you can do that here, after that the first time you use the app you have to authorize it, you do this by pasting the URL that the app gives you in the browser, then clicking Allow, and then pasting the token that it gives you back in the terminal (this only needs to be done once), after that you just run the binary in a cronjob or maybe even as a lambda (but I haven\u0026rsquo;t gone there yet), I usually check the spam folder remove what I don\u0026rsquo;t think it\u0026rsquo;s spam or whatever and then run the script to report everything else that it is clearly spam, it takes a few seconds and then I get the link to confirm all reports (one by one, sadly), this script is not perfect as sometimes spamcop cannot read properly the forwarded email, but I have checked exporting those as a file and I do see them all right, so that will be an investigation for another day, this only took like 2-4 hours, having 0 knowledge of the GMail API, etc.\nAlso you need to setup a spamcop account which you will be using to send your reports, you can do that here\nThe source code can be found here\nCode I have added some comments along the code to make things easy to understand  Running it  Sources Some articles, pages, and files that I used and helped me to do what I wanted to do:\n https://developers.google.com/gmail/api/quickstart/go https://github.com/gsuitedevs/go-samples/blob/master/gmail/quickstart/quickstart.go https://socketloop.com/tutorials/golang-send-email-with-attachment-rfc2822-using-gmail-api-example https://raw.githubusercontent.com/googleapis/google-api-go-client/master/examples/gmail.go https://github.com/xDinomode/Go-Gmail-Api-Example/blob/master/email.go https://www.spamcop.net/reporter.pl https://godoc.org/google.golang.org/api/gmail/v1#Message  Additional notes While this still needs some work hopefully will keep my account clean and probably help someone wondering about how to do the same.\nErrata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":4,"section":"blog","summary":"Introduction This post is a bit different from the others in the sense that it\u0026rsquo;s a small \u0026ldquo;tool\u0026rdquo; I did to ease spam reporting to SpamCop.net, this helps to reduce the true Spam from unknown sources, since for some reason I started to get like 40 emails per day (all went to spam), but it is still somewhat annoying, so I started reporting it to spamcop, but the process was kind of slow and I got tired of that quickly, so I created this \u0026ldquo;script\u0026rdquo; to make things easier.","tags":["development","golang","go","linux"],"title":"How to report your gmail spam folder to spamcop","uri":"https://techsquad.rocks/blog/how_to_report_your_gmail_spam_folder_to_spamcop/","year":"2019"},{"content":"Introduction In this article we will see the basics to have tests for your terraform code using a re-usable pattern, we will use the code from the last article Serverless authentication with Cognito, so refer to that one before starting this one if you want to know how did we get here. Also as a side note this is a very basic example on how to get started with terratest.\nTerratest is a Go library that makes it easier to write automated tests for your infrastructure code, it supports Terraform, Docker, Packer, SSH, AWS, GCP, Kubernetes, Helm, and much more, also as it\u0026rsquo;s written as a Go library you have access to all the existing APIs.\nThe code There are comments all over the code to explain each part, but what I want to highlight here is the pattern being used with the module test-structure, this module allows us to split the test in sections and skip parts that we don\u0026rsquo;t need or want to run, so we have 3 stages here: cleanup, deploy, and validate, this lets you use SKIP_stage, for example SKIP_cleanup when you run your tests with go test -timeout 90m . (I added some extra bits, that I usually use, like timeout by default it\u0026rsquo;s 10 minutes I believe and it\u0026rsquo;s often too short), to only run validate and cleanup, it can be useful while developing a module to test without having to wait for everything to be re-created. package test import ( \u0026#34;crypto/tls\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;testing\u0026#34; \u0026#34;time\u0026#34; http_helper \u0026#34;github.com/gruntwork-io/terratest/modules/http-helper\u0026#34; \u0026#34;github.com/gruntwork-io/terratest/modules/retry\u0026#34; \u0026#34;github.com/gruntwork-io/terratest/modules/terraform\u0026#34; test_structure \u0026#34;github.com/gruntwork-io/terratest/modules/test-structure\u0026#34; ) // Main function, define stages and run. func TestTerraformAws(t *testing.T) { t.Parallel() // Pick a random AWS region to test in. This helps ensure your code works in all regions.  // awsRegion := aws.GetRandomStableRegion(t, nil, nil)  awsRegion := \u0026#34;us-east-1\u0026#34; workingDir := \u0026#34;../terraform\u0026#34; // At the end of the test, undeploy the web app using Terraform  defer test_structure.RunTestStage(t, \u0026#34;cleanup\u0026#34;, func() { destroyTerraform(t, workingDir) }) // Deploy the web app using Terraform  test_structure.RunTestStage(t, \u0026#34;deploy\u0026#34;, func() { deployTerraform(t, awsRegion, workingDir) }) // Validate that the ASG deployed and is responding to HTTP requests  test_structure.RunTestStage(t, \u0026#34;validate\u0026#34;, func() { validateAPIGateway(t, workingDir) }) } // Validate that the API Gateway has been deployed and is working func validateAPIGateway(t *testing.T, workingDir string) { // Load the Terraform Options saved by the earlier deploy_terraform stage  terraformOptions := test_structure.LoadTerraformOptions(t, workingDir) // Run `terraform output` to get the value of an output variable  url := terraform.Output(t, terraformOptions, \u0026#34;URL\u0026#34;) // It can take a few minutes for the API GW and CloudFront to finish spinning up, so retry a few times  // maxRetries := 30  timeBetweenRetries := 15 * time.Second // Setup a TLS configuration to submit with the helper, a blank struct is acceptable  tlsConfig := tls.Config{} // Verify that the API Gateway returns a proper response  apigw := retry.DoInBackgroundUntilStopped(t, fmt.Sprintf(\u0026#34;Check URL %s\u0026#34;, url), timeBetweenRetries, func() { http_helper.HttpGetWithCustomValidation(t, fmt.Sprintf(\u0026#34;%s/app/health\u0026#34;, url), \u0026amp;tlsConfig, func(statusCode int, body string) bool { return statusCode == 200 }) }) // Stop checking the API Gateway  apigw.Done() } // Deploy the resources using Terraform func deployTerraform(t *testing.T, awsRegion string, workingDir string) { terraformOptions := \u0026amp;terraform.Options{ // The path to where our Terraform code is located  TerraformDir: workingDir, } // Save the Terraform Options struct, instance name, and instance text so future test stages can use it  test_structure.SaveTerraformOptions(t, workingDir, terraformOptions) // This will run `terraform init` and `terraform apply` and fail the test if there are any errors  terraform.InitAndApply(t, terraformOptions) } // Destroy the resources using Terraform func destroyTerraform(t *testing.T, workingDir string) { // Load the Terraform Options saved by the earlier deploy_terraform stage  terraformOptions := test_structure.LoadTerraformOptions(t, workingDir) terraform.Destroy(t, terraformOptions) } Some high level notes on each stage:\ndeploy: This stage will take care of running init and then apply.\nvalidate: This stage will take care of running a probe to check if our API is up and if the return code is 200.\ncleanup: This stage will take care of running destroy and cleaning up everything.\nDep Currently terratest uses dep, so you will need this file Gopkg.toml and dep installed to be able to install the dependencies with dep ensure -v. [[constraint]] name = \u0026#34;github.com/gruntwork-io/terratest\u0026#34; version = \u0026#34;0.18.6\u0026#34;\nDockerfile Also you can use this small dockerfile that does all that for you, in this example using the code from the previously mentioned article. FROM golang:alpine MAINTAINER \u0026#34;kainlite \u0026lt;kainlite@gmail.com\u0026gt;\u0026#34; ARG TERRAFORM_VERSION=0.12.8 ENV TERRAFORM_VERSION=$TERRAFORM_VERSION RUN apk --no-cache add curl git unzip gcc g++ make ca-certificates \u0026amp;\u0026amp; \\  curl https://raw.githubusercontent.com/golang/dep/master/install.sh | sh RUN mkdir tmp \u0026amp;\u0026amp; \\  curl \u0026#34;https://releases.hashicorp.com/terraform/${TERRAFORM_VERSION}/terraform_${TERRAFORM_VERSION}_linux_amd64.zip\u0026#34; -o tmp/terraform.zip \u0026amp;\u0026amp; \\  unzip tmp/terraform.zip -d /usr/local/bin \u0026amp;\u0026amp; \\  rm -rf tmp/ ARG GOPROJECTPATH=/go/src/github.com/kainlite/serverless-cognito COPY ./ $GOPROJECTPATH WORKDIR $GOPROJECTPATH/test RUN dep ensure -v CMD [\u0026#34;go\u0026#34;, \u0026#34;test\u0026#34;, \u0026#34; -timeout\u0026#34;, \u0026#34;90m\u0026#34;, \u0026#34;.\u0026#34;]\nManually testing it First we check that the URL actually works, and that everything is in place. $ curl https://api.skynetng.pw/app/health # OUTPUT: # {\u0026#34;status\u0026#34;:\u0026#34;healthy\u0026#34;}\nNext we can test it using our validate stage, using terratest: $ SKIP_deploy=true SKIP_cleanup=true go test -timeout 90m . # OUTPUT: # ok github.com/kainlite/test 1.117s This works because in the terraform code we have an output called URL which is https://api.skynetng.pw, then we add at the end /app/health and check if it return a 200 code, otherwise we wait and retry until it does or times out.\nClosing notes And that\u0026rsquo;s all for now, in the next piece I will cover how to automate this deployment using a CI/CD tool, so you can have truly repeatable infrastructure, which can be of big importance when working on various modules, versions and deployments.\nErrata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":5,"section":"blog","summary":"Introduction In this article we will see the basics to have tests for your terraform code using a re-usable pattern, we will use the code from the last article Serverless authentication with Cognito, so refer to that one before starting this one if you want to know how did we get here. Also as a side note this is a very basic example on how to get started with terratest.","tags":["go","golang","development","linux","terraform"],"title":"Brief introduction to terratest","uri":"https://techsquad.rocks/blog/brief_introduction_to_terratest/","year":"2019"},{"content":"Introduction In this article we will see how to use Terraform and Go to create a serverless API using API Gateway, Lambda, and Go, and we will also handle authentication with AWS Cognito, the repo with the files can be found here.\nTerraform In this example I used terraform 0.12, and I kind of liked the new changes, it feels more like coding and a more natural way to describe things, however I think there are more bugs than usual in this version, but I really like the new output for the plan, apply, etc, getting back to the article since there is a lot of code I will gradually update the post with more notes and content or maybe another post explaining another section, but the initial version will only show the cognito part and the code to make it work and how to test it.\nCognito  As we can see it\u0026rsquo;s really simple to have a cognito user pool working, the most important part here is the auto_verified_attributes because that is what makes cognito to actually send an email or an sms with the confirmation code, the rest is self-describing, it creates a pool and a client, since what we need to be able to interact with out pool is the client that part is of considerable importance even that we have most things with default values. As you might have noticed we defined two explicit_auth_flows and that is to be able to interact with this user pool using user and password.\nACM Next let\u0026rsquo;s see how we manage the certificate creation using ACM.  Here basically we create the certificate using aws_acm_certificate and validate it automatically using the DNS method and the resource aws_acm_certificate_validation, the other resources in the file are just there because they are kind of associated but not necessarily need to be there.\nRoute53 Here we just create an alias record for the API Gateway and the validation record.  API Gateway While this file might seem relatively simple, the API Gateway has many features and can get really complex really fast, basically what we are doing here is creating an API with a resource that accepts all method types and proxy that as it is to our lambda function.  Lambda This file has the lambda function definition, the policy and the roles needed, basically the policy is to be able to log to CloudWatch and to inspect with X-Ray, then the log group to store the logs will set the retention period by default 7 days.  Variables and locals First the variables file with the default values  And last the locals file, in this small snippet we are just making a map with a computed value and the values that can come from a variable which can be quite useful in many scenarios where you don\u0026rsquo;t know all the information in advance or something is dynamically assigned:  Deployment scripts There is a small bash script to make it easier to run the deployment, AKA as compiling the code, zipping it, and running terraform to update our function or whatever we changed.  Go The good thing is that everything is code, but we don\u0026rsquo;t have to manage any server, we just consume services from AWS completely from code, isn\u0026rsquo;t that amazing?, I apologize for the length of the file, but you will notice that it\u0026rsquo;s very repetitive, in most functions we load the AWS configuration, we make a request and return a response, we\u0026rsquo;re also using Gin as a router, which is pretty straight-forward and easy to use, we have only one authenticated path (/user/profile), and we also have another unauthenticated path which is a health check (/app/health), the other two paths (/user and /user/validate) are exclusively for the user creation process with cognito.  All logs go to CloudWatch and you can also use X-Ray to diagnose issues.\nTesting it So we\u0026rsquo;re going to hit the API to create, validate, and query the empty profile of the user from the terminal using curl.  I have added most info in as comments in the snippet, note that I also used my test domain skynetng.pw with the subdomain api for all tests.\nClosing notes This post was heavily inspired by this post from Alexander, kudos to him for the great work!, this post expands on that and adds the certificate with ACM, it also handles a basic AWS Cognito configuration and the necessary go code to make it work, there are other ways to accomplish the same, but what I like about this approach is that you can have some endpoints or paths without authentication and you can use authentication, etc on-demand. This article is a bit different but I will try to re-shape it in the following weeks, and also cover more of the content displayed here, let me know if you have any comments or suggestions!\nIn some near future I will build upon this article in another article adding a few cool things, for example to allow an user to upload an image to an S3 bucket and fetch that with a friendly name using Cloudfront (In a secure manner, and only able to upload/update his/her profile picture, while being able to fetch anyone profile pic), the idea is to have a fully functional small API using AWS services and serverless facilities with common tasks that you can find in any functional website.\nErrata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":6,"section":"blog","summary":"Introduction In this article we will see how to use Terraform and Go to create a serverless API using API Gateway, Lambda, and Go, and we will also handle authentication with AWS Cognito, the repo with the files can be found here.\nTerraform In this example I used terraform 0.12, and I kind of liked the new changes, it feels more like coding and a more natural way to describe things, however I think there are more bugs than usual in this version, but I really like the new output for the plan, apply, etc, getting back to the article since there is a lot of code I will gradually update the post with more notes and content or maybe another post explaining another section, but the initial version will only show the cognito part and the code to make it work and how to test it.","tags":["go","golang","serverless","aws","security","cognito","lambda"],"title":"Serverless authentication with Cognito and Go","uri":"https://techsquad.rocks/blog/serverless_authentication_with_cognito_and_golang/","year":"2019"},{"content":"Introduction In this article we will see a subtle introduction to terraform modules, how to pass data into the module, get something from the module and create a resource (GKE cluster), it\u0026rsquo;s intended to be as simple as possible just to be aware of what a module is composed of, or how can you do your own modules, sometimes it makes sense to have modules to abstract implementations that you use over several projects, or things that are often repeated along the project. So let\u0026rsquo;s see what it takes to create and use a module. The source code for this article can be found here. Note that in this example I\u0026rsquo;m using GCP since they give you $300 USD for a year to try their services and it looks pretty good so far, after sign-up you will need to go to IAM, then create a service account and after that export the the key (this is required for the terraform provider to talk to GCP).\nComposition of a module A module can be any folder with a main.tf file in it, yes, that is the only required file for a module to be usable, but the recommendation is that you also put a README.md file with a description of the module if it\u0026rsquo;s intended to be used by people if it\u0026rsquo;s a sub-module it\u0026rsquo;s not necessary, also you will need a file called variables.tf and other outputs.tf of course if it\u0026rsquo;s a big module that cannot be splitted into sub-modules you can split those files for convenience or readability, variables should have descriptions so the tooling can show you what are they for, you can read more about the basics for a module here.\nBefore moving on let\u0026rsquo;s see the folder structure of our project:  Okay enough talking, show me the code The project Let\u0026rsquo;s start with the main.tf that will call our module, notice that I added a few additional comments but it\u0026rsquo;s pretty much straight forward, we set the provider, then we define some variables, call our module and print some output (output can also be used to pass data between modules).  Then terraform.tfvars has some values to override the defaults that we defined:  The module Now into the module itself, this module will create a GKE cluster, and while it\u0026rsquo;s not a good practice to have a module as a wrapper but for this example we will forget about that rule for a while, this is the main.tf file:  The variables.tf file:  And finally the outputs.tf file:  Notice that we have a lot more outputs than the one we decided to print out, but you can play with that and experiment if you want :)\nTesting it First we need to initialize our project so terraform can put modules, provider files, etc in place, it\u0026rsquo;s a good practice to version things and to move between versions that way everything can be tested and if something is not working as expected you can always rollback to the previous state.  Then we will just run it.  If we check the output we will see that the name of the cluster matches the one from our variables and at the end we can see the output that the module produced.\nClosing notes As you can see, creating a module is pretty simple and with good planing and practice it can save you a lot of effort along big projects or while working on multiple projects, let me know your thoughts about it. Always remember to destroy the resources that you\u0026rsquo;re not going to use with terraform destroy.\nErrata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":7,"section":"blog","summary":"Introduction In this article we will see a subtle introduction to terraform modules, how to pass data into the module, get something from the module and create a resource (GKE cluster), it\u0026rsquo;s intended to be as simple as possible just to be aware of what a module is composed of, or how can you do your own modules, sometimes it makes sense to have modules to abstract implementations that you use over several projects, or things that are often repeated along the project.","tags":["terraform","kubernetes","gcp"],"title":"Getting started with terraform modules","uri":"https://techsquad.rocks/blog/getting_started_with_terraform_modules/","year":"2019"},{"content":"Introduction In this article we will explore gRPC with a cheap ping application, basically we will do a ping and measure the time it takes for the message to go to the server and back before reporting it to the terminal. You can find the source code here.\nProtobuf As you might already know gRPC serializes data using protocol buffers, We are just going to create a Unary RPC as follows.  With this file in place we are defining a service that will be able to send a single PingRequest and get a single PingResponse, we have a Data field that goes back and forth in order to send some bytes over the wire (even that we don\u0026rsquo;t really care about that, it could be important or crucial in a performance test).\nGenerating the code In order to be able to use protobuf we need to generate the code for the app that we\u0026rsquo;re writing in this case for golang the command would be this one:  This will give us a definition of the service and the required structs to carry the data that we have defined as messages.\nClient The client does most of the work here, as you can see you can supply 2 arguments one to point to another host:port and the second to send a string of your liking, then it measures the time it takes to send and receive the message back and prints it to the screen with a similar line to what the actual ping command looks in linux.  Server The server is a merely echo server since it will send back whatever you send to it and log it to the console, by default it will listen in port 50000.  Testing it Regular ping  Client This is what we would see in the terminal while testing it.  As you can see the initial connection takes a bit more time but after that the roundtrip time is very consistent (of course our cheap ping doesn\u0026rsquo;t cover errors, packet loss, etc).\nServer The server just echoes back and logs what received over the wire.  Closing notes As you can see gRPC is pretty fast and simplifies a lot everything that you need to do in order to have a highly efficient message system or communication between microservices for example, it\u0026rsquo;s also easy to generate the boilerplate for whatever language you prefer and have a common interface that everyone has to agree on.\nErrata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":8,"section":"blog","summary":"Introduction In this article we will explore gRPC with a cheap ping application, basically we will do a ping and measure the time it takes for the message to go to the server and back before reporting it to the terminal. You can find the source code here.\nProtobuf As you might already know gRPC serializes data using protocol buffers, We are just going to create a Unary RPC as follows.","tags":["go","grpc"],"title":"Go gRPC Cheap Ping","uri":"https://techsquad.rocks/blog/go_grpc_cheap_ping/","year":"2019"},{"content":"Introduction In this article we will continue where we left off the last time Go continuous integration with Travis CI and Docker, the files used here can be found HERE, and we will be creating our terraform cluster with a load balancer and generating our kubeconfig file based on the certs provided by terraform on travis and then finally creating a basic deployment and validate that everything works.\nDigitalOcean We need to create a token so terraform can create resources using DO API. Go to your account then in the menu on the left click API, then you should see something like this:   Once there click generate token (give it a meaningful name to you), and make sure it can write.   Terraform As the next step it would be good to set the token for terraform, so let\u0026rsquo;s examine all files and see what they are going to do, but first we\u0026rsquo;re going to provide the secrets to our app via environment variables, and I\u0026rsquo;ve found quite useful to use direnv on many projects, so the content of the first file .envrc would look something like:  and after that you will need to allow it\u0026rsquo;s execution by running direnv allow.\nThe first terraform file that we are going to check is provider.tf:  As we\u0026rsquo;re using environment variables we need to declare it and then set it in the provider, for now we only need the token.\nThen the kubernetes.tf file:  This file will be the responsible of creating the kubernetes cluster, as it\u0026rsquo;s our development cluster we only need one node.\nNext the file lb.tf:  This one is particularly interesting because it will provide a point of access to our applications (port 80 on it\u0026rsquo;s public IP address), and it also uses a basic health check.\nAnd last but not least the output.tf file:  This file will print the kubernetes config file that we need to be able to use kubectl, and also the IP address of our load balancer.\nSo what do we do with all of this?, first you will need to run terraform init inside the terraform folder to download plugins and providers, once that is done you can run terraform plan to see what changes terraform wants to make or terraform apply to do the changes. How is that going to look?:  This will create our cluster in DigitalOcean, remember to destroy it after you\u0026rsquo;re done using it with terraform destroy, if you don\u0026rsquo;t use a plan you will be prompted for a confirmation when you do terraform apply, review and say yes.\nTravis We did some additions to our .travis.yml file, which are mostly to prepare kubectl and to also trigger a deployment if the build succeeded.  As shown in the screenshot we took the base64 encoded certificates and loaded them into travis as environment variables (KUBERNETES_CA, KUBERNETES_CLIENT_CA, KUBERNETES_CLIENT_KEY, KUBERNETES_ENDPOINT), then we decode that into files, create the configuration using kubectl and set it as active and then we apply the deployment with the newly rendered hash.\nThis is how it should look in travis:   Let\u0026rsquo;s take a look at the generated kubernetes configuration and what values you should take into account:  Never do that, don\u0026rsquo;t share your configuration or anybody will be able to use your cluster, also be careful not to commit it to your repo, in this example it\u0026rsquo;s no longer valid because after running the examples I destroyed the cluster with terraform destroy. Now there are four values of interest for us: certificate-authority-data: KUBERNETES_CA, client-certificate-data: KUBERNETES_CLIENT_CA, client-key-data: KUBERNETES_CLIENT_KEY and server: KUBERNETES_ENDPOINT, with these variables we can re-create our kubernetes configuration easily using kubectl, be aware that we\u0026rsquo;re not decoding to save it in travis, we do that in the travis configuration file (.travis.yml).\nKubernetes So after all that, we still need to have a deployment template to deploy our application, and it\u0026rsquo;s a template because we need to replace the SHA of the current build in the manifest before committing it to the Kubernetes API, so let\u0026rsquo;s check it manifest.yml.template:  Here we expose our service in the port 30000 as a NodePort, and deploy the current SHA (replaced during execution by travis)\nTesting everything Validate that the deployment went well by checking our kubernetes cluster:  First we test the load balancer, and as we will see the ip is not right, it\u0026rsquo;s the internal ip of the load balancer and not our public ip address.  But if we hit our service directly we can see the correct IP address, this could be improved but it\u0026rsquo;s left as an exercise for the avid reader ◕_◕.  Finally let\u0026rsquo;s check what we should see in travis:   As we can see everything went well and our deployment applied successfully in our cluster   Closing notes I will be posting some articles about CI and CD and good practices that DevOps/SREs should have in mind, tips, tricks, and full deployment examples, this is the second part of a possible series of three articles (Next one should be about the same but using Jenkins) with a complete but basic example of CI first and then CD. This can of course change and any feedback would be greatly appreciated :).\nIn this example many things could be improved, for example we use a node port and there is no firewall so we can hit our app directly via nodeport or using the load balancer, we should add some firewall rules so only the load balancer is able to talk to the node port range (30000-32767).\nAlso be aware that for production this setup will not be sufficient but for a development environment would suffice initially.\nSome useful links for travis and terraform.\nErrata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":9,"section":"blog","summary":"Introduction In this article we will continue where we left off the last time Go continuous integration with Travis CI and Docker, the files used here can be found HERE, and we will be creating our terraform cluster with a load balancer and generating our kubeconfig file based on the certs provided by terraform on travis and then finally creating a basic deployment and validate that everything works.\nDigitalOcean We need to create a token so terraform can create resources using DO API.","tags":["travis-ci","docker","golang","go","linux","continuous-integration","continuous-delivery","terraform"],"title":"Go continuous delivery with Terraform and Kubernetes","uri":"https://techsquad.rocks/blog/go_continuous_delivery_with_terraform_and_kubernetes/","year":"2019"},{"content":"Introduction In this article we will see how to create a simple continuous integration process using Github, Travis-CI and Docker HUB, the files used here can be found HERE, in the next article we will continue with what we have here to provide continuous deployment possibly using Jenkins or maybe Travis, let me know which one you would prefer to see.\nFirst thing first App We will review the docker file, the app code and the travis-ci file, so let\u0026rsquo;s start with the app main.go:  Let\u0026rsquo;s quickly check what this code does, first we check for the port to use, then convert it to a number, register the handler for our HTTP function and listen for requests, this code should print our ip address as you would expect by the name.\nThen the main_test.go code:  The test is fairly simple it just checks that the web server works by trying to fetch / and checking for an empty body and 200 status code.\nDocker Next the Dockerfile:  We set the working directory to please go, then fetch dependencies and install our binary, we also generate a test binary, expose the port that we want to use and set the user as nobody in case someone can exploit our app and jump into our container, then just set the command to execute on docker run.\nTravis And last but not least the .travis.yml file:  We let travis know that we will be running some go code and also docker, then build the image, run the tests and then the app as initialization, after that we validate that the app works and lastly login to dockerhub and push the image, the important things to have in mind here is that we use variables for example the repo name, the commit SHA, and the docker username and password in a secure way, since travis-ci hides the values that we tell them to.\nPutting everything together So far we got the repo going, the configuration for travis, the dockerfile, the app, but now we need to make use of it, so you will need to create a travis account for this to work then link your github account to it, then you will be able to sync your repositories and you should see something like this:   Once you have your account linked you will be able to sync and enable repositories to be built.\nAfter enabling the repository you can configure some details like environment variables, here we will set the credentials for dockerhub.   And now we will create the repository in dockerhub:   After the repository is created we can trigger a build from travis or push a commit to the repo in order to trigger a build and to validate that everything works.\nYou should see something like this in travis if everything went well:   You can validate that everything went well by checking the commit SHA that triggered the build.\nAnd dockerhub:   The same SHA will be used to tag the image.\nClosing notes I will be posting some articles about CI and CD and good practices that DevOps/SREs should have in mind, tips, tricks, and full deployment examples, this is the first part of a possible series of two or three articles with a complete but basic example of CI first and then CD. This can of course change and any feedback would be greatly appreciated :).\nSome useful links for travis and docker and the environment variables list that can be used.\nErrata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":10,"section":"blog","summary":"Introduction In this article we will see how to create a simple continuous integration process using Github, Travis-CI and Docker HUB, the files used here can be found HERE, in the next article we will continue with what we have here to provide continuous deployment possibly using Jenkins or maybe Travis, let me know which one you would prefer to see.\nFirst thing first App We will review the docker file, the app code and the travis-ci file, so let\u0026rsquo;s start with the app main.","tags":["travis-ci","docker","golang","linux","continuous-integration"],"title":"Go continuous integration with Travis CI and Docker","uri":"https://techsquad.rocks/blog/go_continuous_integration_with_travis_ci_and_docker/","year":"2019"},{"content":"Introduction In the previous article we configured Vault with Consul on our cluster, now it\u0026rsquo;s time to go ahead and use it to provision secrets to our pods/applications. If you don\u0026rsquo;t remember about it or don\u0026rsquo;t have your Vault already configured you can go to Getting started with HashiCorp Vault on Kubernetes.\nIn this article we will actually create an example using mutual TLS and provision some secrets to our app, You can find the files used here in this repo.\nCreating a cert for our new client As we see here we need to enable kv version 1 on /secret for this to work, then we just create a secret and store it as a kubernetes secret for myapp, note that the CA was created in the previous article and we rely on these certificates so we can keep building on that.  Service account for kubernetes In Kubernetes, a service account provides an identity for processes that run in a Pod so that the processes can contact the API server.  Vault policy Then we need to set a read-only policy for our secrets, we don\u0026rsquo;t want or app to be able to write or rewrite secrets.  Kubernetes configuration Set the environment variables to point to the running Minikube environment and enable the kubernetes authentication method and then validate it from a temporal Pod.  The deployment and the consul-template configuration If you check the volume mounts and the secrets we load the certificates we created initially and use them to fetch the secret from vault  This is where the magic happens so we\u0026rsquo;re able to fetch secrets (thanks to that role and the token that then will be stored there)  And last but not least we create a file based in the template provided which our nginx container will render on the screen later, this is done using Consul Template.  Test it! The last step would be to test all that, so after having deployed the files to kubernetes we should see something like this  Closing notes This post was heavily inspired by this doc page, the main difference is that we have mutual TLS on, the only thing left would be to auto unseal our Vault, but we will left that for a future article or as an exercise for the reader.\nErrata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":11,"section":"blog","summary":"Introduction In the previous article we configured Vault with Consul on our cluster, now it\u0026rsquo;s time to go ahead and use it to provision secrets to our pods/applications. If you don\u0026rsquo;t remember about it or don\u0026rsquo;t have your Vault already configured you can go to Getting started with HashiCorp Vault on Kubernetes.\nIn this article we will actually create an example using mutual TLS and provision some secrets to our app, You can find the files used here in this repo.","tags":["kubernetes","vault","linux","security"],"title":"Actually using Vault on Kubernetes","uri":"https://techsquad.rocks/blog/actually_using_vault_on_kubernetes/","year":"2019"},{"content":"Introduction Vault secures, stores, and tightly controls access to tokens, passwords, certificates, API keys, and other secrets in modern computing. What this means is that you can safely store all your App secrets in Vault without having to worry anymore how to store, provide, and use those secrets, we will see how to install it on a running kubernetes cluster and save and read a secret by our application, in this page we will be using Vault version 1.1.1, we will be using dynamic secrets, that means that each pod will have a different secret and that secret will expire once the pod is killed.\nBefore you start you will need Consul, Vault client binaries and Minikube or any running cluster, you can find the files used here in this repo.\nThis is the part one of two\nPreparing the cluster Let\u0026rsquo;s start minikube and validate that we can reach our cluster with minikube start and then with kubectl get nodes, also the dashboard can become handy you can invoke it like this minikube dashboard  Creating certificates for Consul and Vault Vault needs a backend to store data, this backend can be consul, etcd, postgres, and many more, so the first thing that we are going to do is create a certificate so consul and vault can speak to each other securely.  Consul The next steps would be to create an encryption key for the consul cluster and to create all the kubernetes resources associated with it  Vault Once we have Consul running starting vault should be straight forward, we need to create all kubernetes resources associated with it and then initialize and unseal the vault.  Closing notes As you can see it takes a while to configure a Vault server but I really like the pattern that renders for the apps using it, in the next post we will see how to unlock it automatically with kubernetes and also how to mount the secrets automatically to our pods so our applications can use it :), this post was heavily inspired by this one and this one.\nErrata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":12,"section":"blog","summary":"Introduction Vault secures, stores, and tightly controls access to tokens, passwords, certificates, API keys, and other secrets in modern computing. What this means is that you can safely store all your App secrets in Vault without having to worry anymore how to store, provide, and use those secrets, we will see how to install it on a running kubernetes cluster and save and read a secret by our application, in this page we will be using Vault version 1.","tags":["kubernetes","vault","linux","security"],"title":"Getting started with HashiCorp Vault on Kubernetes","uri":"https://techsquad.rocks/blog/getting_started_with_hashicorp_vault_on_kubernetes/","year":"2019"},{"content":"Introduction Here we will see how to use terraform to manage lambda functions, it will be a simple hello world in node.js, available as a gist here, note that I did not create this example but it\u0026rsquo;s really close to the official documentation but shorter, you can see another example with python here.\nBefore you start make sure you already have your account configured for awscli and terraform installed.\nConfiguration files First of all we need to get our terraform file or files (in a normal case scenario, but since this is a hello world it is easier to have everything in the same file), I have added some comments of what each part does as you can see.  The code itself Then we need the code that we need or want to run there.  Initialize terraform First of all we will need to initialize terraform like in the gist below, this will download the necessary plugins that we used in the code, otherwise it won\u0026rsquo;t be able to run.  Apply the changes The next step would be to apply the changes, you can also plan to an outfile and then apply from that file, but also apply works, this command will take care of doing everything that we defined, it will archive the code, the IAM role and the function itself.  Running the function Then the last step would be to run our function to see if it actually works, in this case we\u0026rsquo;re using the awscli but you can use the AWS console as well, the result will be the same.  Clean up Remember to clean up before leaving.  I don\u0026rsquo;t know about you, but I\u0026rsquo;m going to keep using the serverless framework for now, but it\u0026rsquo;s good to see that we have alternatives and with some effort can give us the same functionality.\nErrata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":13,"section":"blog","summary":"Introduction Here we will see how to use terraform to manage lambda functions, it will be a simple hello world in node.js, available as a gist here, note that I did not create this example but it\u0026rsquo;s really close to the official documentation but shorter, you can see another example with python here.\nBefore you start make sure you already have your account configured for awscli and terraform installed.\nConfiguration files First of all we need to get our terraform file or files (in a normal case scenario, but since this is a hello world it is easier to have everything in the same file), I have added some comments of what each part does as you can see.","tags":["development","serverless","aws","terraform"],"title":"Creating a lambda function with terraform","uri":"https://techsquad.rocks/blog/creating_a_lambda_function_with_terraform/","year":"2019"},{"content":"Introduction SSH is a great tool not only to connect and interact with remote servers, in this article we will explore SSH Socks proxy and what it means, we also will explore SSH Remote Port Forward and SSH Local Port Forward and how to use that functionality.\nExplanation SOCKS is an Internet protocol that exchanges network packets between a client and server through a proxy server (Extracted from Wikipedia). So basically it allows our remote server to become a VPNey likey thingy using SSH, so let\u0026rsquo;s see the different options of how and when to use it. But we will need to tell the application to use that SOCKS proxy, for example our browser or curl.\nThe command ssh -D 9999 -Nn ec2-user@54.210.37.203\nFor example I started a EC2 instance for this example and this is the output from curl: $ curl --socks4 localhost:9999 icanhazip.com # OUTPUT: # 54.210.37.203\nThe parameters and their meaning I extracted a portion of the meaning of parameter from the man page, but in a nutshell it means dynamic port forward without a shell. -N Do not execute a remote command. This is useful for just forwarding ports. -n Redirects stdin from /dev/null (actually, prevents reading from stdin). This must be used when ssh is run in the background. -D Specifies a local “dynamic” application-level port forwarding. This works by allocating a socket to listen to port on the local side, optionally bound to the specified bind_address.\nClosing notes As you can see this option can be really handy to have a temporary VPN or proxy, also if you want to make this automatic and not so temporary you can check autossh or any real VPN solution like OpenVPN. You can use this kind of proxy in any App that supports SOCKS, most browsers do for example.\nErrata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":14,"section":"blog","summary":"Introduction SSH is a great tool not only to connect and interact with remote servers, in this article we will explore SSH Socks proxy and what it means, we also will explore SSH Remote Port Forward and SSH Local Port Forward and how to use that functionality.\nExplanation SOCKS is an Internet protocol that exchanges network packets between a client and server through a proxy server (Extracted from Wikipedia). So basically it allows our remote server to become a VPNey likey thingy using SSH, so let\u0026rsquo;s see the different options of how and when to use it.","tags":["openssh","tips-and-tricks","linux"],"title":"SSH Socks Proxy","uri":"https://techsquad.rocks/blog/ssh_socks_proxy/","year":"2019"},{"content":"Introduction SSH is a great tool not only to connect and interact with remote servers, in this article we will explore SSH Remote port forward and what it means, we also will explore SSH Local Port Forward and SSH Socks Proxy and how to use that functionality.\nExplanation Remote port forward basically let\u0026rsquo;s you forward one port from your machine to a remote machine, for example you want to connect to a local service from a remote server but just temporarily, let\u0026rsquo;s say you want to connect to a mysql instance on the default port (3306).\nThe command ssh -Nn -R 3306:localhost:3306 user@example.com\nThe parameters and their meaning I extracted a portion of the meaning of parameter from the man page, but in a nutshell it means remote port forward without a shell. -N Do not execute a remote command. This is useful for just forwarding ports. -n Redirects stdin from /dev/null (actually, prevents reading from stdin). This must be used when ssh is run in the background. -R Specifies that connections to the given TCP port or Unix socket on the remote (server) host are to be forwarded to the local side.\nServer configuration There are two configuration parameters that can change the behaviour of remote and local forwarded ports, those parameters are GatewayPorts and AllowTcpForwarding.\nGatewayPorts By default this option is no which means that only the remote computer will be able to connect to the forwarded port, you can set it to yes or clientspecified to allow other machines use that remote port-forward (handy and dangerous).\nAllowTcpForwarding By default this option is set to yes, you can restrict remote and local port forwarding by setting it to no or allow only local by setting it to local.\nClosing notes As you can see this option can be really handy to bypass firewalls for example or have a temporary port forward, also if you want to make this automatic and not so temporary you can check autossh. You can use nc (netcat) if you don\u0026rsquo;t want to install anything to test the connections and the tunnels (nc -l -p PORT) in the server side and (nc HOST PORT) in the client.\nErrata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":15,"section":"blog","summary":"Introduction SSH is a great tool not only to connect and interact with remote servers, in this article we will explore SSH Remote port forward and what it means, we also will explore SSH Local Port Forward and SSH Socks Proxy and how to use that functionality.\nExplanation Remote port forward basically let\u0026rsquo;s you forward one port from your machine to a remote machine, for example you want to connect to a local service from a remote server but just temporarily, let\u0026rsquo;s say you want to connect to a mysql instance on the default port (3306).","tags":["openssh","tips-and-tricks","linux"],"title":"SSH Remote Port Forward","uri":"https://techsquad.rocks/blog/ssh_remote_port_forward/","year":"2019"},{"content":"Introduction SSH is a great tool not only to connect and interact with remote servers, in this article we will explore SSH Local port forward and what it means, we also will explore SSH Remote Port Forward and SSH Socks Proxy and how to use that functionality.\nExplanation Local port forward basically let\u0026rsquo;s you forward one port from a remote machine to your local machine, for example you want to connect to a remote service from machine but just temporarily or there is a firewall that won\u0026rsquo;t let you do it, let\u0026rsquo;s say you want to connect to a mysql instance on the default port (3306).\nThe command  Here we are forwarding localhost:3306 in the remote machine to localhost:3306, but you can specify another address in the network for example 172.16.16.200 and the command would look like this:\n This will give you access to the ip 172.16.16.200 and port 3306 in the remote network.\nThe parameters and their meaning I extracted a portion of the meaning of parameter from the man page, but in a nutshell it means local port forward without a shell.  Server configuration There is a configuration parameter that can change the behaviour of remote and local forwarded ports, that parameter is AllowTcpForwarding.\nAllowTcpForwarding By default this option is set to yes, you can restrict remote and local port forwarding by setting it to no or allow only local by setting it to local.\nClosing notes As you can see this option can be really handy to bypass firewalls for example or have a temporary port forward, also if you want to make this automatic and not so temporary you can check autossh. You can use nc (netcat) if you don\u0026rsquo;t want to install anything to test the connections and the tunnels (nc -l -p PORT) in the server side and (nc HOST PORT) in the client.\nErrata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":16,"section":"blog","summary":"Introduction SSH is a great tool not only to connect and interact with remote servers, in this article we will explore SSH Local port forward and what it means, we also will explore SSH Remote Port Forward and SSH Socks Proxy and how to use that functionality.\nExplanation Local port forward basically let\u0026rsquo;s you forward one port from a remote machine to your local machine, for example you want to connect to a remote service from machine but just temporarily or there is a firewall that won\u0026rsquo;t let you do it, let\u0026rsquo;s say you want to connect to a mysql instance on the default port (3306).","tags":["openssh","tips-and-tricks","linux"],"title":"SSH Local Port Forward","uri":"https://techsquad.rocks/blog/ssh_local_port_forward/","year":"2019"},{"content":"Introduction In this article we will create a lambda function and an API Gateway route like we did with the serverless framework but only using AWS tools, we will be using the same generated code for our function from the last article What does the serverless framework does for me, so refer to that one before starting this one if you want to know how did we get here. Also as a side note this is a very basic example on how to get started with lambda without any additional tool.\nLet\u0026rsquo;s see the code one more time package main import ( \u0026#34;bytes\u0026#34; \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;github.com/aws/aws-lambda-go/events\u0026#34; \u0026#34;github.com/aws/aws-lambda-go/lambda\u0026#34; ) // Response is of type APIGatewayProxyResponse since we\u0026#39;re leveraging the // AWS Lambda Proxy Request functionality (default behavior) // // https://serverless.com/framework/docs/providers/aws/events/apigateway/#lambda-proxy-integration type Response events.APIGatewayProxyResponse // Handler is our lambda handler invoked by the `lambda.Start` function call func Handler(ctx context.Context) (Response, error) { var buf bytes.Buffer body, err := json.Marshal(map[string]interface{}{ \u0026#34;message\u0026#34;: \u0026#34;Go Serverless v1.0! Your function executed successfully!\u0026#34;, }) if err != nil { return Response{StatusCode: 404}, err } json.HTMLEscape(\u0026amp;buf, body) resp := Response{ StatusCode: 200, IsBase64Encoded: false, Body: buf.String(), Headers: map[string]string{ \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-MyCompany-Func-Reply\u0026#34;: \u0026#34;hello-handler\u0026#34;, }, } return resp, nil } func main() { lambda.Start(Handler) } With that code as a starting point, now we need to build, package, upload, and deploy our function:\nBuild GOOS=linux go build main.go\nPackage zip main.zip ./main # OUTPUT: # adding: main (deflated 51%)\nCreate the role\nGo to IAM \u0026gt; Roles \u0026gt; Create. Then select Lambda, assign a name and a description and then get the ARN for this role. Note that with the serverless framework this is done automatically for us, so we don\u0026rsquo;t need to create a new role for each\nUpload / Deploy aws lambda create-function \\  --region us-east-1 \\  --function-name helloworld \\  --memory 128 \\  --role arn:aws:iam::894527626897:role/testing-aws-go \\  --runtime go1.x \\  --zip-file fileb://main.zip \\  --handler main # OUTPUT: # { # \u0026#34;FunctionName\u0026#34;: \u0026#34;helloworld\u0026#34;, # \u0026#34;FunctionArn\u0026#34;: \u0026#34;arn:aws:lambda:us-east-1:894527626897:function:helloworld\u0026#34;, # \u0026#34;Runtime\u0026#34;: \u0026#34;go1.x\u0026#34;, # \u0026#34;Role\u0026#34;: \u0026#34;arn:aws:iam::894527626897:role/testing-aws-go\u0026#34;, # \u0026#34;Handler\u0026#34;: \u0026#34;main\u0026#34;, # \u0026#34;CodeSize\u0026#34;: 4346283, # \u0026#34;Description\u0026#34;: \u0026#34;\u0026#34;, # \u0026#34;Timeout\u0026#34;: 3, # \u0026#34;MemorySize\u0026#34;: 128, # \u0026#34;LastModified\u0026#34;: \u0026#34;2019-02-16T15:44:10.610+0000\u0026#34;, # \u0026#34;CodeSha256\u0026#34;: \u0026#34;02/PQBeQuCC8JS1TLjyU38oiUwiyQSmKJXjya25XpFA=\u0026#34;, # \u0026#34;Version\u0026#34;: \u0026#34;$LATEST\u0026#34;, # \u0026#34;TracingConfig\u0026#34;: { # \u0026#34;Mode\u0026#34;: \u0026#34;PassThrough\u0026#34; # }, # \u0026#34;RevisionId\u0026#34;: \u0026#34;7c9030e5-4a26-4f7e-968d-3a4f65dfde21\u0026#34; # } Note that your function-name must match the name of your Lambda handler name (Handler). Note that this role might be insecure in some scenarios if you grant too much permissions, so try to restrict it as much as possible as with any role and policy.\nTest the function aws lambda invoke --function-name helloworld --log-type Tail /dev/stdout # OUTPUT: # {\u0026#34;statusCode\u0026#34;:200,\u0026#34;headers\u0026#34;:{\u0026#34;Content-Type\u0026#34;:\u0026#34;application/json\u0026#34;,\u0026#34;X-MyCompany-Func-Reply\u0026#34;:\u0026#34;hello-handler\u0026#34;},\u0026#34;body\u0026#34;:\u0026#34;{\\\u0026#34;message\\\u0026#34;:\\\u0026#34;Go Serverless v1.0! Your function executed successfully!\\\u0026#34;}\u0026#34;}{ # \u0026#34;StatusCode\u0026#34;: 200, # \u0026#34;LogResult\u0026#34;: \u0026#34;U1RBUlQgUmVxdWVzdElkOiBmZTRmMWE4Zi1kYzAyLTQyYWQtYjBlYy0wMjA5YjY4MDY1YWQgVmVyc2lvbjogJExBVEVTVApFTkQgUmVxdWVzdElkOiBmZTRmMWE4Zi1kYzAyLTQyYWQtYjBlYy0wMjA5YjY4MDY1YWQKUkVQT1JUIFJlcXVlc3RJZDogZmU0ZjFhOGYtZGMwMi00MmFkLWIwZWMtMDIwOWI2ODA2NWFkCUR1cmF0aW9uOiAxMy4xOSBtcwlCaWxsZWQgRHVyYXRpb246IDEwMCBtcyAJTWVtb3J5IFNpemU6IDEyOCBNQglNYXggTWVtb3J5IFVzZWQ6IDQ1IE1CCQo=\u0026#34;, # \u0026#34;ExecutedVersion\u0026#34;: \u0026#34;$LATEST\u0026#34; # } Everything looks about right, so what\u0026rsquo;s next? We will eventually need to communicate with this code from an external source, so let\u0026rsquo;s see how we can do that with the API Gateway. Also the log is encoded in base64, so if you want to see what the log result was do the following.\nCheck the logs echo \u0026#34;U1RBUlQgUmVxdWVzdElkOiBmZTRmMWE4Zi1kYzAyLTQyYWQtYjBlYy0wMjA5YjY4MDY1YWQgVmVyc2lvbjogJExBVEVTVApFTkQgUmVxdWVzdElkOiBmZTRmMWE4Zi1kYzAyLTQyYWQtYjBlYy0wMjA5YjY4MDY1YWQKUkVQT1JUIFJlcXVlc3RJZDogZmU0ZjFhOGYtZGMwMi00MmFkLWIwZWMtMDIwOWI2ODA2NWFkCUR1cmF0aW9uOiAxMy4xOSBtcwlCaWxsZWQgRHVyYXRpb246IDEwMCBtcyAJTWVtb3J5IFNpemU6IDEyOCBNQglNYXggTWVtb3J5IFVzZWQ6IDQ1IE1CCQo=\u0026#34; | base64 -d # OUTPUT: # START RequestId: fe4f1a8f-dc02-42ad-b0ec-0209b68065ad Version: $LATEST # END RequestId: fe4f1a8f-dc02-42ad-b0ec-0209b68065ad # REPORT RequestId: fe4f1a8f-dc02-42ad-b0ec-0209b68065ad Duration: 13.19 ms Billed Duration: 100 ms Memory Size: 128 MB Max Memory Used: 45 MB You should also be able to see this same output in CloudWatch.\nAPI Gateway To make this step simpler I decided to use the AWS Console instead of the CLI it will also cut down the size of this article substantially.\nNow we need to create the API Gateway endpoint\nNote that you only have to go to Lambda-\u0026gt;Functions-\u0026gt;helloworld-\u0026gt;Add triggers-\u0026gt;API Gateway. And then complete as shown in the image, when you save this new trigger you will get the resource that then can be used to test the API Gateway integration.   The endpoint will show as follows (Click on API Gateway):   Test the API curl -v https://r8efasfb26.execute-api.us-east-1.amazonaws.com/default/helloworld # OUTPUT: # * Trying 54.236.123.239... # * TCP_NODELAY set # * Connected to r8efasfb26.execute-api.us-east-1.amazonaws.com (54.236.123.239) port 443 (#0) # * ALPN, offering h2 # * ALPN, offering http/1.1 # * successfully set certificate verify locations: # * CAfile: /etc/ssl/certs/ca-certificates.crt # CApath: none # * TLSv1.3 (OUT), TLS handshake, Client hello (1): # * TLSv1.3 (IN), TLS handshake, Server hello (2): # * TLSv1.2 (IN), TLS handshake, Certificate (11): # * TLSv1.2 (IN), TLS handshake, Server key exchange (12): # * TLSv1.2 (IN), TLS handshake, Server finished (14): # * TLSv1.2 (OUT), TLS handshake, Client key exchange (16): # * TLSv1.2 (OUT), TLS change cipher, Change cipher spec (1): # * TLSv1.2 (OUT), TLS handshake, Finished (20): # * TLSv1.2 (IN), TLS handshake, Finished (20): # * SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256 # * ALPN, server accepted to use h2 # * Server certificate: # * subject: CN=*.execute-api.us-east-1.amazonaws.com # * start date: Sep 20 00:00:00 2018 GMT # * expire date: Oct 20 12:00:00 2019 GMT # * subjectAltName: host \u0026#34;r8efasfb26.execute-api.us-east-1.amazonaws.com\u0026#34; matched cert\u0026#39;s \u0026#34;*.execute-api.us-east-1.amazonaws.com\u0026#34; # * issuer: C=US; O=Amazon; OU=Server CA 1B; CN=Amazon # * SSL certificate verify ok. # * Using HTTP2, server supports multi-use # * Connection state changed (HTTP/2 confirmed) # * Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0 # * Using Stream ID: 1 (easy handle 0x56394c766db0) # \u0026gt; GET /default/helloworld HTTP/2 # \u0026gt; Host: r8efasfb26.execute-api.us-east-1.amazonaws.com # \u0026gt; User-Agent: curl/7.63.0 # \u0026gt; Accept: */* # \u0026gt; # * Connection state changed (MAX_CONCURRENT_STREAMS == 128)! # \u0026lt; HTTP/2 200 # \u0026lt; date: Sat, 16 Feb 2019 17:17:58 GMT # \u0026lt; content-type: application/json # \u0026lt; content-length: 70 # \u0026lt; x-amzn-requestid: ce5c5863-320e-11e9-9e76-875e7540974c # \u0026lt; x-amz-apigw-id: VM_XAGhoIAMFqoQ= # \u0026lt; x-mycompany-func-reply: hello-handler # \u0026lt; x-amzn-trace-id: Root=1-5c6845c6-920cfc7da3cfd94f3e644647;Sampled=0 # \u0026lt; # * Connection #0 to host r8efasfb26.execute-api.us-east-1.amazonaws.com left intact # {\u0026#34;message\u0026#34;:\u0026#34;Go Serverless v1.0! Your function executed successfully!\u0026#34;}\nIf you ask me that was a lot of effort to handle without automation, maybe AWS SAM or the serverless framework can make things easier and let you focus on your application rather than the boilerplate required for it to run.\nClean up Always remember to clean up and delete everything that you created (to avoid surprises and save money), in this article I will leave that as an exercise for the reader :)\nErrata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":17,"section":"blog","summary":"Introduction In this article we will create a lambda function and an API Gateway route like we did with the serverless framework but only using AWS tools, we will be using the same generated code for our function from the last article What does the serverless framework does for me, so refer to that one before starting this one if you want to know how did we get here. Also as a side note this is a very basic example on how to get started with lambda without any additional tool.","tags":["go","golang","development","serverless","linux","terraform"],"title":"Create an AWS lambda function in Go","uri":"https://techsquad.rocks/blog/create_an_aws_lambda_function_in_go/","year":"2019"},{"content":"Introduction The serverless framework is a nice tool to manage all your cloud functions. from their page:\n The Serverless Framework helps you develop and deploy your AWS Lambda functions, along with the AWS infrastructure resources they require. It\u0026rsquo;s a CLI that offers structure, automation and best practices out-of-the-box, allowing you to focus on building sophisticated, event-driven, serverless architectures, comprised of Functions and Events.\n Let\u0026rsquo;s take the golang example for a spin So let\u0026rsquo;s generate a project with the serverless framework and see everything that it does for us. mkdir foo \u0026amp;\u0026amp; cd \u0026#34;$_\u0026#34; \u0026amp;\u0026amp; serverless create -t aws-go # OUTPUT: # Serverless: Generating boilerplate... # _______ __ # | _ .-----.----.--.--.-----.----| .-----.-----.-----. # | |___| -__| _| | | -__| _| | -__|__ --|__ --| # |____ |_____|__| \\___/|_____|__| |__|_____|_____|_____| # | | | The Serverless Application Framework # | | serverless.com, v1.36.1 # -------\u0026#39; # # Serverless: Successfully generated boilerplate for template: \u0026#34;aws-go\u0026#34; # Serverless: NOTE: Please update the \u0026#34;service\u0026#34; property in serverless.yml with your service name Got you a bit of command line fu right there with the \u0026ldquo;$_\u0026rdquo; (it means the first parameter of the previous command)\nOkay all peachy but what just happened? We initialized a serverless framework project with the template aws-go (as you probably figured by now) the serverless framework can handle different languages and cloud providers, in this example we picked aws and go (there is another template for go called aws-go-dep which as the name indicates uses dep to manage dependencies), enough talking.\nLet\u0026rsquo;s take a look at the files tree . # OUTPUT: # ├── hello # │ └── main.go # ├── Makefile # ├── serverless.yml # └── world # └── main.go # # 2 directories, 4 files We got a manifest serverless.yml a Makefile (which you can use to build your functions (to validate syntax errors for instance or run in test mode before pushing them to AWS, it will also be used to build them while deploying)\nThe manifest file indicates a lot of things, I will add comments to the code frameworkVersion: \u0026#34;\u0026gt;=1.28.0 \u0026lt;2.0.0\u0026#34; provider: name: aws runtime: go1.x # Which files needs to be included and which to be ignored package: exclude: - ./** include: - ./bin/** # The functions and the handlers (the actual function definition in the code), and events which then will be translated into API Gateway endpoints for your functions functions: hello: handler: bin/hello events: - http: path: hello method: get world: handler: bin/world events: - http: path: world method: get\nLet\u0026rsquo;s take a look at the hello function / file package main import ( \u0026#34;bytes\u0026#34; \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;github.com/aws/aws-lambda-go/events\u0026#34; \u0026#34;github.com/aws/aws-lambda-go/lambda\u0026#34; ) // Response is of type APIGatewayProxyResponse since we\u0026#39;re leveraging the // AWS Lambda Proxy Request functionality (default behavior) // // https://serverless.com/framework/docs/providers/aws/events/apigateway/#lambda-proxy-integration type Response events.APIGatewayProxyResponse // Handler is our lambda handler invoked by the `lambda.Start` function call func Handler(ctx context.Context) (Response, error) { var buf bytes.Buffer body, err := json.Marshal(map[string]interface{}{ \u0026#34;message\u0026#34;: \u0026#34;Go Serverless v1.0! Your function executed successfully!\u0026#34;, }) if err != nil { return Response{StatusCode: 404}, err } json.HTMLEscape(\u0026amp;buf, body) resp := Response{ StatusCode: 200, IsBase64Encoded: false, Body: buf.String(), Headers: map[string]string{ \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-MyCompany-Func-Reply\u0026#34;: \u0026#34;hello-handler\u0026#34;, }, } return resp, nil } func main() { lambda.Start(Handler) } This function only returns some text with some headers, every lambda function requires the lambda.Start with your function name as an entrypoint, in this case Handler, the context is usually used to pass data between calls or functions. The rest of the code is pretty straight forward go code, it builds a json object and returns it along with some headers.\nLet\u0026rsquo;s deploy it serverless deploy # OUTPUT: # Serverless: WARNING: Missing \u0026#34;tenant\u0026#34; and \u0026#34;app\u0026#34; properties in serverless.yml. Without these properties, you can not publish the service to the Serverless Platform. # Serverless: Packaging service... # Serverless: Excluding development dependencies... # Serverless: Uploading CloudFormation file to S3... # Serverless: Uploading artifacts... # Serverless: Uploading service .zip file to S3 (10.88 MB)... # Serverless: Validating template... # Serverless: Updating Stack... # Serverless: Checking Stack update progress... # ............ # Serverless: Stack update finished... # Service Information # service: aws-go # stage: dev # region: us-east-1 # stack: aws-go-dev # api keys: # None # endpoints: # GET - https://cfr9zyw3r1.execute-api.us-east-1.amazonaws.com/dev/hello # GET - https://cfr9zyw3r1.execute-api.us-east-1.amazonaws.com/dev/world # functions: # hello: aws-go-dev-hello # world: aws-go-dev-world # layers: # None So a lot happened here, the deploy function compiled our binary, packaged it, uploaded that package to s3, created a cloudformation stack and after everything was completed, returned the endpoints that were defined, as you can see the framework enabled us to create and deploy a function (two actually) really easily which totally simplifies the process of managing functions and events.\nAnd test it curl -v https://cfr9zyw3r1.execute-api.us-east-1.amazonaws.com/dev/hello # OUTPUT: # * Trying 99.84.27.2... # * TCP_NODELAY set # * Connected to cfr9zyw3r1.execute-api.us-east-1.amazonaws.com (99.84.27.2) port 443 (#0) # * ALPN, offering h2 # * ALPN, offering http/1.1 # * successfully set certificate verify locations: # * CAfile: /etc/ssl/certs/ca-certificates.crt # CApath: none # * TLSv1.3 (OUT), TLS handshake, Client hello (1): # * TLSv1.3 (IN), TLS handshake, Server hello (2): # * TLSv1.2 (IN), TLS handshake, Certificate (11): # * TLSv1.2 (IN), TLS handshake, Server key exchange (12): # * TLSv1.2 (IN), TLS handshake, Server finished (14): # * TLSv1.2 (OUT), TLS handshake, Client key exchange (16): # * TLSv1.2 (OUT), TLS change cipher, Change cipher spec (1): # * TLSv1.2 (OUT), TLS handshake, Finished (20): # * TLSv1.2 (IN), TLS handshake, Finished (20): # * SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256 # * ALPN, server accepted to use h2 # * Server certificate: # * subject: CN=*.execute-api.us-east-1.amazonaws.com # * start date: Oct 9 00:00:00 2018 GMT # * expire date: Oct 9 12:00:00 2019 GMT # * subjectAltName: host \u0026#34;cfr9zyw3r1.execute-api.us-east-1.amazonaws.com\u0026#34; matched cert\u0026#39;s \u0026#34;*.execute-api.us-east-1.amazonaws.com\u0026#34; # * issuer: C=US; O=Amazon; OU=Server CA 1B; CN=Amazon # * SSL certificate verify ok. # * Using HTTP2, server supports multi-use # * Connection state changed (HTTP/2 confirmed) # * Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0 # * Using Stream ID: 1 (easy handle 0x55944b9d7db0) # \u0026gt; GET /dev/hello HTTP/2 # \u0026gt; Host: cfr9zyw3r1.execute-api.us-east-1.amazonaws.com # \u0026gt; User-Agent: curl/7.63.0 # \u0026gt; Accept: */* # \u0026gt; # * Connection state changed (MAX_CONCURRENT_STREAMS == 128)! # \u0026lt; HTTP/2 200 # \u0026lt; content-type: application/json # \u0026lt; content-length: 70 # \u0026lt; date: Sat, 16 Feb 2019 04:32:04 GMT # \u0026lt; x-amzn-requestid: cf4c6094-31a3-11e9-b61e-bb2138b2f390 # \u0026lt; x-amz-apigw-id: VLPKmHj4oAMFbbw= # \u0026lt; x-mycompany-func-reply: hello-handler # \u0026lt; x-amzn-trace-id: Root=1-5c679243-d4f945debb1a2b675c41675f;Sampled=0 # \u0026lt; x-cache: Miss from cloudfront # \u0026lt; via: 1.1 655473215401ef909f449b92f216caa1.cloudfront.net (CloudFront) # \u0026lt; x-amz-cf-id: LOHG0oG-WbGKpTnlGz-VDVqb9DxXQX-kgJJEUkchh1v_zLfUqNCpEQ== # \u0026lt; # * Connection #0 to host cfr9zyw3r1.execute-api.us-east-1.amazonaws.com left intact # {\u0026#34;message\u0026#34;:\u0026#34;Go Serverless v1.0! Your function executed successfully!\u0026#34;}% As expected we can see the headers x-my-company-func-reply and the json object that it created for us.\nCleanup serverless remove # OUTPUT: # Serverless: WARNING: Missing \u0026#34;tenant\u0026#34; and \u0026#34;app\u0026#34; properties in serverless.yml. Without these properties, you can not publish the service to the Serverless Platform. # Serverless: Getting all objects in S3 bucket... # Serverless: Removing objects in S3 bucket... # Serverless: Removing Stack... # Serverless: Checking Stack removal progress... # ............... # Serverless: Stack removal finished... This will as you expect remove everything that was created with the deploy command.\nIn the next article we will explore how to do create and deploy a function like this one by hand.\nErrata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":18,"section":"blog","summary":"Introduction The serverless framework is a nice tool to manage all your cloud functions. from their page:\n The Serverless Framework helps you develop and deploy your AWS Lambda functions, along with the AWS infrastructure resources they require. It\u0026rsquo;s a CLI that offers structure, automation and best practices out-of-the-box, allowing you to focus on building sophisticated, event-driven, serverless architectures, comprised of Functions and Events.\n Let\u0026rsquo;s take the golang example for a spin So let\u0026rsquo;s generate a project with the serverless framework and see everything that it does for us.","tags":["development","serverless","go","golang"],"title":"What does the serverless framework does for me","uri":"https://techsquad.rocks/blog/what_does_the_serverless_framework_does_for_me/","year":"2019"},{"content":"Serverless series Part I: Serving static websites with s3 and cloudfront, so refer to that one before starting this one if you want to know how did we get here.\nPart II: Sending emails with AWS Lambda and SES from a HTML form, You are here.\nIntroduction This article is part of the serverless series, in this article we will see how to create a serverless function in AWS Lambda to send an email coming from the HTML form in the site the source code can be found here, that is the go version but if you prefer node you can use this one.\nServerless framework As usual I will be using the serverless framework to manage our functions, create the project mkdir techsquad-functions \u0026amp;\u0026amp; cd techsquad-functions \u0026amp;\u0026amp; serverless create -t aws-go # OUTPUT: # Serverless: Generating boilerplate... # _______ __ # | _ .-----.----.--.--.-----.----| .-----.-----.-----. # | |___| -__| _| | | -__| _| | -__|__ --|__ --| # |____ |_____|__| \\___/|_____|__| |__|_____|_____|_____| # | | | The Serverless Application Framework # | | serverless.com, v1.36.1 # -------\u0026#39; # # Serverless: Successfully generated boilerplate for template: \u0026#34;aws-go\u0026#34; # Serverless: NOTE: Please update the \u0026#34;service\u0026#34; property in serverless.yml with your service name\nAfter creating the project we can update the serverless manifest as follow: service: sendMail frameworkVersion: \u0026#34;\u0026gt;=1.28.0 \u0026lt;2.0.0\u0026#34; provider: name: aws runtime: go1.x region: us-east-1 memorySize: 128 versionFunctions: false stage: \u0026#39;prod\u0026#39; iamRoleStatements: - Effect: \u0026#34;Allow\u0026#34; Action: - \u0026#34;ses:*\u0026#34; - \u0026#34;lambda:*\u0026#34; Resource: - \u0026#34;*\u0026#34; package: exclude: - ./** include: - ./send_mail/send_mail functions: send_mail: handler: send_mail/send_mail events: - http: path: sendMail method: post The interesting parts here are the IAM permissions and the function send_mail, the rest is pretty much standard, we define a function and the event HTTP POST for the API Gateway, where our executable can be found and we also request permissions to send emails via SES.\nDeploy the function make deploy # OUTPUT: # rm -rf ./send_mail/send_mail # env GOOS=linux go build -ldflags=\u0026#34;-s -w\u0026#34; -o send_mail/send_mail send_mail/main.go # sls deploy --verbose # Serverless: WARNING: Missing \u0026#34;tenant\u0026#34; and \u0026#34;app\u0026#34; properties in serverless.yml. Without these properties, you can not publish the service to the Serverless Platform. # Serverless: Packaging service... # Serverless: Excluding development dependencies... # Serverless: Uploading CloudFormation file to S3... # Serverless: Uploading artifacts... # Serverless: Uploading service .zip file to S3 (7.31 MB)... # Serverless: Validating template... # Serverless: Updating Stack... # Serverless: Checking Stack update progress... # CloudFormation - UPDATE_IN_PROGRESS - AWS::CloudFormation::Stack - sendMail-prod # CloudFormation - UPDATE_IN_PROGRESS - AWS::Lambda::Function - SendUnderscoremailLambdaFunction # CloudFormation - UPDATE_COMPLETE - AWS::Lambda::Function - SendUnderscoremailLambdaFunction # CloudFormation - CREATE_IN_PROGRESS - AWS::ApiGateway::Deployment - ApiGatewayDeployment1549246566486 # CloudFormation - CREATE_IN_PROGRESS - AWS::ApiGateway::Deployment - ApiGatewayDeployment1549246566486 # CloudFormation - CREATE_COMPLETE - AWS::ApiGateway::Deployment - ApiGatewayDeployment1549246566486 # CloudFormation - UPDATE_COMPLETE_CLEANUP_IN_PROGRESS - AWS::CloudFormation::Stack - sendMail-prod # CloudFormation - DELETE_IN_PROGRESS - AWS::ApiGateway::Deployment - ApiGatewayDeployment1549246013644 # CloudFormation - DELETE_COMPLETE - AWS::ApiGateway::Deployment - ApiGatewayDeployment1549246013644 # CloudFormation - UPDATE_COMPLETE - AWS::CloudFormation::Stack - sendMail-prod # Serverless: Stack update finished... # Service Information # service: sendMail # stage: prod # region: us-east-1 # stack: sendMail-prod # api keys: # None # endpoints: # POST - https://m8ebtlirjg.execute-api.us-east-1.amazonaws.com/prod/sendMail # functions: # send_mail: sendMail-prod-send_mail # layers: # None # # Stack Outputs # ServiceEndpoint: https://m8ebtlirjg.execute-api.us-east-1.amazonaws.com/prod # ServerlessDeploymentBucketName: sendmail-prod-serverlessdeploymentbucket-1vbmb6gwt8559 Everything looks right, so what\u0026rsquo;s next? the source code.\nLambda This is basically the full source code for this function, as you will see it\u0026rsquo;s really simple: package main import ( \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/aws/aws-lambda-go/events\u0026#34; \u0026#34;github.com/aws/aws-lambda-go/lambda\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws/awserr\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws/session\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/service/ses\u0026#34; ) type Response events.APIGatewayProxyResponse type RequestData struct { Email string Message string } // This could be env vars const ( Sender = \u0026#34;web@serverless.techsquad.rocks\u0026#34; Recipient = \u0026#34;kainlite@gmail.com\u0026#34; CharSet = \u0026#34;UTF-8\u0026#34; ) func Handler(ctx context.Context, request events.APIGatewayProxyRequest) (Response, error) { fmt.Printf(\u0026#34;Request: %+v\\n\u0026#34;, request) fmt.Printf(\u0026#34;Processing request data for request %s.\\n\u0026#34;, request.RequestContext.RequestID) fmt.Printf(\u0026#34;Body size = %d.\\n\u0026#34;, len(request.Body)) var requestData RequestData json.Unmarshal([]byte(request.Body), \u0026amp;requestData) fmt.Printf(\u0026#34;RequestData: %+v\u0026#34;, requestData) var result string if len(requestData.Email) \u0026gt; 0 \u0026amp;\u0026amp; len(requestData.Message) \u0026gt; 0 { result, _ = send(requestData.Email, requestData.Message) } resp := Response{ StatusCode: 200, IsBase64Encoded: false, Body: result, Headers: map[string]string{ \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-MyCompany-Func-Reply\u0026#34;: \u0026#34;send-mail-handler\u0026#34;, }, } return resp, nil } func send(Email string, Message string) (string, error) { // This could be an env var \tsess, err := session.NewSession(\u0026amp;aws.Config{ Region: aws.String(\u0026#34;us-east-1\u0026#34;)}, ) // Create an SES session. \tsvc := ses.New(sess) // Assemble the email. \tinput := \u0026amp;ses.SendEmailInput{ Destination: \u0026amp;ses.Destination{ CcAddresses: []*string{}, ToAddresses: []*string{ aws.String(Recipient), }, }, Message: \u0026amp;ses.Message{ Body: \u0026amp;ses.Body{ Html: \u0026amp;ses.Content{ Charset: aws.String(CharSet), Data: aws.String(Message), }, Text: \u0026amp;ses.Content{ Charset: aws.String(CharSet), Data: aws.String(Message), }, }, Subject: \u0026amp;ses.Content{ Charset: aws.String(CharSet), Data: aws.String(Email), }, }, // We are using the same sender because it needs to be validated in SES. \tSource: aws.String(Sender), // Uncomment to use a configuration set \t//ConfigurationSetName: aws.String(ConfigurationSet), \t} // Attempt to send the email. \tresult, err := svc.SendEmail(input) // Display error messages if they occur. \tif err != nil { if aerr, ok := err.(awserr.Error); ok { switch aerr.Code() { case ses.ErrCodeMessageRejected: fmt.Println(ses.ErrCodeMessageRejected, aerr.Error()) case ses.ErrCodeMailFromDomainNotVerifiedException: fmt.Println(ses.ErrCodeMailFromDomainNotVerifiedException, aerr.Error()) case ses.ErrCodeConfigurationSetDoesNotExistException: fmt.Println(ses.ErrCodeConfigurationSetDoesNotExistException, aerr.Error()) default: fmt.Println(aerr.Error()) } } else { // Print the error, cast err to awserr.Error to get the Code and \t// Message from an error. \tfmt.Println(err.Error()) } return \u0026#34;there was an unexpected error\u0026#34;, err } fmt.Println(\u0026#34;Email Sent to address: \u0026#34; + Recipient) fmt.Println(result) return \u0026#34;sent!\u0026#34;, err } func main() { lambda.Start(Handler) } The code is pretty much straight forward it only expects 2 parameters and it will send an email and return sent! if everything went well. You can debug and compile your function before uploading by issuing the command make (This is really useful), and if you use make deploy you will save lots of time by only deploying working files.\nSES For this to work you will need to verify/validate your domain in SES.\nGo to SES-\u0026gt;Domains-\u0026gt;Verify a New Domain.   After putting your domain in, you will see something like this:   As I don\u0026rsquo;t have this domain in Route53 I don\u0026rsquo;t have a button to add the records to it (which makes everything simpler and faster), but it\u0026rsquo;s easy enough just create a few dns records and wait a few minutes until you get something like this:   After that just test it serverless invoke -f send_mail -d \u0026#39;{ \u0026#34;Email\u0026#34;: \u0026#34;kainlite@gmail.com\u0026#34;, \u0026#34;Message\u0026#34;: \u0026#34;test\u0026#34; }\u0026#39; # OUTPUT: { \u0026#34;statusCode\u0026#34;: 200, \u0026#34;headers\u0026#34;: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-MyCompany-Func-Reply\u0026#34;: \u0026#34;send-mail-handler\u0026#34; }, \u0026#34;body\u0026#34;: \u0026#34;\u0026#34; } After hitting enter the message popped up right away in my inbox :).\nAnother option is to use httpie echo \u0026#39;{ \u0026#34;email\u0026#34;: \u0026#34;kainlite@gmail.com\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;test2\u0026#34; }\u0026#39; | http https://m8ebtlirjg.execute-api.us-east-1.amazonaws.com/prod/sendMail # OUTPUT: # HTTP/1.1 200 OK # Access-Control-Allow-Origin: * # Connection: keep-alive # Content-Length: 32 # Content-Type: application/json # Date: Sun, 03 Feb 2019 02:24:25 GMT # Via: 1.1 3421ea0c15d4fdc0bcb792131861cb1f.cloudfront.net (CloudFront) # X-Amz-Cf-Id: kGK4R9kTpcWjZap8aeyPu0vdiCtpQ4gnhCAtCeeA6OJufzaTDL__0w== # X-Amzn-Trace-Id: Root=1-5c5650d9-7c3c8fcc5e303ca480739560;Sampled=0 # X-Cache: Miss from cloudfront # x-amz-apigw-id: UgGR7FlWIAMF75Q= # x-amzn-RequestId: d2f45b14-275a-11e9-a8f3-47d675eed13e # # sent!\nOR curl curl -i -X POST https://m8ebtlirjg.execute-api.us-east-1.amazonaws.com/prod/sendMail -d \u0026#39;{ \u0026#34;email\u0026#34;: \u0026#34;kainlite@gmail.com\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;test3\u0026#34; }\u0026#39; # OUTPUT: # HTTP/2 200 # content-type: application/json # content-length: 32 # date: Sun, 03 Feb 2019 02:28:04 GMT # x-amzn-requestid: 55cc72d0-275b-11e9-99bd-91c3fab78a2f # access-control-allow-origin: * # x-amz-apigw-id: UgG0OEigoAMF-Yg= # x-amzn-trace-id: Root=1-5c5651b4-fc5427b4798e14dc61fe161e;Sampled=0 # x-cache: Miss from cloudfront # via: 1.1 2167e4d6cf81822217c1ea31b3d3ba7e.cloudfront.net (CloudFront) # x-amz-cf-id: FttmBoeUaSwQ2AArTgVmI5br51zwVMfUrVpXPLGm1HacV4yS9IYMHA== # # sent!\nAnd that\u0026rsquo;s all for now, see you in the next article.\nErrata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":19,"section":"blog","summary":"Serverless series Part I: Serving static websites with s3 and cloudfront, so refer to that one before starting this one if you want to know how did we get here.\nPart II: Sending emails with AWS Lambda and SES from a HTML form, You are here.\nIntroduction This article is part of the serverless series, in this article we will see how to create a serverless function in AWS Lambda to send an email coming from the HTML form in the site the source code can be found here, that is the go version but if you prefer node you can use this one.","tags":["go","golang","development","serverless","linux"],"title":"Sending emails with AWS Lambda and SES from a HTML form","uri":"https://techsquad.rocks/blog/sending_emails_with_lambda_and_ses/","year":"2019"},{"content":"Serverless series Part I: Serving static websites with s3 and cloudfront, You\u0026rsquo;re here.\nPart II: Sending emails with AWS Lambda and SES from a HTML form\nIntroduction This article will be part of a series, the idea is to get a fully serverless site up and running with login functionality, maybe a profile page, and some random utility, but as we are just starting with it we will host our first draft of the page with a contact form, for the distribution of the files we will see how to configure CloudFront and for storing the files we will be using S3, S3 is an object storage service that offers industry leading scalability, data availability, security and performance, and CloudFront is a fast content delivery network (CDN). The site that we will be using were written using elm and can be found here.\nS3 First of all we need to create a bucket aws s3api create-bucket --bucket techsquad-serverless-site --region us-east-1 # OUTPUT: # { # \u0026#34;Location\u0026#34;: \u0026#34;/techsquad-serverless-site\u0026#34; # } We could serve directly from S3 but that can be expensive in a site with lots of traffic (You can do it by enabling web hosting in the bucket).\\\nFor this setup to work we first need to create a cloud-front-origin-access-identity aws cloudfront create-cloud-front-origin-access-identity --cloud-front-origin-access-identity-config CallerReference=techsquad-serverless-site-cloudfront-origin,Comment=techsquad-serverless-site-cloudfront-origin { \u0026#34;Location\u0026#34;: \u0026#34;https://cloudfront.amazonaws.com/2018-11-05/origin-access-identity/cloudfront/E3IJG9M5PO9BYE\u0026#34;, \u0026#34;ETag\u0026#34;: \u0026#34;E2XHDQQ0DDY9IJ\u0026#34;, \u0026#34;CloudFrontOriginAccessIdentity\u0026#34;: { \u0026#34;Id\u0026#34;: \u0026#34;E3IJG9M5PO9BYE\u0026#34;, \u0026#34;S3CanonicalUserId\u0026#34;: \u0026#34;c951e48af14afcf935c2455a6d503150c80f20df93b27af9ed0928eb48feb67d1b933aa1adb7e1bf88a7aacccccccccc\u0026#34;, \u0026#34;CloudFrontOriginAccessIdentityConfig\u0026#34;: { \u0026#34;CallerReference\u0026#34;: \u0026#34;techsquad-serverless-site-cloudfront-origin\u0026#34;, \u0026#34;Comment\u0026#34;: \u0026#34;techsquad-serverless-site-cloudfront-origin\u0026#34; } } } Our origin access identity was successfully created, we need to grab the S3CanonicalUserId for our S3 bucket policy.\nLimit access to your bucket with the following policy (save as bucket-policy.json) { \u0026#34;Version\u0026#34;:\u0026#34;2012-10-17\u0026#34;, \u0026#34;Id\u0026#34;:\u0026#34;PolicyForCloudFrontPrivateContent\u0026#34;, \u0026#34;Statement\u0026#34;:[ { \u0026#34;Sid\u0026#34;:\u0026#34;techsquad-serverless-site-cloudfront-origin\u0026#34;, \u0026#34;Effect\u0026#34;:\u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;:{\u0026#34;CanonicalUser\u0026#34;:\u0026#34;c951e48af14afcf935c2455a6d503150c80f20df93b27af9ed0928eb48feb67d1b933aa1adb7e1bf88a7aacccccccccc\u0026#34;}, \u0026#34;Action\u0026#34;:\u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;:\u0026#34;arn:aws:s3:::techsquad-serverless-site/*\u0026#34; } ] } This policy will only allow CloudFront to fetch the files from the S3 bucket, because we want to avoid users or anyone actually from hitting the bucket directly.\nAnd then just attach that policy to the bucket aws s3api put-bucket-policy --bucket techsquad-serverless-site --policy file://bucket-policy.json\nI\u0026rsquo;m using an old example I created and probably will continue building upon it, copy the files (the source files are in this github repo) aws s3 sync . s3://techsquad-serverless-site/ So far so good, We have our S3 bucket ready.\nCloudFront We will use this file to create our CF distribution (save it as distconfig.json or generate it with aws cloudfront create-distribution --generate-cli-skeleton \u0026gt; /tmp/distconfig.json and then replace the values: Id, DomainName, TargetOriginId, and the cname in Aliases.Items): { \u0026#34;CallerReference\u0026#34;: \u0026#34;techsquad-serverless-site-distribution\u0026#34;, \u0026#34;Aliases\u0026#34;: { \u0026#34;Quantity\u0026#34;: 1, \u0026#34;Items\u0026#34;: [ \u0026#34;serverless.techsquad.rocks\u0026#34; ] }, \u0026#34;DefaultRootObject\u0026#34;: \u0026#34;index.html\u0026#34;, \u0026#34;Origins\u0026#34;: { \u0026#34;Quantity\u0026#34;: 1, \u0026#34;Items\u0026#34;: [ { \u0026#34;Id\u0026#34;: \u0026#34;techsquad-serverless-site-cloudfront\u0026#34;, \u0026#34;DomainName\u0026#34;: \u0026#34;techsquad-serverless-site.s3.amazonaws.com\u0026#34;, \u0026#34;S3OriginConfig\u0026#34;: { \u0026#34;OriginAccessIdentity\u0026#34;: \u0026#34;origin-access-identity/cloudfront/E3IJG9M5PO9BYE\u0026#34; } } ] }, \u0026#34;DefaultCacheBehavior\u0026#34;: { \u0026#34;TargetOriginId\u0026#34;: \u0026#34;techsquad-serverless-site-cloudfront\u0026#34;, \u0026#34;ForwardedValues\u0026#34;: { \u0026#34;QueryString\u0026#34;: true, \u0026#34;Cookies\u0026#34;: { \u0026#34;Forward\u0026#34;: \u0026#34;none\u0026#34; } }, \u0026#34;TrustedSigners\u0026#34;: { \u0026#34;Enabled\u0026#34;: false, \u0026#34;Quantity\u0026#34;: 0 }, \u0026#34;ViewerProtocolPolicy\u0026#34;: \u0026#34;allow-all\u0026#34;, \u0026#34;MinTTL\u0026#34;: 3600 }, \u0026#34;CacheBehaviors\u0026#34;: { \u0026#34;Quantity\u0026#34;: 0 }, \u0026#34;Comment\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Logging\u0026#34;: { \u0026#34;Enabled\u0026#34;: false, \u0026#34;IncludeCookies\u0026#34;: true, \u0026#34;Bucket\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Prefix\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;PriceClass\u0026#34;: \u0026#34;PriceClass_All\u0026#34;, \u0026#34;Enabled\u0026#34;: true } We will leave most values in their defaults, but if you want to know more or customize your deployment check here or type aws cloudfront create-distribution help.\nLet\u0026rsquo;s finally create the CloudFront distribution for our site aws cloudfront create-distribution --distribution-config file://distconfig.json # OUTPUT: # { # \u0026#34;Location\u0026#34;: \u0026#34;https://cloudfront.amazonaws.com/2018-11-05/distribution/E1M22XXNIJ5BLN\u0026#34;, # \u0026#34;ETag\u0026#34;: \u0026#34;EW1AZUQ33NKQ7\u0026#34;, # \u0026#34;Distribution\u0026#34;: { # \u0026#34;Id\u0026#34;: \u0026#34;E1M22XXNIJ5BLN\u0026#34;, # \u0026#34;ARN\u0026#34;: \u0026#34;arn:aws:cloudfront::894527626897:distribution/E1M22XXNIJ5BLN\u0026#34;, # \u0026#34;Status\u0026#34;: \u0026#34;InProgress\u0026#34;, # \u0026#34;LastModifiedTime\u0026#34;: \u0026#34;2019-02-02T19:35:45.729Z\u0026#34;, # \u0026#34;InProgressInvalidationBatches\u0026#34;: 0, # \u0026#34;DomainName\u0026#34;: \u0026#34;d3v3xtkl1l2ynj.cloudfront.net\u0026#34;, # \u0026#34;ActiveTrustedSigners\u0026#34;: { # \u0026#34;Enabled\u0026#34;: false, # \u0026#34;Quantity\u0026#34;: 0 # }, # \u0026#34;DistributionConfig\u0026#34;: { # \u0026#34;CallerReference\u0026#34;: \u0026#34;techsquad-serverless-site-distribution\u0026#34;, # \u0026#34;Aliases\u0026#34;: { # \u0026#34;Quantity\u0026#34;: 0 # }, # \u0026#34;DefaultRootObject\u0026#34;: \u0026#34;index.html\u0026#34;, # \u0026#34;Origins\u0026#34;: { # \u0026#34;Quantity\u0026#34;: 1, # \u0026#34;Items\u0026#34;: [ # { # \u0026#34;Id\u0026#34;: \u0026#34;techsquad-serverless-site-cloudfront\u0026#34;, # \u0026#34;DomainName\u0026#34;: \u0026#34;techsquad-serverless-site.s3.amazonaws.com\u0026#34;, # \u0026#34;OriginPath\u0026#34;: \u0026#34;\u0026#34;, # \u0026#34;CustomHeaders\u0026#34;: { # \u0026#34;Quantity\u0026#34;: 0 # }, # \u0026#34;S3OriginConfig\u0026#34;: { # \u0026#34;OriginAccessIdentity\u0026#34;: \u0026#34;origin-access-identity/cloudfront/E3IJG9M5PO9BYE\u0026#34; # } # } # ] # }, # \u0026#34;OriginGroups\u0026#34;: { # \u0026#34;Quantity\u0026#34;: 0, # \u0026#34;Items\u0026#34;: [] # }, # \u0026#34;DefaultCacheBehavior\u0026#34;: { # \u0026#34;TargetOriginId\u0026#34;: \u0026#34;techsquad-serverless-site-cloudfront\u0026#34;, # \u0026#34;ForwardedValues\u0026#34;: { # \u0026#34;QueryString\u0026#34;: true, # \u0026#34;Cookies\u0026#34;: { # \u0026#34;Forward\u0026#34;: \u0026#34;none\u0026#34; # }, # \u0026#34;Headers\u0026#34;: { # \u0026#34;Quantity\u0026#34;: 0 # }, # \u0026#34;QueryStringCacheKeys\u0026#34;: { # \u0026#34;Quantity\u0026#34;: 0 # } # }, # \u0026#34;TrustedSigners\u0026#34;: { # \u0026#34;Enabled\u0026#34;: false, # \u0026#34;Quantity\u0026#34;: 0 # }, # \u0026#34;ViewerProtocolPolicy\u0026#34;: \u0026#34;allow-all\u0026#34;, # \u0026#34;MinTTL\u0026#34;: 3600, # \u0026#34;AllowedMethods\u0026#34;: { # \u0026#34;Quantity\u0026#34;: 2, # \u0026#34;Items\u0026#34;: [ # \u0026#34;HEAD\u0026#34;, # \u0026#34;GET\u0026#34; # ], # \u0026#34;CachedMethods\u0026#34;: { # \u0026#34;Quantity\u0026#34;: 2, # \u0026#34;Items\u0026#34;: [ # \u0026#34;HEAD\u0026#34;, # \u0026#34;GET\u0026#34; # ] # } # }, # \u0026#34;SmoothStreaming\u0026#34;: false, # \u0026#34;DefaultTTL\u0026#34;: 86400, # \u0026#34;MaxTTL\u0026#34;: 31536000, # \u0026#34;Compress\u0026#34;: false, # \u0026#34;LambdaFunctionAssociations\u0026#34;: { # \u0026#34;Quantity\u0026#34;: 0 # }, # \u0026#34;FieldLevelEncryptionId\u0026#34;: \u0026#34;\u0026#34; # }, # \u0026#34;CacheBehaviors\u0026#34;: { # \u0026#34;Quantity\u0026#34;: 0 # }, # \u0026#34;CustomErrorResponses\u0026#34;: { # \u0026#34;Quantity\u0026#34;: 0 # }, # \u0026#34;Comment\u0026#34;: \u0026#34;\u0026#34;, # \u0026#34;Logging\u0026#34;: { # \u0026#34;Enabled\u0026#34;: false, # \u0026#34;IncludeCookies\u0026#34;: false, # \u0026#34;Bucket\u0026#34;: \u0026#34;\u0026#34;, # \u0026#34;Prefix\u0026#34;: \u0026#34;\u0026#34; # }, # \u0026#34;PriceClass\u0026#34;: \u0026#34;PriceClass_All\u0026#34;, # \u0026#34;Enabled\u0026#34;: true, # \u0026#34;ViewerCertificate\u0026#34;: { # \u0026#34;CloudFrontDefaultCertificate\u0026#34;: true, # \u0026#34;MinimumProtocolVersion\u0026#34;: \u0026#34;TLSv1\u0026#34;, # \u0026#34;CertificateSource\u0026#34;: \u0026#34;cloudfront\u0026#34; # }, # \u0026#34;Restrictions\u0026#34;: { # \u0026#34;GeoRestriction\u0026#34;: { # \u0026#34;RestrictionType\u0026#34;: \u0026#34;none\u0026#34;, # \u0026#34;Quantity\u0026#34;: 0 # } # }, # \u0026#34;WebACLId\u0026#34;: \u0026#34;\u0026#34;, # \u0026#34;HttpVersion\u0026#34;: \u0026#34;http2\u0026#34;, # \u0026#34;IsIPV6Enabled\u0026#34;: true # } # } # } Woah a lot of details in there, but what we might need later is the ETAG if we want to download and update our distribution, so have that handy, also we can see our CloudFront URL in there which is: d3v3xtkl1l2ynj.cloudfront.net in this case.\nIt might take a few minutes to initialize the distribution, you can check the progress with aws cloudfront list-distributions | jq \u0026#34;.DistributionList.Items[0].Status\u0026#34; # OUTPUT: # \u0026#34;InProgress\u0026#34; Once it\u0026rsquo;s ready the status will be: \u0026ldquo;Deployed\u0026rdquo;, and now if we go to the CloudFront url you should see the site :). The S3 bucket will only let CloudFront access to the files so the only way to serve the site is through CloudFront.\nDNS The only thing missing is the record in the DNS (I don\u0026rsquo;t have this domain name in Route53, shame on me but a CNAME will do for now), so let\u0026rsquo;s add it and verify it using dig. dig serverless.techsquad.rocks # OUTPUT: # dig CNAME serverless.techsquad.rocks # # ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.13.5 \u0026lt;\u0026lt;\u0026gt;\u0026gt; CNAME serverless.techsquad.rocks # ;; global options: +cmd # ;; Got answer: # ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 52651 # ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 # # ;; OPT PSEUDOSECTION: # ; EDNS: version: 0, flags:; udp: 1452 # ;; QUESTION SECTION: # ;serverless.techsquad.rocks. IN CNAME # # ;; ANSWER SECTION: # serverless.techsquad.rocks. 292 IN CNAME d3v3xtkl1l2ynj.cloudfront.net. # # ;; Query time: 20 msec # ;; SERVER: 1.1.1.1#53(1.1.1.1) # ;; WHEN: Sat Feb 02 17:47:11 -03 2019 # ;; MSG SIZE rcvd: 98 As we can see the record is already there so we can go to http://serverless.techsquad.rocks (note that this only works because we set that alias in the distribution), We could add SSL by creating a certificate using Amazon Certificate Manager, but we will leave that as an exercise or a future small article.\nUseful commands In case you need to get some information some useful commands:\\\nThis command will give us the Id of our distribution aws cloudfront list-distributions --output table --query \u0026#39;DistributionList.Items[*].Id\u0026#39; # OUTPUT: # ------------------- # |ListDistributions| # +-----------------+ # | EFJVJEPWAPGU2 | # +-----------------+\nThis one the ETag (needed to perform updates for example) aws cloudfront get-distribution-config --id EFJVJEPWAPGU2 | jq \u0026#39;. | .ETag\u0026#39; # OUTPUT: # \u0026#34;E2TPQRAUPJL2P3\u0026#34;\nAnd this one will save the current config in /tmp so we can update it. aws cloudfront get-distribution-config --id EFJVJEPWAPGU2 | jq \u0026#39;. | .DistributionConfig\u0026#39; \u0026gt; /tmp/curent-distribution-E2TPQRAUPJL2P\nUpcoming articles This article is the first one in this series of serverless articles, the idea is to build a fully functional website using only serverless technologies, the next post will cover the AWS Lambda function used to send the contact form, also all code from the site can be found here.\nErrata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":20,"section":"blog","summary":"Serverless series Part I: Serving static websites with s3 and cloudfront, You\u0026rsquo;re here.\nPart II: Sending emails with AWS Lambda and SES from a HTML form\nIntroduction This article will be part of a series, the idea is to get a fully serverless site up and running with login functionality, maybe a profile page, and some random utility, but as we are just starting with it we will host our first draft of the page with a contact form, for the distribution of the files we will see how to configure CloudFront and for storing the files we will be using S3, S3 is an object storage service that offers industry leading scalability, data availability, security and performance, and CloudFront is a fast content delivery network (CDN).","tags":["elm","development","serverless","aws"],"title":"Serve your static website with S3 and CloudFront","uri":"https://techsquad.rocks/blog/serving_static_sites_with_s3_and_cloudfront/","year":"2019"},{"content":"Introduction GitHub offers static web hosting for you and your apps this is called GitHub Pages, you can use markdown (jekyll or just plain html), for example for this blog I generate all the files with Hugo.io and that gets deployed to GitHub Pages, the configuration is fairly simple as we will see in the following example (this blog setup).\nGitHub pages offers some great examples that are really easy to follow, but if you want to know how I configured everything for this blog continue reading :), if you like it or have any comment use the disqus box at the bottom of the page.\nPages The first step in order to use GH Pages is to create a repo (assuming that you already have an account) with the following name: username.github.io in my case is kainlite.github.io, as we can see in the following screenshot:   This repo already has the blog files, but as with any github repo you will see the default commands to push something to it, the next step is to configure the pages itself, for that you need to go to Settings (be sure to replace username in the link), then scroll down to the GitHub Pages section. It will look something like this:\n  As you can see the configuration is fairly simple, you choose the branch that will be used to serve the site, you can even pick a theme if you are going to go with Jekyll, and you can also have a custom domain and https, in this case as I push the static html files to the master branch I selected that branch, you can have any branch you like but it\u0026rsquo;s common to use gh-pages.\nDNS For the custom domain you need to create the following entries in your DNS dig techsquad.rocks, you can find these ips in this page: techsquad.rocks. 300 IN A 185.199.110.153 techsquad.rocks. 300 IN A 185.199.111.153 techsquad.rocks. 300 IN A 185.199.108.153 techsquad.rocks. 300 IN A 185.199.109.153 After a few minutes it should start working, and whatever you have in that repo will be served as static files, there are some limits but they are really high so you can probably start your site or blog or whatever without having to worry to much about it. If you want to know what those limits are go here, as of now the repository size limit is 1Gb, and there is a soft bandwidth limit of 100GB per month, also 10 builds per hour.\nGo Hugo Now to the interesting part, Hugo let\u0026rsquo;s you configure and customize several aspects of the generated files, first be sure to install hugo with your package manager or with go, the steps to create a blog are fairly simple: hugo new site testing-hugo # OUTPUT: # Congratulations! Your new Hugo site is created in /home/kainlite/Webs/testing-hugo. # # Just a few more steps and you\u0026#39;re ready to go: # # 1. Download a theme into the same-named folder. # Choose a theme from https://themes.gohugo.io/, or # create your own with the \u0026#34;hugo new theme \u0026lt;THEMENAME\u0026gt;\u0026#34; command. # 2. Perhaps you want to add some content. You can add single files # with \u0026#34;hugo new \u0026lt;SECTIONNAME\u0026gt;/\u0026lt;FILENAME\u0026gt;.\u0026lt;FORMAT\u0026gt;\u0026#34;. # 3. Start the built-in live server via \u0026#34;hugo server\u0026#34;. # # Visit https://gohugo.io/ for quickstart guide and full documentation. As I have shown in the tmux article, I like to have 2 panes one small pane where I can see the files being rebuilt at each save and another pane with Vim to edit the source code. You can start the hugo webserver for development with hugo serve -D and it will listen by default in the port 1313. It is very common to use themes, so you can go to the themes page and start your project with one of those, there are several ways to install the themes, and you can see the installation steps at the theme page, for example for the blog I started with Sustain but then modified it to match my needs.\nPublishing with git push The most interesting part of this setup is the simple automation that I use to publish with git push, I created the following hook in the blog repo .git/hooks/pre-push: #!/bin/bash  COMMIT_MESSAGE=`git log -n 1 --pretty=format:%s ${local_ref}` hugo -d ~/Webs/kainlite.github.io ANYTHING_CHANGED=`cd ~/Webs/kainlite.github.io \u0026amp;\u0026amp; git diff --exit-code` if [[ $? -gt 0 ]]; then cd ~/Webs/kainlite.github.io \u0026amp;\u0026amp; git add . \u0026amp;\u0026amp; git commit -m \u0026#34;${COMMIT_MESSAGE}\u0026#34; \u0026amp;\u0026amp; git push origin master fi What this simple hook does is check if there is any change and push the changes with the same commit message than in the original repo, we first grab the commit message from the original repo, and then check if something changed with git, if it did then we just add all files and push that to the repo, that will trigger a build in github pages and once completed our page will be updated and visible (it can take a few seconds sometimes, but in general it\u0026rsquo;s pretty fast).\nAnd that\u0026rsquo;s how this blog was configured, in the upcoming articles I will show you how to host your static website with S3 and serve it with cloudflare, after that we will use a go lambda function to send the form email, let me know any comments or anything that you might want me to write about.\nPages Environment If you paid attention at the first screenshot you probably noticed that it says 1 Environment that means that GH Pages have been already configured and if we click it we can see something like this:   For static html sites it would be unlikely to see a failure, but it can happen if you use Jekyll for example and there is any syntax error.\nErrata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":21,"section":"blog","summary":"Introduction GitHub offers static web hosting for you and your apps this is called GitHub Pages, you can use markdown (jekyll or just plain html), for example for this blog I generate all the files with Hugo.io and that gets deployed to GitHub Pages, the configuration is fairly simple as we will see in the following example (this blog setup).\nGitHub pages offers some great examples that are really easy to follow, but if you want to know how I configured everything for this blog continue reading :), if you like it or have any comment use the disqus box at the bottom of the page.","tags":["development","serverless","git"],"title":"Serve your static website in Github","uri":"https://techsquad.rocks/blog/serving_static_sites_with_github/","year":"2019"},{"content":"Introduction In this article I want to introduce you to tmux, you might have used screen in the past or heard about it, what tmux and screen are is terminal multiplexers, what does that mean? That you can have many windows/tabs and splits/panes in just one terminal window, this can really make things easier when using it as a development environment for example, you can detach from the terminal and leave things running indefinitely, or share your terminal with a colleague over ssh, for the examples I will be explaining bits of my configuration and how do I use it. The full configuration can be found here. I\u0026rsquo;m using ZSH as shell and Vim as text editor.\nTmux I also use tmux to maintain sessions, for example I can only have one terminal window open because with the help from ZSH it will attach automatically to a session thanks to oh-my-zsh and the plugin tmux, I use tabs aka windows a lot, sometimes I also use splits aka as panes.\nNow you have some basic understanding of what tmux does and how does it name its things, let\u0026rsquo;s examine some bits of the config and how to use it.\nAttach/detach from a session In order to create a session and attach to it you need to execute tmux new -s my-session, then to detach from it: CTRL-a d and to re-attach tmux a -t my-session, then kill it tmux kill-session -t my-session or logout from all windows.\nPrefix I don\u0026rsquo;t use the default prefix that is: CTRL-b, I use CTRL-a like in screen. # Use ctrl-a instead of ctrl-b set -g prefix C-a unbind C-b bind C-a send-prefix\nExample You can print the numbers of the panes with CTRL-a q, and you can navigate windows and panes as a list with CTRL-a w.  I usually like to have 3 panes, something like this:  I can edit the code or whatever in the pane 0, run commands if I need to in the pane 1, and have the webserver or code compiler, etc in the pane 2. This is very handy because I can write and test things at the same time without leaving the keyboard, or look at 2 different projects/files/etc side by side.\nWith tmux ls we can list active sessions, also tmux has a command mode (CTRL-a :) like Vim, where you can issue some commands or change settings on the fly, for example instead of executing tmux ls, you can get the same information doing CTRL-a : and then ls\u0026lt;CR\u0026gt;.\nDefaults Some helpful settings, for example start windows at 1 instead of 0, renumber on exit also makes it easier with windows. # Start window numbers at 1 to match keyboard order with tmux window order set -g base-index 1 # Scrollback buffer n lines set -g history-limit 10000 # Renumber tabs on exit set-option -g renumber-windows on # Use vi keybindings in copy and choice modes set-window-option -g mode-keys vi # Enable mouse, enables you to scroll in the tmux history buffer. set -g mouse on\nMovement I move between windows with CTRL+h and CTRL+l, and between panes with CTRL-a [hjkl]. # Move between windows bind-key -n C-h prev bind-key -n C-l next # Move between panes unbind h bind h select-pane -L unbind j bind j select-pane -D unbind k bind k select-pane -U unbind l bind l select-pane -R\nConfiguration A handy trick if you are testing the configuration is to reload it from the file with CTRL-a r # Force a reload of the config file unbind r bind r source-file ~/.tmux.conf \\; display \u0026#34;Reloaded!\u0026#34;\nPanes Everything is nice and shiny, but how do I open a pane or a new window? # Horizontal and vertical splits unbind | bind | split-window -h unbind - bind - split-window Easy, CTRL-a | will give you a vertical pane, and CTRL-a - will give you an horizontal pane. You an also re-order the panes with CTRL-a SPACE\nYou can also re-order windows with SHIFT-Left Arrow and SHIFT-Right Arrow. # Swap windows bind-key -n S-Left swap-window -t -1 bind-key -n S-Right swap-window -t +1\nStatus bar The status bar and the colors, it\u0026rsquo;s fairly simple but I like it. # Status bar has a dim gray background set-option -g status-bg colour234 set-option -g status-fg colour0 # Left shows the session name, in blue set-option -g status-left-bg default set-option -g status-left-fg colour74 # Right is some CPU stats, so terminal green set-option -g status-right-bg default set-option -g status-right-fg colour71 # Windows are medium gray; current window is white set-window-option -g window-status-fg colour244 set-window-option -g window-status-current-fg \u0026#39;#ffffff\u0026#39; set-window-option -g window-status-current-bg \u0026#39;#000000\u0026#39; # Beeped windows get a blinding orange background set-window-option -g window-status-bell-fg \u0026#39;#000000\u0026#39; set-window-option -g window-status-bell-bg \u0026#39;#d78700\u0026#39; set-window-option -g window-status-bell-attr none # Trim window titles to a reasonable length set-window-option -g window-status-format \u0026#39;#[fg=yellow] #F#I#[default] #W \u0026#39; set-window-option -g window-status-current-format \u0026#39;#[bg=yellow] #I#[bg=yellow] #W \u0026#39;\nCopy/paste  Tmux also supports the vi-copy mode, you can enter this mode with CTRL-a ESC, then pressing v for normal selection or V for line selection you can mark and copy with Y (by default is ENTER aka \u0026lt;CR\u0026gt;).\nAnd as you can imagine you can paste with CTRL-a p, this is really handy when copying from one pane to another or from one window to another, in Vim I recommend you :set paste! before pasting into it, so it doesn\u0026rsquo;t try to format, etc.\nIt also copies to the clipboard buffer, using xsel. # Make copy mode more vim like bind Escape copy-mode unbind p bind p paste-buffer bind-key -T edit-mode-vi Up send-keys -X history-up bind-key -T edit-mode-vi Down send-keys -X history-down unbind-key -T copy-mode-vi Space ; bind-key -T copy-mode-vi v send-keys -X begin-selection unbind-key -T copy-mode-vi Enter ; bind-key -T copy-mode-vi y send-keys -X copy-pipe-and-cancel \u0026#34;xsel -i --clipboard\u0026#34; bind-key -T copy-mode-vi Enter send-keys -X copy-pipe-and-cancel \u0026#34;xsel -i --clipboard\u0026#34; unbind-key -T copy-mode-vi C-v ; bind-key -T copy-mode-vi C-v send-keys -X rectangle-toggle unbind-key -T copy-mode-vi [ ; bind-key -T copy-mode-vi [ send-keys -X begin-selection unbind-key -T copy-mode-vi ] ; bind-key -T copy-mode-vi ] send-keys -X copy-selection\nIf you want to learn more about tmux a good place to start is the Arch Linux wiki.\nnotes Sometimes you can have issues with the keys HOME and END, this can help with that. # Home / End patch bind -n End send-key C-e bind -n Home send-key C-a\nErrata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":22,"section":"blog","summary":"Introduction In this article I want to introduce you to tmux, you might have used screen in the past or heard about it, what tmux and screen are is terminal multiplexers, what does that mean? That you can have many windows/tabs and splits/panes in just one terminal window, this can really make things easier when using it as a development environment for example, you can detach from the terminal and leave things running indefinitely, or share your terminal with a colleague over ssh, for the examples I will be explaining bits of my configuration and how do I use it.","tags":["urxvt","vim","linux","tmux"],"title":"Give super powers to your terminal with tmux","uri":"https://techsquad.rocks/blog/give_super_powers_to_your_terminal_with_tmux/","year":"2019"},{"content":"Introduction This article explains how to create a serverless tweet-bot, basically pulls articles from this blog and post them to twitter in a nice way. It uses cron as the trigger so it should post a tweet every 12 hours, or you can invoke it manually.\nTwitter So before you can start with the Twitter API you need to get a developer account in this url, after submitted and created, you then need to create an App and generate the keys and tokens to be able to use it, it might take a while, I recommend you read everything that Twitter wants you to read while creating both the dev account and the app, so you can understand the scope and the good practices of using their services.\nThe code I added several comments over the code so it\u0026rsquo;s easy to understand what everything is supposed to do, also it can be found here. package main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;math/rand\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/aws/aws-lambda-go/lambda\u0026#34; \u0026#34;github.com/dghubble/go-twitter/twitter\u0026#34; \u0026#34;github.com/dghubble/oauth1\u0026#34; \u0026#34;github.com/joho/godotenv\u0026#34; log \u0026#34;github.com/sirupsen/logrus\u0026#34; ) // Version var Version string // Environment var Env string // Page JSON object type Page struct { Version string `json:\u0026#34;version\u0026#34;` Title string `json:\u0026#34;title\u0026#34;` BaseURL string `json:\u0026#34;home_page_url\u0026#34;` FeedURL string `json:\u0026#34;feed_url\u0026#34;` Articles []Article `json:\u0026#34;items\u0026#34;` } // Article JSON object type Article struct { ID string `json:\u0026#34;id\u0026#34;` URL string `json:\u0026#34;url\u0026#34;` Title string `json:\u0026#34;title\u0026#34;` Content string `json:\u0026#34;content_html\u0026#34;` Published string `json:\u0026#34;date_published\u0026#34;` } // Twitter Access type Twitter struct { config *oauth1.Config token *oauth1.Token httpClient *http.Client client *twitter.Client tweetFormat string screenName string } // This functions grabs all the necessary bits to connect to the Twitter API. func (t *Twitter) Setup() { log.Debug(\u0026#34;Setting up twitter client\u0026#34;) var twitterAccessKey string var twitterAccessSecret string var twitterConsumerKey string var twitterConsumerSecret string // Get the access keys from ENV \ttwitterAccessKey = os.Getenv(\u0026#34;TWITTER_ACCESS_KEY\u0026#34;) twitterAccessSecret = os.Getenv(\u0026#34;TWITTER_ACCESS_SECRET\u0026#34;) twitterConsumerKey = os.Getenv(\u0026#34;TWITTER_CONSUMER_KEY\u0026#34;) twitterConsumerSecret = os.Getenv(\u0026#34;TWITTER_CONSUMER_SECRET\u0026#34;) twitterScreenName := os.Getenv(\u0026#34;TWITTER_SCREEN_NAME\u0026#34;) if twitterScreenName == \u0026#34;\u0026#34; { log.Fatalf(\u0026#34;Twitter screen name cannot be null\u0026#34;) } if twitterConsumerKey == \u0026#34;\u0026#34; { log.Fatal(\u0026#34;Twitter consumer key can not be null\u0026#34;) } if twitterConsumerSecret == \u0026#34;\u0026#34; { log.Fatal(\u0026#34;Twitter consumer secret can not be null\u0026#34;) } if twitterAccessKey == \u0026#34;\u0026#34; { log.Fatal(\u0026#34;Twitter access key can not be null\u0026#34;) } if twitterAccessSecret == \u0026#34;\u0026#34; { log.Fatal(\u0026#34;Twitter access secret can not be null\u0026#34;) } log.Debug(\u0026#34;Setting up oAuth for twitter\u0026#34;) // Setup the new oauth client \tt.config = oauth1.NewConfig(twitterConsumerKey, twitterConsumerSecret) t.token = oauth1.NewToken(twitterAccessKey, twitterAccessSecret) t.httpClient = t.config.Client(oauth1.NoContext, t.token) // Twitter client \tt.client = twitter.NewClient(t.httpClient) // Set the screen name for later use \tt.screenName = twitterScreenName // This is the format of the tweet \tt.tweetFormat = \u0026#34;%s: %s - TBO\u0026#34; log.Debug(\u0026#34;Twitter client setup complete\u0026#34;) } // Format tweets func (t *Twitter) GetTweetString(article Article) string { return fmt.Sprintf(t.tweetFormat, article.Title, article.URL) } // Send the tweet func (t *Twitter) Send(article Article) { log.Debug(\u0026#34;Sending tweet\u0026#34;) if Env != \u0026#34;production\u0026#34; { log.Infof(\u0026#34;Non production mode, would\u0026#39;ve tweeted: %s\u0026#34;, t.GetTweetString(article)) } if Env == \u0026#34;production\u0026#34; { log.Infof(\u0026#34;Sending tweet: %s\u0026#34;, t.GetTweetString(article)) if _, _, err := t.client.Statuses.Update(t.GetTweetString(article), nil); err != nil { log.Fatalf(\u0026#34;Error sending tweet to twitter: %s\u0026#34;, err) } } } // Get a random article from the feed // This functions checks that the same tweet is not present // in the last 30 tweets func (t *Twitter) PickArticle(article Article) bool { log.Debug(\u0026#34;Checking to see if the tweet appeared in the last 30 tweets\u0026#34;) tweets, _, err := t.client.Timelines.UserTimeline(\u0026amp;twitter.UserTimelineParams{ ScreenName: t.screenName, Count: 30, TweetMode: \u0026#34;extended\u0026#34;, }) if err != nil { log.Fatalf(\u0026#34;Error getting last 30 tweets from user: %s\u0026#34;, err) } for _, tweet := range tweets { if strings.Contains(tweet.Text, t.GetTweetString(article)) { return true } } return false } // Twitter API constant var tw Twitter // This function is rather large, but basically grabs the a big json from // the jsonfeed url and tests several tweets until it finds one that it\u0026#39;s valid // a tweet could be invalid if for example it was already tweeted in the last 30 tweets func GetArticle() Article { url := \u0026#34;https://techsquad.rocks/index.json\u0026#34; // Setup a new HTTP Client with 3 seconds timeout \thttpClient := http.Client{ Timeout: time.Second * 3, } // Create a new HTTP Request \treq, err := http.NewRequest(http.MethodGet, url, nil) if err != nil { // An error has occurred that we can\u0026#39;t recover from, bail. \tlog.Fatalf(\u0026#34;Error occurred creating new request: %s\u0026#34;, err) } // Set the user agent to tbo \u0026lt;version\u0026gt; - twitter.com/kainlite \treq.Header.Set(\u0026#34;User-Agent\u0026#34;, fmt.Sprintf(\u0026#34;TBO %s - twitter.com/kainlite\u0026#34;, Version)) // Tell the remote server to send us JSON \treq.Header.Set(\u0026#34;Accept\u0026#34;, \u0026#34;application/json\u0026#34;) // We\u0026#39;re only going to try maxTries times, otherwise we\u0026#39;ll fatal out. \t// Execute the request \tlog.Debugf(\u0026#34;Attempting request to %s\u0026#34;, req) res, getErr := httpClient.Do(req) if getErr != nil { // We got an error, lets bail out, we can\u0026#39;t do anything more \tlog.Fatalf(\u0026#34;Error occurred retrieving article from API: %s\u0026#34;, getErr) } // BGet the body from the result \tbody, readErr := ioutil.ReadAll(res.Body) if readErr != nil { // This shouldn\u0026#39;t happen, but if it does, error out. \tlog.Fatalf(\u0026#34;Error occurred reading from result body: %s\u0026#34;, readErr) } // Parse json into the struct Page \tvar page Page if err := json.Unmarshal(body, \u0026amp;page); err != nil { // Invalid JSON was received, bail out \tlog.Fatalf(\u0026#34;Error occurred decoding article: %s\u0026#34;, err) } invalidArticle := true try := 0 maxTries := 10 // Attempt to get a valid article for the next tweet \tvar article Article for invalidArticle { rand.Seed(time.Now().UnixNano()) randomInt := rand.Intn(len(page.Articles)) article = page.Articles[randomInt] fmt.Printf(\u0026#34;%+v\u0026#34;, randomInt) // check to make sure the tweet hasn\u0026#39;t been sent before \tif tw.PickArticle(article) { try += 1 continue } // If we get here we\u0026#39;ve found a tweet, exit the loop \tinvalidArticle = false if try \u0026gt;= maxTries { log.Fatal(\u0026#34;Exiting after attempts to retrieve article failed.\u0026#34;) } } // Return the valid article response \treturn article } // HandleRequest - Handle the incoming Lambda request func HandleRequest() { log.Debug(\u0026#34;Started handling request\u0026#34;) tw.Setup() article := GetArticle() // Send tweet \ttw.Send(article) } // Set the local environment func setRunningEnvironment() { // Get the environment variable \tswitch os.Getenv(\u0026#34;APP_ENV\u0026#34;) { case \u0026#34;production\u0026#34;: Env = \u0026#34;production\u0026#34; case \u0026#34;development\u0026#34;: Env = \u0026#34;development\u0026#34; default: Env = \u0026#34;development\u0026#34; } if Env != \u0026#34;production\u0026#34; { Version = Env } } func shutdown() { log.Info(\u0026#34;Shutdown request registered\u0026#34;) } func init() { // Set the environment \tsetRunningEnvironment() // Set logging configuration \tlog.SetFormatter(\u0026amp;log.TextFormatter{ DisableColors: true, FullTimestamp: true, }) log.SetReportCaller(true) switch Env { case \u0026#34;development\u0026#34;: log.SetLevel(log.DebugLevel) case \u0026#34;production\u0026#34;: log.SetLevel(log.ErrorLevel) default: log.SetLevel(log.InfoLevel) } } func main() { // Start the bot \tlog.Debug(\u0026#34;Starting main\u0026#34;) log.Printf(\u0026#34;TBO %s\u0026#34;, Version) if Env == \u0026#34;production\u0026#34; { lambda.Start(HandleRequest) } else { // this environment variables are used locally while debugging, it can be quite handy \tif err := godotenv.Load(); err != nil { log.Fatal(\u0026#34;Error loading .env file\u0026#34;) } HandleRequest() } } The code is fairly straigth forward, it checks for the environment to have a locally runable/debuggable app if it\u0026rsquo;s development or if it\u0026rsquo;s running as an AWS Function in production.\nWhile debugging locally it can be ran like this You can save use an .env file to test debug how your tweets are going to look. go run . # OUTPUT: # time=\u0026#34;2019-01-21T22:39:15-03:00\u0026#34; level=debug msg=\u0026#34;Starting main\u0026#34; func=main.main file=\u0026#34;/home/kainlite/Webs/tbo/tbo/main.go:279\u0026#34; # time=\u0026#34;2019-01-21T22:39:15-03:00\u0026#34; level=info msg=\u0026#34;TBO development\u0026#34; func=main.main file=\u0026#34;/home/kainlite/Webs/tbo/tbo/main.go:280\u0026#34; # time=\u0026#34;2019-01-21T22:39:15-03:00\u0026#34; level=debug msg=\u0026#34;Started handling request\u0026#34; func=main.HandleRequest file=\u0026#34;/home/kainlite/Webs/tbo/tbo/main.go:225\u0026#34; # time=\u0026#34;2019-01-21T22:39:15-03:00\u0026#34; level=debug msg=\u0026#34;Setting up twitter client\u0026#34; func=\u0026#34;main.(*Twitter).Setup\u0026#34; file=\u0026#34;/home/kainlite/Webs/tbo/tbo/main.go:55\u0026#34; # time=\u0026#34;2019-01-21T22:39:15-03:00\u0026#34; level=debug msg=\u0026#34;Setting up oAuth for twitter\u0026#34; func=\u0026#34;main.(*Twitter).Setup\u0026#34; file=\u0026#34;/home/kainlite/Webs/tbo/tbo/main.go:88\u0026#34; # time=\u0026#34;2019-01-21T22:39:15-03:00\u0026#34; level=debug msg=\u0026#34;Twitter client setup complete\u0026#34; func=\u0026#34;main.(*Twitter).Setup\u0026#34; file=\u0026#34;/home/kainlite/Webs/tbo/tbo/main.go:102\u0026#34; # time=\u0026#34;2019-01-21T22:39:15-03:00\u0026#34; level=debug msg=\u0026#34;Attempting request to \u0026amp;{GET https://techsquad.rocks/index.json HTTP/1.1 %!s(int=1) %!s(int=1) map[User-Agent:[TBO development - twitter.com/kainlite] Accept:[application/json]] \u0026lt;nil\u0026gt; %!s(func() (io.ReadCloser, error)=\u0026lt;nil\u0026gt;) %!s(int64=0) [] %!s(bool=false) techsquad.rocks map[] map[] %!s(*multipart.Form=\u0026lt;nil\u0026gt;) map[] %!s(*tls.ConnectionState=\u0026lt;nil\u0026gt;) %!s(\u0026lt;-chan struct {}=\u0026lt;nil\u0026gt;) %!s(*http.Response=\u0026lt;nil\u0026gt;) \u0026lt;nil\u0026gt;}\u0026#34; func=main.GetArticle file=\u0026#34;/home/kainlite/Webs/tbo/tbo/main.go:173\u0026#34; # 4time=\u0026#34;2019-01-21T22:39:15-03:00\u0026#34; level=debug msg=\u0026#34;Checking to see if the tweet appeared in the last 30 tweets\u0026#34; func=\u0026#34;main.(*Twitter).PickArticle\u0026#34; file=\u0026#34;/home/kainlite/Webs/tbo/tbo/main.go:125\u0026#34; # time=\u0026#34;2019-01-21T22:39:16-03:00\u0026#34; level=debug msg=\u0026#34;Sending tweet\u0026#34; func=\u0026#34;main.(*Twitter).Send\u0026#34; file=\u0026#34;/home/kainlite/Webs/tbo/tbo/main.go:111\u0026#34; # time=\u0026#34;2019-01-21T22:39:16-03:00\u0026#34; level=info msg=\u0026#34;Non production mode, would\u0026#39;ve tweeted: Getting started with skaffold: https://techsquad.rocks/blog/getting_started_with_skaffold/ - TBO\u0026#34; func=\u0026#34;main.(*Twitter).Send\u0026#34; file=\u0026#34;/home/kainlite/Webs/tbo/tbo/main.go:113\u0026#34; The output is very verbose but it will show you everything that the function will do.\nCreating the project But how did you get the project skeleton? mkdir tbo \u0026amp;\u0026amp; cd tbo \u0026amp;\u0026amp; serverless create -t aws-go By default it creates two go functions: hello and world, if you look at the files serverless.yaml and the go code, it will be easy to understand how everything is tied together in the default example.\nServerless framework This function is managed by the serverless framework, as you can see it\u0026rsquo;s an easy way to manage your functions, what this small block of YAML will do is compile, upload, and schedule our function (because we use an event schedule) # Welcome to Serverless! # # This file is the main config file for your service. # It\u0026#39;s very minimal at this point and uses default values. # You can always add more config options for more control. # We\u0026#39;ve included some commented out config examples here. # Just uncomment any of them to get that config option. # # For full config options, check the docs: # docs.serverless.com # # Happy Coding! service: handler # You can pin your service to only deploy with a specific Serverless version # Check out our docs for more details # frameworkVersion: \u0026#34;=X.X.X\u0026#34; frameworkVersion: \u0026#34;\u0026gt;=1.28.0 \u0026lt;2.0.0\u0026#34; provider: name: aws runtime: go1.x region: ${env:AWS_DEFAULT_REGION, \u0026#39;us-east-1\u0026#39;} stage: ${env:TBO_BUILD_STAGE, \u0026#39;prod\u0026#39;} memorySize: 128 versionFunctions: false package: exclude: - ./** include: - ./tbo/tbo functions: tweet: handler: tbo/tbo events: - schedule: cron(0 */12 * * ? *) environment: APP_ENV: \u0026#34;production\u0026#34; TWITTER_SCREEN_NAME: \u0026#34;twitter_username\u0026#34; TWITTER_CONSUMER_KEY: \u0026#34;example_key\u0026#34; TWITTER_CONSUMER_SECRET: \u0026#34;example_secret\u0026#34; TWITTER_ACCESS_KEY: \u0026#34;example_key\u0026#34; TWITTER_ACCESS_SECRET: \u0026#34;example_secret\u0026#34; We provide the environment variables there that the app needs to run, under the hood what serverless will do is create an s3 bucket for this function with a cloudformation stack and a zip file with your function (for each version or deployment), then it will apply that that stack and validate that everything went ok.\nDeploy the function Once the code is ready and you are ready to test it in production aka send a real tweet, just deploy it. serverless deploy # OUTPUT: # Serverless: WARNING: Missing \u0026#34;tenant\u0026#34; and \u0026#34;app\u0026#34; properties in serverless.yml. Without these properties, you can not publish the service to the Serverless Platform. # Serverless: Packaging service... # Serverless: Excluding development dependencies... # Serverless: Uploading CloudFormation file to S3... # Serverless: Uploading artifacts... # Serverless: Uploading service .zip file to S3 (9.86 MB)... # Serverless: Validating template... # Serverless: Updating Stack... # Serverless: Checking Stack update progress... # .................. # Serverless: Stack update finished... # Service Information # service: handler # stage: prod # region: us-east-1 # stack: handler-prod # api keys: # None # endpoints: # None # functions: # tweet: handler-prod-tweet # layers: # None As we described before you can see everything that the serverless framework did for us, nothing really hard to remember and everything automated.\nS3 Example s3 bucket from the previous deployment. aws s3 ls # OUTPUT: # 2019-01-21 22:42:05 handler-prod-serverlessdeploymentbucket-1s5fs5igk2pwc As we can see after the deployment we see a new bucket with our function and if we take a look at the files we will find several (depending on how many deployments you do) stacks/manifests and the zip file with our function for each version/deployment.\nInvoke the function Ok, but I don\u0026rsquo;t want to wait 12 hours to see if everything is okay, then just invoke the function. serverless invoke -f tweet # OUTPUT: # null Wait, where did tweet came from?, if you look at the serverless manifest you will see that our function is called tweet. If everything went well you will be able to see that tweet in your profile, something like this:  Notes  Why TBO, what\u0026rsquo;s tbo? bot misspelled. The Serverless framework is really cool and works in a variety of environments, I certainly recommend taking a look and at least trying it, I use it for a few small projects and it eases my life a lot.  Errata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":23,"section":"blog","summary":"Introduction This article explains how to create a serverless tweet-bot, basically pulls articles from this blog and post them to twitter in a nice way. It uses cron as the trigger so it should post a tweet every 12 hours, or you can invoke it manually.\nTwitter So before you can start with the Twitter API you need to get a developer account in this url, after submitted and created, you then need to create an App and generate the keys and tokens to be able to use it, it might take a while, I recommend you read everything that Twitter wants you to read while creating both the dev account and the app, so you can understand the scope and the good practices of using their services.","tags":["go","aws","serverless"],"title":"How to create a serverless twitter bot","uri":"https://techsquad.rocks/blog/creating_a_serverless_twitter_bot/","year":"2019"},{"content":"Introduction In this article we will create a cluster from scratch with kops (K8s installation, upgrades and management) in AWS, We will configure aws-alb-ingress-controller (External traffic into our services/pods) and external dns (Update the records based in the ingress rules) and also learn a bit about awscli in the process.\nBasically we will have a fully functional cluster that will be able to handle public traffic in minutes, first we will install the cluster with kops, then we will enable the ingress controller and lastly external-dns, then we will deploy a basic app to test that everything works fine, SSL/TLS is out of the scope but it\u0026rsquo;s fairly easy to implement if you are using ACM.\nJust in case you don\u0026rsquo;t know this setup is not going to be free, cheap for sure because we will use small instances, etc, but not completely free, so before you dive in, be sure that you can spend a few bucks testing it out.\nKops This is an awesome tool to setup and maintain your clusters, currently only compatible with AWS and GCE, other platforms are planned and some are also supported in alpha, we will be using AWS in this example, it requires kubectl so make sure you have it installed: curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d \u0026#39;\u0026#34;\u0026#39; -f 4)/kops-linux-amd64 chmod +x kops-linux-amd64 sudo mv kops-linux-amd64 /usr/local/bin/kops\nExport the credentials that we will be using to create the kops user and policies export AWS_ACCESS_KEY_ID=XXXX \u0026amp;\u0026amp; export AWS_SECRET_ACCESS_KEY=XXXXX You can do it this way or just use aws configure and set a profile.\nThe next thing that we need are IAM credentials for kops to work, you will need awscli configured and working with your AWS admin-like account most likely before proceeding: # Create iam group aws iam create-group --group-name kops # OUTPUT: # { # \u0026#34;Group\u0026#34;: { # \u0026#34;Path\u0026#34;: \u0026#34;/\u0026#34;, # \u0026#34;GroupName\u0026#34;: \u0026#34;kops\u0026#34;, # \u0026#34;GroupId\u0026#34;: \u0026#34;AGPAIABI3O4WYM46AIX44\u0026#34;, # \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::894527626897:group/kops\u0026#34;, # \u0026#34;CreateDate\u0026#34;: \u0026#34;2019-01-18T01:04:23Z\u0026#34; # } # } aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess --group-name kops aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonRoute53FullAccess --group-name kops aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess --group-name kops aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/IAMFullAccess --group-name kops # Attach policies aws iam create-user --user-name kops aws iam add-user-to-group --user-name kops --group-name kops # OUTPUT: # { # \u0026#34;Group\u0026#34;: { # \u0026#34;Path\u0026#34;: \u0026#34;/\u0026#34;, # \u0026#34;GroupName\u0026#34;: \u0026#34;kops\u0026#34;, # \u0026#34;GroupId\u0026#34;: \u0026#34;AGPAIABI3O4WYM46AIX44\u0026#34;, # \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::894527626897:group/kops\u0026#34;, # \u0026#34;CreateDate\u0026#34;: \u0026#34;2019-01-18T01:04:23Z\u0026#34; # } # } # Create access key - save the output of this command. aws iam create-access-key --user-name kops # OUTPUT: # { # \u0026#34;AccessKey\u0026#34;: { # \u0026#34;UserName\u0026#34;: \u0026#34;kops\u0026#34;, # \u0026#34;AccessKeyId\u0026#34;: \u0026#34;AKIAJE*********\u0026#34;, # \u0026#34;Status\u0026#34;: \u0026#34;Active\u0026#34;, # \u0026#34;SecretAccessKey\u0026#34;: \u0026#34;zWJhfemER**************************\u0026#34;, # \u0026#34;CreateDate\u0026#34;: \u0026#34;2019-01-18T01:05:44Z\u0026#34; # } # } The last command will output the access key and the secret key for the kops user, save that information because we will use it from now on, note that we gave kops a lot of power with that user, so be careful with the keys.\nAdditional permissions to be able to create ALBs cat \u0026lt;\u0026lt; EOF \u0026gt; kops-alb-policy.json { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;iam:CreateServiceLinkedRole\u0026#34;, \u0026#34;tag:GetResources\u0026#34;, \u0026#34;elasticloadbalancing:*\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ] } ] } EOF aws iam create-policy --policy-name kops-alb-policy --policy-document file://kops-alb-policy.json # OUTPUT: # { # \u0026#34;Policy\u0026#34;: { # \u0026#34;PolicyName\u0026#34;: \u0026#34;kops-alb-policy\u0026#34;, # \u0026#34;PolicyId\u0026#34;: \u0026#34;ANPAIRIYZZZTCPJGNZZXS\u0026#34;, # \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::894527626897:policy/kops-alb-policy\u0026#34;, # \u0026#34;Path\u0026#34;: \u0026#34;/\u0026#34;, # \u0026#34;DefaultVersionId\u0026#34;: \u0026#34;v1\u0026#34;, # \u0026#34;AttachmentCount\u0026#34;: 0, # \u0026#34;PermissionsBoundaryUsageCount\u0026#34;: 0, # \u0026#34;IsAttachable\u0026#34;: true, # \u0026#34;CreateDate\u0026#34;: \u0026#34;2019-01-18T03:50:00Z\u0026#34;, # \u0026#34;UpdateDate\u0026#34;: \u0026#34;2019-01-18T03:50:00Z\u0026#34; # } # } cat \u0026lt;\u0026lt; EOF \u0026gt; kops-route53-policy.json { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;route53:ChangeResourceRecordSets\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:route53:::hostedzone/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53:ListResourceRecordSets\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ] } ] } EOF aws iam create-policy --policy-name kops-route53-policy --policy-document file://kops-route53-policy.json # OUTPUT: # { # \u0026#34;Policy\u0026#34;: { # \u0026#34;PolicyName\u0026#34;: \u0026#34;kops-route53-policy\u0026#34;, # \u0026#34;PolicyId\u0026#34;: \u0026#34;ANPAIEWAGN62HBYC7QOS2\u0026#34;, # \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::894527626897:policy/kops-route53-policy\u0026#34;, # \u0026#34;Path\u0026#34;: \u0026#34;/\u0026#34;, # \u0026#34;DefaultVersionId\u0026#34;: \u0026#34;v1\u0026#34;, # \u0026#34;AttachmentCount\u0026#34;: 0, # \u0026#34;PermissionsBoundaryUsageCount\u0026#34;: 0, # \u0026#34;IsAttachable\u0026#34;: true, # \u0026#34;CreateDate\u0026#34;: \u0026#34;2019-01-18T03:15:37Z\u0026#34;, # \u0026#34;UpdateDate\u0026#34;: \u0026#34;2019-01-18T03:15:37Z\u0026#34; # } # } Note that even we just created these kops policies for alb and route53 we cannot add them right now, we need to first create the cluster, you can skip them if you don\u0026rsquo;t plan on using these resources.\nNow we will also export or set the cluster name and kops state store as environment variables export NAME=k8s.techsquad.rocks export KOPS_STATE_STORE=techsquad-cluster-state-store We will be using these in a few places, so to not repeat ourselves let\u0026rsquo;s better have it as variables.\nCreate the zone for the subdomain in Route53 ID=$(uuidgen) \u0026amp;\u0026amp; aws route53 create-hosted-zone --name ${NAME} --caller-reference $ID | jq .DelegationSet.NameServers # OUTPUT: # [ # \u0026#34;ns-848.awsdns-42.net\u0026#34;, # \u0026#34;ns-12.awsdns-01.com\u0026#34;, # \u0026#34;ns-1047.awsdns-02.org\u0026#34;, # \u0026#34;ns-1862.awsdns-40.co.uk\u0026#34; # ] As I\u0026rsquo;m already using this domain for the blog with github we can create a subdomain for it and add some NS records in our root zone for that subdomain, in this case k8s.techsquad.rocks. To make this easier I will show you how it should look like:   So with this change and our new zone in Route53 for the subdomain, we can freely manage it like if it was another domain, this means that everything that goes to *.k8s.techsquad.rocks will be handled by our Route53 zone.\nCreate a bucket to store the cluster state aws s3api create-bucket \\  --bucket ${KOPS_STATE_STORE} \\  --region us-east-1 # OUTPUT: # { # \u0026#34;Location\u0026#34;: \u0026#34;/techsquad-cluster-state-store\u0026#34; # } Note that bucket names are unique, so it\u0026rsquo;s always a good idea to prefix them with your domain name or something like that.\nSet the versioning on, in case we need to rollback at some point aws s3api put-bucket-versioning --bucket ${KOPS_STATE_STORE} --versioning-configuration Status=Enabled\nSet encryption on for the bucket aws s3api put-bucket-encryption --bucket ${KOPS_STATE_STORE} --server-side-encryption-configuration \u0026#39;{\u0026#34;Rules\u0026#34;:[{\u0026#34;ApplyServerSideEncryptionByDefault\u0026#34;:{\u0026#34;SSEAlgorithm\u0026#34;:\u0026#34;AES256\u0026#34;}}]}\u0026#39;\nAnd finally let\u0026rsquo;s create our cluster export KOPS_STATE_STORE=\u0026#34;s3://${KOPS_STATE_STORE}\u0026#34; kops create cluster \\  --zones us-east-1a \\  --networking calico \\  ${NAME} \\  --yes # OUTPUT: # I0117 23:14:06.449479 10314 create_cluster.go:1318] Using SSH public key: /home/kainlite/.ssh/id_rsa.pub # I0117 23:14:08.367862 10314 create_cluster.go:472] Inferred --cloud=aws from zone \u0026#34;us-east-1a\u0026#34; # I0117 23:14:09.736030 10314 subnets.go:184] Assigned CIDR 172.20.32.0/19 to subnet us-east-1a # W0117 23:14:18.049687 10314 firewall.go:249] Opening etcd port on masters for access from the nodes, for calico. This is unsafe in untrusted environments. # I0117 23:14:19.385541 10314 executor.go:91] Tasks: 0 done / 77 total; 34 can run # I0117 23:14:21.779681 10314 vfs_castore.go:731] Issuing new certificate: \u0026#34;apiserver-aggregator-ca\u0026#34; # I0117 23:14:21.940026 10314 vfs_castore.go:731] Issuing new certificate: \u0026#34;ca\u0026#34; # I0117 23:14:24.404810 10314 executor.go:91] Tasks: 34 done / 77 total; 24 can run # I0117 23:14:26.548234 10314 vfs_castore.go:731] Issuing new certificate: \u0026#34;master\u0026#34; # I0117 23:14:26.689470 10314 vfs_castore.go:731] Issuing new certificate: \u0026#34;apiserver-aggregator\u0026#34; # I0117 23:14:26.766563 10314 vfs_castore.go:731] Issuing new certificate: \u0026#34;kube-scheduler\u0026#34; # I0117 23:14:26.863562 10314 vfs_castore.go:731] Issuing new certificate: \u0026#34;kube-controller-manager\u0026#34; # I0117 23:14:26.955776 10314 vfs_castore.go:731] Issuing new certificate: \u0026#34;kubecfg\u0026#34; # I0117 23:14:26.972837 10314 vfs_castore.go:731] Issuing new certificate: \u0026#34;apiserver-proxy-client\u0026#34; # I0117 23:14:26.973239 10314 vfs_castore.go:731] Issuing new certificate: \u0026#34;kops\u0026#34; # I0117 23:14:27.055466 10314 vfs_castore.go:731] Issuing new certificate: \u0026#34;kubelet\u0026#34; # I0117 23:14:27.127778 10314 vfs_castore.go:731] Issuing new certificate: \u0026#34;kubelet-api\u0026#34; # I0117 23:14:27.570516 10314 vfs_castore.go:731] Issuing new certificate: \u0026#34;kube-proxy\u0026#34; # I0117 23:14:29.503168 10314 executor.go:91] Tasks: 58 done / 77 total; 17 can run # I0117 23:14:31.594404 10314 executor.go:91] Tasks: 75 done / 77 total; 2 can run # I0117 23:14:33.297131 10314 executor.go:91] Tasks: 77 done / 77 total; 0 can run # I0117 23:14:33.297168 10314 dns.go:153] Pre-creating DNS records # I0117 23:14:34.947302 10314 update_cluster.go:291] Exporting kubecfg for cluster # kops has set your kubectl context to k8s.techsquad.rocks # # Cluster is starting. It should be ready in a few minutes. # # Suggestions: # * validate cluster: kops validate cluster # * list nodes: kubectl get nodes --show-labels # * ssh to the master: ssh -i ~/.ssh/id_rsa admin@api.k8s.techsquad.rocks # * the admin user is specific to Debian. If not using Debian please use the appropriate user based on your OS. # * read about installing addons at: https://github.com/kubernetes/kops/blob/master/docs/addons.md. We set the KOPS_STATE_STORE to a valid S3 url for kops, and then created the cluster, this will set kubectl context to our new cluster, we might need to wait a few minutes before being able to use it, but before doing anything let\u0026rsquo;s validate that\u0026rsquo;s up and ready.\nkops validate cluster ${NAME} # OUTPUT: # Using cluster from kubectl context: k8s.techsquad.rocks # # Validating cluster k8s.techsquad.rocks # # INSTANCE GROUPS # NAME ROLE MACHINETYPE MIN MAX SUBNETS # master-us-east-1a Master m3.medium 1 1 us-east-1a # nodes Node t2.medium 2 2 us-east-1a # # NODE STATUS # NAME ROLE READY # ip-172-20-39-123.ec2.internal node True # ip-172-20-52-65.ec2.internal node True # ip-172-20-61-51.ec2.internal master True # # Your cluster k8s.techsquad.rocks is ready The validation passed and we can see that our cluster is ready, it can take several minutes until the cluster is up and functional, in this case it took about 3-5 minutes.\nWe will create an additional subnet to satisfy our ALB: aws ec2 create-subnet --vpc-id vpc-06e2e104ad785474c --cidr-block 172.20.64.0/19 --availability-zone us-east-1b # OUTPUT: # { # \u0026#34;Subnet\u0026#34;: { # \u0026#34;AvailabilityZone\u0026#34;: \u0026#34;us-east-1b\u0026#34;, # \u0026#34;AvailableIpAddressCount\u0026#34;: 8187, # \u0026#34;CidrBlock\u0026#34;: \u0026#34;172.20.64.0/19\u0026#34;, # \u0026#34;DefaultForAz\u0026#34;: false, # \u0026#34;MapPublicIpOnLaunch\u0026#34;: false, # \u0026#34;State\u0026#34;: \u0026#34;pending\u0026#34;, # \u0026#34;SubnetId\u0026#34;: \u0026#34;subnet-017a5609ce6104e1b\u0026#34;, # \u0026#34;VpcId\u0026#34;: \u0026#34;vpc-06e2e104ad785474c\u0026#34;, # \u0026#34;AssignIpv6AddressOnCreation\u0026#34;: false, # \u0026#34;Ipv6CidrBlockAssociationSet\u0026#34;: [] # } # } aws ec2 create-tags --resources subnet-017a5609ce6104e1b --tags Key=KubernetesCluster,Value=k8s.techsquad.rocks aws ec2 create-tags --resources subnet-017a5609ce6104e1b --tags Key=Name,Value=us-east-1b.k8s.techsquad.rocks aws ec2 create-tags --resources subnet-017a5609ce6104e1b --tags Key=SubnetType,Value=Public aws ec2 create-tags --resources subnet-017a5609ce6104e1b --tags Key=kubernetes.io/cluster/k8s.techsquad.rocks,Value=owned aws ec2 create-tags --resources subnet-017a5609ce6104e1b --tags Key=kubernetes.io/role/elb,Value=1 Note that we applied some required tags for the controller, and created an extra subnet, in a HA setup this would not be necessary since kops would create it for us but this is a small testing/dev cluster, so we will need to do it manually.\nAnd lastly a security group for our ALB: aws ec2 create-security-group --group-name WebApps --description \u0026#34;Default web security group\u0026#34; --vpc-id vpc-06e2e104ad785474c # OUTPUT: # { # \u0026#34;GroupId\u0026#34;: \u0026#34;sg-09f0b1233696e65ef\u0026#34; # } aws ec2 authorize-security-group-ingress --group-id sg-09f0b1233696e65ef --protocol tcp --port 80 --cidr 0.0.0.0/0 aws ec2 authorize-security-group-ingress --group-id sg-057d2b0f6e288aa70 --protocol all --port 0 --source-group sg-09f0b1233696e65ef Note that this rule will open the port 80 to the world, you can add your ip or your VPN ips there if you want to restrict it, the second rule will allow the traffic from the load balancer to reach the nodes where our app is running.\nAws-alb-ingress-controller We will use Aws ALB Ingress Controller, to serve our web traffic, this will create an manage an ALB based in our ingress rules.\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.0.0/docs/examples/rbac-role.yaml clusterrole.rbac.authorization.k8s.io \u0026#34;alb-ingress-controller\u0026#34; created clusterrolebinding.rbac.authorization.k8s.io \u0026#34;alb-ingress-controller\u0026#34; created serviceaccount \u0026#34;alb-ingress\u0026#34; created Download the manifest and then modify the cluster-name to k8s.techsquad.rocks and a few other parameters, you can list the vpcs with aws ec2 describe-vpcs it will have some kops tags, so it\u0026rsquo;s easy to identify. curl -sS \u0026#34;https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.0.0/docs/examples/alb-ingress-controller.yaml\u0026#34; \u0026gt; alb-ingress-controller.yaml\nOr copy and paste the following lines: cat \u0026lt;\u0026lt; EOF \u0026gt; alb-ingress-controller.yaml # Application Load Balancer (ALB) Ingress Controller Deployment Manifest. # This manifest details sensible defaults for deploying an ALB Ingress Controller. # GitHub: https://github.com/kubernetes-sigs/aws-alb-ingress-controller apiVersion: apps/v1 kind: Deployment metadata: labels: app: alb-ingress-controller name: alb-ingress-controller # Namespace the ALB Ingress Controller should run in. Does not impact which # namespaces it\u0026#39;s able to resolve ingress resource for. For limiting ingress # namespace scope, see --watch-namespace. namespace: kube-system spec: replicas: 1 selector: matchLabels: app: alb-ingress-controller strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 1 type: RollingUpdate template: metadata: creationTimestamp: null labels: app: alb-ingress-controller spec: containers: - args: - -v=1 # Limit the namespace where this ALB Ingress Controller deployment will # resolve ingress resources. If left commented, all namespaces are used. # - --watch-namespace=your-k8s-namespace - --feature-gates=waf=false # Setting the ingress-class flag below ensures that only ingress resources with the # annotation kubernetes.io/ingress.class: \u0026#34;alb\u0026#34; are respected by the controller. You may # choose any class you\u0026#39;d like for this controller to respect. - --ingress-class=alb # Name of your cluster. Used when naming resources created # by the ALB Ingress Controller, providing distinction between # clusters. - --cluster-name=k8s.techsquad.rocks # AWS VPC ID this ingress controller will use to create AWS resources. # If unspecified, it will be discovered from ec2metadata. - --aws-vpc-id=vpc-06e2e104ad785474c # AWS region this ingress controller will operate in. # If unspecified, it will be discovered from ec2metadata. # List of regions: http://docs.aws.amazon.com/general/latest/gr/rande.html#vpc_region - --aws-region=us-east-1 # Enables logging on all outbound requests sent to the AWS API. # If logging is desired, set to true. # - ---aws-api-debug # Maximum number of times to retry the aws calls. # defaults to 10. # - --aws-max-retries=10 env: # AWS key id for authenticating with the AWS API. # This is only here for examples. It\u0026#39;s recommended you instead use # a project like kube2iam for granting access. #- name: AWS_ACCESS_KEY_ID # value: KEYVALUE # AWS key secret for authenticating with the AWS API. # This is only here for examples. It\u0026#39;s recommended you instead use # a project like kube2iam for granting access. #- name: AWS_SECRET_ACCESS_KEY # value: SECRETVALUE # Repository location of the ALB Ingress Controller. image: 894847497797.dkr.ecr.us-west-2.amazonaws.com/aws-alb-ingress-controller:v1.0.0 imagePullPolicy: Always name: server resources: {} terminationMessagePath: /dev/termination-log dnsPolicy: ClusterFirst restartPolicy: Always securityContext: {} terminationGracePeriodSeconds: 30 serviceAccountName: alb-ingress serviceAccount: alb-ingress EOF Note that I only modified the args section if you want to compare it with the original.\nThen finally apply it. kubectl apply -f alb-ingress-controller.yaml # OUTPUT: # deployment.apps \u0026#34;alb-ingress-controller\u0026#34; created\nExternal-dns External DNS will update our zone in Route53 based in the ingress rules as well, so everything will be done automatically for us once we add an ingress resource.\nBut first let\u0026rsquo;s attach those policies that we created before: aws iam attach-role-policy --policy-arn arn:aws:iam::894527626897:policy/kops-route53-policy --role-name nodes.k8s.techsquad.rocks aws iam attach-role-policy --policy-arn arn:aws:iam::894527626897:policy/kops-route53-policy --role-name masters.k8s.techsquad.rocks aws iam attach-role-policy --policy-arn arn:aws:iam::894527626897:policy/kops-alb-policy --role-name nodes.k8s.techsquad.rocks aws iam attach-role-policy --policy-arn arn:aws:iam::894527626897:policy/kops-alb-policy --role-name masters.k8s.techsquad.rocks Note that we just used the policies that we created before but we needed the cluster running because kops creates the roles nodes.k8s.techsquad.rocks and masters.k8s.techsquad.rocks, and this is needed for the aws-alb-ingress-controller and external-dns so these are able to do their job.\nWe need to download the manifests and modify a few parameters to match our deployment, the parameters are domain-filter and txt-owner-id, the rest is as is: curl -Ss https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.0/docs/examples/external-dns.yaml \u0026gt; external-dns.yaml This configuration will only update records, that\u0026rsquo;s the default policy (upsert), and it will only look for public hosted zones.\nOr copy and paste the following lines: cat \u0026lt;\u0026lt; EOF \u0026gt; external-dns.yaml apiVersion: v1 kind: ServiceAccount metadata: name: external-dns --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: external-dns rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;services\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;] - apiGroups: [\u0026#34;extensions\u0026#34;] resources: [\u0026#34;ingresses\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;nodes\u0026#34;] verbs: [\u0026#34;list\u0026#34;] --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: external-dns-viewer roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: external-dns subjects: - kind: ServiceAccount name: external-dns namespace: default --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: external-dns spec: strategy: type: Recreate template: metadata: labels: app: external-dns spec: serviceAccountName: external-dns containers: - name: external-dns image: registry.opensource.zalan.do/teapot/external-dns:v0.5.9 args: - --source=service - --source=ingress - --domain-filter=k8s.techsquad.rocks # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=aws - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both) - --registry=txt - --txt-owner-id=k8s.techsquad.rocks EOF\nAnd apply it: kubectl apply -f external-dns.yaml # OUTPUT: # serviceaccount \u0026#34;external-dns\u0026#34; unchanged # clusterrole.rbac.authorization.k8s.io \u0026#34;external-dns\u0026#34; configured # clusterrolebinding.rbac.authorization.k8s.io \u0026#34;external-dns-viewer\u0026#34; configured # deployment.extensions \u0026#34;external-dns\u0026#34; created\nValidate that we have everything that we installed up and running: kubectl get pods # OUTPUT: # NAME READY STATUS RESTARTS AGE # external-dns-7d7998f7bb-lb5kq 1/1 Running 0 2m kubectl get pods -n kube-system # OUTPUT: # NAME READY STATUS RESTARTS AGE # alb-ingress-controller-5885ddd5f9-9rsc8 1/1 Running 0 12m # calico-kube-controllers-f6bc47f75-n99tl 1/1 Running 0 27m # calico-node-4ps9c 2/2 Running 0 25m # calico-node-kjztv 2/2 Running 0 27m # calico-node-zs4fg 2/2 Running 0 25m # dns-controller-67f5c6b7bd-r67pl 1/1 Running 0 27m # etcd-server-events-ip-172-20-42-37.ec2.internal 1/1 Running 0 26m # etcd-server-ip-172-20-42-37.ec2.internal 1/1 Running 0 26m # kube-apiserver-ip-172-20-42-37.ec2.internal 1/1 Running 0 27m # kube-controller-manager-ip-172-20-42-37.ec2.internal 1/1 Running 0 26m # kube-dns-756bfc7fdf-2kzjs 3/3 Running 0 24m # kube-dns-756bfc7fdf-rq5nd 3/3 Running 0 27m # kube-dns-autoscaler-787d59df8f-c2d52 1/1 Running 0 27m # kube-proxy-ip-172-20-42-109.ec2.internal 1/1 Running 0 25m # kube-proxy-ip-172-20-42-37.ec2.internal 1/1 Running 0 26m # kube-proxy-ip-172-20-54-175.ec2.internal 1/1 Running 0 25m # kube-scheduler-ip-172-20-42-37.ec2.internal 1/1 Running 0 26m We can see that alb-ingress-controller is running, also external-dns, and everything looks good and healthy, time to test it with a deployment.\nTesting everything kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.0.0/docs/examples/2048/2048-namespace.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.0.0/docs/examples/2048/2048-deployment.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.0.0/docs/examples/2048/2048-service.yaml # OUTPUT: # namespace \u0026#34;2048-game\u0026#34; created # deployment.extensions \u0026#34;2048-deployment\u0026#34; created # service \u0026#34;service-2048\u0026#34; created We need to download and edit the ingress resource to make it use our domain so we can then see the record pointing to the ALB. curl -Ss https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.0.0/docs/examples/2048/2048-ingress.yaml \u0026gt; 2048-ingress.yaml\nOr just copy and paste the next snippet. cat \u0026lt;\u0026lt; EOF \u0026gt; 2048-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: \u0026#34;2048-ingress\u0026#34; namespace: \u0026#34;2048-game\u0026#34; annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: instance alb.ingress.kubernetes.io/subnets: subnet-017a5609ce6104e1b, subnet-060e6d3c3d3c2b34a alb.ingress.kubernetes.io/security-groups: sg-09f0b1233696e65ef # You can check all the alternatives here: # https://github.com/riccardofreixo/alb-ingress-controller/blob/master/docs/ingress-resources.md labels: app: 2048-ingress spec: rules: - host: 2048.k8s.techsquad.rocks http: paths: - backend: serviceName: \u0026#34;service-2048\u0026#34; servicePort: 80 path: /* EOF You can use aws ec2 describe-subnets, to find the first subnet id, this subnet already has some tags that we need in order to make it work, for example: kubernetes.io/role/elb: 1, and the second subnet is the one that we created manually and applied the same tags.\nAnd finally apply it: kubectl apply -f 2048-ingress.yaml # OUTPUT: # ingress.extensions \u0026#34;2048-ingress\u0026#34; created Wait a few moments and verify.\nResults The ALB  The DNS records  And the app  Clean up Remember this is not free, and if you don\u0026rsquo;t want to get charged after you\u0026rsquo;re done testing just shutdown and delete everything. kubectl delete -f 2048-ingress.yaml aws iam detach-role-policy --policy-arn arn:aws:iam::894527626897:policy/kops-route53-policy --role-name nodes.k8s.techsquad.rocks aws iam detach-role-policy --policy-arn arn:aws:iam::894527626897:policy/kops-route53-policy --role-name masters.k8s.techsquad.rocks aws iam detach-role-policy --policy-arn arn:aws:iam::894527626897:policy/kops-alb-policy --role-name nodes.k8s.techsquad.rocks aws iam detach-role-policy --policy-arn arn:aws:iam::894527626897:policy/kops-alb-policy --role-name masters.k8s.techsquad.rocks kops delete cluster ${NAME} --yes # OUTPUT: # ... # Deleted kubectl config for k8s.techsquad.rocks # # Deleted cluster: \u0026#34;k8s.techsquad.rocks\u0026#34; This command is really verbose, so I skipped it to the end, be aware that in order to delete the cluster with kops you first need to detach the additionally attached privileges. Also be careful to delete first the ingress resources so the ALB gets removed before you delete the cluster, or you will have an ALB laying around afterwards. You can re-run it if it gets stuck and cannot delete any resource.\nNotes  I was going to use helm and deploy a more complex application here, but the article was already too long, so I decided to go with the aws alb ingress controller example. If something doesn\u0026rsquo;t go well or things aren\u0026rsquo;t happening you can always check the logs for external-dns and aws-alb-ingress-controller, the messages are usually very descriptive and easy to understand. For an ALB you need two subnets in two different AZs beforehand. If you are going to use ALBs, have in mind that it will create an ALB for each deployment, there is a small project that merges everything into one ALB but you need to have a unified or consolidated way to do health checks or or some of the apps will fail and the ALB will return a 502, the project can be found here. Documenting what you do and how you do it (Also keeping the documentation updated is really important), not only will help the future you (Yes, you can thank your past self when reading and old doc), but also it will make it easier to share the knowledge and purpose of whatever you are implementing with your team. I spent 3 bucks with all the instances and dns zones, etc during this tutorial in case you are interested :). Notes I also removed all $ from the code blocks and added the output of the commands with # OUTPUT:, let me know if this is clear and easy to read, or if you have any suggestion.  Errata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":24,"section":"blog","summary":"Introduction In this article we will create a cluster from scratch with kops (K8s installation, upgrades and management) in AWS, We will configure aws-alb-ingress-controller (External traffic into our services/pods) and external dns (Update the records based in the ingress rules) and also learn a bit about awscli in the process.\nBasically we will have a fully functional cluster that will be able to handle public traffic in minutes, first we will install the cluster with kops, then we will enable the ingress controller and lastly external-dns, then we will deploy a basic app to test that everything works fine, SSL/TLS is out of the scope but it\u0026rsquo;s fairly easy to implement if you are using ACM.","tags":["AWS","kops","kubernetes"],"title":"From zero to hero with kops and AWS","uri":"https://techsquad.rocks/blog/from_zero_to_hero_with_kops_and_aws/","year":"2019"},{"content":"Introduction This article is about my current configuration, but I\u0026rsquo;m going to talk only about the terminal and my text editor because those will work in any linux distribution, I\u0026rsquo;m currently using Arch Linux and AwesomeWM (I used Gnome Shell previously, and Gnome 2 before that), you can find my dotfiles here with all my configurations.\nWhile my terminal doesn\u0026rsquo;t look exactly like the one from the picture you can get something like that with GBT.\nTerminal My current terminal is rxvt-unicode and I\u0026rsquo;m pretty happy with it, it\u0026rsquo;s relatively easy to configure and use, it looks like this:  And the configuration file can be found here, note that even if you don\u0026rsquo;t like Ponys by any reason, it\u0026rsquo;s useful to test colors in the terminal.\nIt\u0026rsquo;s different than other terminals I have tried in the way it manages and uses the configuration, it uses an additional tool called xrdb (X server resource database utility) to manage the configuration provided in the configuration file .Xresources. # Loads the configuration from Xresources in xrdb $ xrdb -merge .Xresources # List the current configuration $ xrdb -query # Deletes the current database $ xrdb -remove\nTheme My current theme is gruvbox in Vim and also in my terminal, and changing from solazired to it is what inspired this small article.\nTmux I also use tmux to maintan sessions, some of it\u0026rsquo;s nice features are tiling, tabs. The configuration can be found here. I move between tabs with control-h and control-l, and between panes with control-a [hjkl].\nVim As my text editor I really like and enjoy using Vim, there is always something to learn but once you make some good habits it pays off in the way you write and move around, you can check some amazing screencasts on vim here and also the book Practical Vim can be really helpful to get started and/or improve your current vim-fu.\nAs a plugin manager I use Plug even that it\u0026rsquo;s not really necessary with Vim 8, but that is a matter of taste I guess. You can see my full vim configuration here.\nIt looks something like this, as you can see I have a small tmux pane in the bottom with Hugo compiling the site after every save and live reloading it in my browser:  Notes  I\u0026rsquo;m also using zsh and oh-my-zsh with the theme agnoster. I really like zsh it\u0026rsquo;s fast and has some nice features like autocomplete everywhere, but again this is a matter of taste. I like to take advantage of all the space in the screen, that\u0026rsquo;s why AwesomeWM fits great (even that I do not use the tiling feature a lot, tabs and full screen apps), with some minor configuration I\u0026rsquo;m able to do everything from the keyboard, I use the mouse when checking emails and things like that but otherwise the keyboard is more than enough. I used cowsay and ponysay in the first screenshot so you can have an idea of how the terminal looks like. If you are going to use unicode I recommend you to install the fonts from nerd-fonts.  Errata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":25,"section":"blog","summary":"Introduction This article is about my current configuration, but I\u0026rsquo;m going to talk only about the terminal and my text editor because those will work in any linux distribution, I\u0026rsquo;m currently using Arch Linux and AwesomeWM (I used Gnome Shell previously, and Gnome 2 before that), you can find my dotfiles here with all my configurations.\nWhile my terminal doesn\u0026rsquo;t look exactly like the one from the picture you can get something like that with GBT.","tags":["urxvt","vim","linux","tmux"],"title":"My local environment","uri":"https://techsquad.rocks/blog/my_local_environment/","year":"2019"},{"content":"Introduction This article builds up on what we did in the last article, so refer to that one before starting this one, if you are planing to follow the documentation examples you will find many similarities since I based this article on that.\nIn this example I will be using Digital Ocean (that\u0026rsquo;s my referral link), note that I do not have any association with Digital Ocean but they give you $100 to test their products for 60 days, if you spend $25 I get another $25.\nBefore starting a few concepts  A VirtualService defines the rules that control how requests for a service are routed within an Istio service mesh. A DestinationRule configures the set of policies to be applied to a request after VirtualService routing has occurred. A ServiceEntry is commonly used to enable requests to services outside of an Istio service mesh. A Gateway configures a load balancer for HTTP/TCP traffic, most commonly operating at the edge of the mesh to enable ingress traffic for an application. These basic concepts will help you understand the manifest that we are going to see.  Let\u0026rsquo;s get started We already have the bookinfo project deployed and using all three versions of the service (ratings) but we will need to make some changes to test route based on user identity, you can check the configuration with: $ kubectl get destinationrules -o yaml apiVersion: v1 items: - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;networking.istio.io/v1alpha3\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;DestinationRule\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;details\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;host\u0026#34;:\u0026#34;details\u0026#34;,\u0026#34;subsets\u0026#34;:[{\u0026#34;labels\u0026#34;:{\u0026#34;version\u0026#34;:\u0026#34;v1\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;v1\u0026#34;},{\u0026#34;labels\u0026#34;:{\u0026#34;version\u0026#34;:\u0026#34;v2\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;v2\u0026#34;}]}} creationTimestamp: 2019-01-11T00:58:54Z generation: 1 name: details namespace: default resourceVersion: \u0026#34;921688\u0026#34; selfLink: /apis/networking.istio.io/v1alpha3/namespaces/default/destinationrules/details uid: 11490656-153c-11e9-9eda-6a85233ec1d5 spec: host: details subsets: - labels: version: v1 name: v1 - labels: version: v2 name: v2 - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;networking.istio.io/v1alpha3\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;DestinationRule\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;productpage\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;host\u0026#34;:\u0026#34;productpage\u0026#34;,\u0026#34;subsets\u0026#34;:[{\u0026#34;labels\u0026#34;:{\u0026#34;version\u0026#34;:\u0026#34;v1\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;v1\u0026#34;}]}} creationTimestamp: 2019-01-11T00:58:53Z generation: 1 name: productpage namespace: default resourceVersion: \u0026#34;921684\u0026#34; selfLink: /apis/networking.istio.io/v1alpha3/namespaces/default/destinationrules/productpage uid: 10a42a24-153c-11e9-9eda-6a85233ec1d5 spec: host: productpage subsets: - labels: version: v1 name: v1 - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;networking.istio.io/v1alpha3\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;DestinationRule\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;ratings\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;host\u0026#34;:\u0026#34;ratings\u0026#34;,\u0026#34;subsets\u0026#34;:[{\u0026#34;labels\u0026#34;:{\u0026#34;version\u0026#34;:\u0026#34;v1\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;v1\u0026#34;},{\u0026#34;labels\u0026#34;:{\u0026#34;version\u0026#34;:\u0026#34;v2\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;v2\u0026#34;},{\u0026#34;labels\u0026#34;:{\u0026#34;version\u0026#34;:\u0026#34;v2-mysql\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;v2-mysql\u0026#34;},{\u0026#34;labels\u0026#34;:{\u0026#34;version\u0026#34;:\u0026#34;v2-mysql-vm\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;v2-mysql-vm\u0026#34;}]}} creationTimestamp: 2019-01-11T00:58:54Z generation: 1 name: ratings namespace: default resourceVersion: \u0026#34;921686\u0026#34; selfLink: /apis/networking.istio.io/v1alpha3/namespaces/default/destinationrules/ratings uid: 111299e1-153c-11e9-9eda-6a85233ec1d5 spec: host: ratings subsets: - labels: version: v1 name: v1 - labels: version: v2 name: v2 - labels: version: v2-mysql name: v2-mysql - labels: version: v2-mysql-vm name: v2-mysql-vm - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;networking.istio.io/v1alpha3\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;DestinationRule\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;reviews\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;host\u0026#34;:\u0026#34;reviews\u0026#34;,\u0026#34;subsets\u0026#34;:[{\u0026#34;labels\u0026#34;:{\u0026#34;version\u0026#34;:\u0026#34;v1\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;v1\u0026#34;},{\u0026#34;labels\u0026#34;:{\u0026#34;version\u0026#34;:\u0026#34;v2\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;v2\u0026#34;},{\u0026#34;labels\u0026#34;:{\u0026#34;version\u0026#34;:\u0026#34;v3\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;v3\u0026#34;}]}} creationTimestamp: 2019-01-11T00:58:53Z generation: 1 name: reviews namespace: default resourceVersion: \u0026#34;921685\u0026#34; selfLink: /apis/networking.istio.io/v1alpha3/namespaces/default/destinationrules/reviews uid: 10db9ee2-153c-11e9-9eda-6a85233ec1d5 spec: host: reviews subsets: - labels: version: v1 name: v1 - labels: version: v2 name: v2 - labels: version: v3 name: v3 kind: List metadata: resourceVersion: \u0026#34;\u0026#34; selfLink: \u0026#34;\u0026#34; There we have all the destination rules, and now we need to apply the new manifest that will send everything to the version 1 and the user jason to the version 2 of the reviews microservice.\nistio-1.0.5/samples/bookinfo $ kubectl apply -f networking/virtual-service-reviews-test-v2.yaml virtualservice.networking.istio.io \u0026#34;reviews\u0026#34; created $ kubectl get virtualservice reviews -o yaml apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;networking.istio.io/v1alpha3\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;VirtualService\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;reviews\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;hosts\u0026#34;:[\u0026#34;reviews\u0026#34;],\u0026#34;http\u0026#34;:[{\u0026#34;match\u0026#34;:[{\u0026#34;headers\u0026#34;:{\u0026#34;end-user\u0026#34;:{\u0026#34;exact\u0026#34;:\u0026#34;jason\u0026#34;}}}],\u0026#34;route\u0026#34;:[{\u0026#34;destination\u0026#34;:{\u0026#34;host\u0026#34;:\u0026#34;reviews\u0026#34;,\u0026#34;subset\u0026#34;:\u0026#34;v2\u0026#34;}}]},{\u0026#34;route\u0026#34;:[{\u0026#34;destination\u0026#34;:{\u0026#34;host\u0026#34;:\u0026#34;reviews\u0026#34;,\u0026#34;subset\u0026#34;:\u0026#34;v1\u0026#34;}}]}]}} creationTimestamp: 2019-01-11T02:30:35Z generation: 1 name: reviews namespace: default resourceVersion: \u0026#34;930577\u0026#34; selfLink: /apis/networking.istio.io/v1alpha3/namespaces/default/virtualservices/reviews uid: e0701f0d-1548-11e9-9eda-6a85233ec1d5 spec: hosts: - reviews http: - match: - headers: end-user: exact: jason route: - destination: host: reviews subset: v2 - route: - destination: host: reviews subset: v1 What\u0026rsquo;s going on here, how Istio knows what user is logged in?, well, the app adds a header called end-user and value jason then the route will be used, it\u0026rsquo;s a nifty trick.\nNot jason:  jason:  As you can see the difference in the v1 and v2 of the app are the stars below the reviews, but that is more than enough to indicate that it works, this is really nice for beta testers you don\u0026rsquo;t need or have to complicate your code but just add a header.\nInjecting an HTTP abort fault: This time we will inject a failure for our friend jason: istio-1.0.5/samples/bookinfo $ kubectl apply -f networking/virtual-service-ratings-test-abort.yaml virtualservice.networking.istio.io \u0026#34;ratings\u0026#34; created $ kubectl get virtualservice ratings -o yaml apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;networking.istio.io/v1alpha3\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;VirtualService\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;ratings\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;hosts\u0026#34;:[\u0026#34;ratings\u0026#34;],\u0026#34;http\u0026#34;:[{\u0026#34;fault\u0026#34;:{\u0026#34;abort\u0026#34;:{\u0026#34;httpStatus\u0026#34;:500,\u0026#34;percent\u0026#34;:100}},\u0026#34;match\u0026#34;:[{\u0026#34;headers\u0026#34;:{\u0026#34;end-user\u0026#34;:{\u0026#34;exact\u0026#34;:\u0026#34;jason\u0026#34;}}}],\u0026#34;route\u0026#34;:[{\u0026#34;destination\u0026#34;:{\u0026#34;host\u0026#34;:\u0026#34;ratings\u0026#34;,\u0026#34;subset\u0026#34;:\u0026#34;v1\u0026#34;}}]},{\u0026#34;route\u0026#34;:[{\u0026#34;destination\u0026#34;:{\u0026#34;host\u0026#34;:\u0026#34;ratings\u0026#34;,\u0026#34;subset\u0026#34;:\u0026#34;v1\u0026#34;}}]}]}} creationTimestamp: 2019-01-11T02:50:59Z generation: 1 name: ratings namespace: default resourceVersion: \u0026#34;932552\u0026#34; selfLink: /apis/networking.istio.io/v1alpha3/namespaces/default/virtualservices/ratings uid: b98799b0-154b-11e9-9eda-6a85233ec1d5 spec: hosts: - ratings http: - fault: abort: httpStatus: 500 percent: 100 match: - headers: end-user: exact: jason route: - destination: host: ratings subset: v1 - route: - destination: host: ratings subset: v1\nSo he decided to check the book reviews again and boom, the ratings service was not available but everything else works just fine, this only applies for jason everyone else will see the version without stars or the error message.  Notes Istio seems an it is indeed really powerful, there many more features like:\n Traffic shifting. Requests timeouts. Circuit breaking. Mirroring. And a lot more.  I left aside Policies, Telemetry and Security, if you want to learn more about Istio I highly recommend you to try the examples yourself and read on the official documentation.\nI also spent some time improving the navigation of the blog and some other minor details, but I wanted to keep the articles going so that\u0026rsquo;s why this one is so simple and similar to the documentation.\nUpcoming topics and ideas I Want to start creating series of content on different topics, brief articles that can get you started with some new technology or maybe give you an idea of how it works, let me know if you are interested in that kind of content in the comments or via twitter 🐦 (it\u0026rsquo;s a bird, in case you cannot see unicode characters).\nErrata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":26,"section":"blog","summary":"Introduction This article builds up on what we did in the last article, so refer to that one before starting this one, if you are planing to follow the documentation examples you will find many similarities since I based this article on that.\nIn this example I will be using Digital Ocean (that\u0026rsquo;s my referral link), note that I do not have any association with Digital Ocean but they give you $100 to test their products for 60 days, if you spend $25 I get another $25.","tags":["istio","routing","service-mesh","kubernetes"],"title":"Exploring some Istio features","uri":"https://techsquad.rocks/blog/exploring_some_istio_features/","year":"2019"},{"content":"Introduction This time we will see how to get started with Istio and why do we need to use a service mesh.\nIn this example I will be using Digital Ocean (that\u0026rsquo;s my referral link), note that I do not have any association with Digital Ocean but they give you $100 to test their products for 60 days, if you spend $25 I get another $25.\nIstio So\u0026hellip; You might be wondering some of those questions: why Istio? Why do I need a service mesh?, when do I need that? And I want to help you with some answers:\nWhy do I need a service mesh? Basically because in cloud environments you cannot trust that the network will be reliable 100% of the time, that the latency will be low, that the network is secure and the bandwidth is infinite, the service mesh is just an extra layer to help microservices communicate with each other safely and reliably.\nWhen do I need to have one? This one can be tricky and will depend on your environment, but the moment that you start experiencing network issues between your microservices would be a good moment to do it, it could be done before of course, but it will highly depend on the project, if you can start early with it the better and easier to implement will be, always have in mind the benefits of added security, observability and likely performance improvement.\nWhy Istio? This will be a small series of service meshes for kubernetes and I decided to start with Istio.\nIn case you don\u0026rsquo;t agree with my explanations that\u0026rsquo;s ok, this is a TL;DR version and also I simplified things a lot, for a more complete overview you can check this article or this one.\nLet\u0026rsquo;s get started First of all we need to download and install Istio in our cluster, the recommended way of doing it is using helm (In this case I will be using the no Tiller alternative, but it could be done with helm install as well, check here for more info): $ curl -L https://git.io/getLatestIstio | sh - This will download and extract the latest release, in this case 1.0.5 at this moment.\nSo let\u0026rsquo;s install Istio\u0026hellip; only pay attention to the first 3 commands, then you can skip until the end of the code block, I post all the output because I like full examples :) istio-1.0.5 $ helm template install/kubernetes/helm/istio --name istio --namespace istio-system --set grafana.enabled=true \u0026gt; $HOME/istio.yaml istio-1.0.5 $ kubectl create namespace istio-system namespace \u0026#34;istio-system\u0026#34; created istio-1.0.5 $ kubectl apply -f $HOME/istio.yaml configmap \u0026#34;istio-galley-configuration\u0026#34; created configmap \u0026#34;istio-statsd-prom-bridge\u0026#34; created configmap \u0026#34;prometheus\u0026#34; created configmap \u0026#34;istio-security-custom-resources\u0026#34; created configmap \u0026#34;istio\u0026#34; created configmap \u0026#34;istio-sidecar-injector\u0026#34; created serviceaccount \u0026#34;istio-galley-service-account\u0026#34; created serviceaccount \u0026#34;istio-egressgateway-service-account\u0026#34; created serviceaccount \u0026#34;istio-ingressgateway-service-account\u0026#34; created serviceaccount \u0026#34;istio-mixer-service-account\u0026#34; created serviceaccount \u0026#34;istio-pilot-service-account\u0026#34; created serviceaccount \u0026#34;prometheus\u0026#34; created serviceaccount \u0026#34;istio-cleanup-secrets-service-account\u0026#34; created clusterrole.rbac.authorization.k8s.io \u0026#34;istio-cleanup-secrets-istio-system\u0026#34; created clusterrolebinding.rbac.authorization.k8s.io \u0026#34;istio-cleanup-secrets-istio-system\u0026#34; created job.batch \u0026#34;istio-cleanup-secrets\u0026#34; created serviceaccount \u0026#34;istio-security-post-install-account\u0026#34; created clusterrole.rbac.authorization.k8s.io \u0026#34;istio-security-post-install-istio-system\u0026#34; created clusterrolebinding.rbac.authorization.k8s.io \u0026#34;istio-security-post-install-role-binding-istio-system\u0026#34; created job.batch \u0026#34;istio-security-post-install\u0026#34; created serviceaccount \u0026#34;istio-citadel-service-account\u0026#34; created serviceaccount \u0026#34;istio-sidecar-injector-service-account\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;virtualservices.networking.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;destinationrules.networking.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;serviceentries.networking.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;gateways.networking.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;envoyfilters.networking.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;httpapispecbindings.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;httpapispecs.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;quotaspecbindings.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;quotaspecs.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;rules.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;attributemanifests.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;bypasses.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;circonuses.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;deniers.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;fluentds.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;kubernetesenvs.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;listcheckers.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;memquotas.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;noops.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;opas.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;prometheuses.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;rbacs.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;redisquotas.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;servicecontrols.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;signalfxs.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;solarwindses.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;stackdrivers.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;statsds.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;stdios.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;apikeys.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;authorizations.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;checknothings.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;kuberneteses.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;listentries.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;logentries.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;edges.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;metrics.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;quotas.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;reportnothings.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;servicecontrolreports.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;tracespans.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;rbacconfigs.rbac.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;serviceroles.rbac.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;servicerolebindings.rbac.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;adapters.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;instances.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;templates.config.istio.io\u0026#34; created customresourcedefinition.apiextensions.k8s.io \u0026#34;handlers.config.istio.io\u0026#34; created clusterrole.rbac.authorization.k8s.io \u0026#34;istio-galley-istio-system\u0026#34; created clusterrole.rbac.authorization.k8s.io \u0026#34;istio-egressgateway-istio-system\u0026#34; created clusterrole.rbac.authorization.k8s.io \u0026#34;istio-ingressgateway-istio-system\u0026#34; created clusterrole.rbac.authorization.k8s.io \u0026#34;istio-mixer-istio-system\u0026#34; created clusterrole.rbac.authorization.k8s.io \u0026#34;istio-pilot-istio-system\u0026#34; created clusterrole.rbac.authorization.k8s.io \u0026#34;prometheus-istio-system\u0026#34; created clusterrole.rbac.authorization.k8s.io \u0026#34;istio-citadel-istio-system\u0026#34; created clusterrole.rbac.authorization.k8s.io \u0026#34;istio-sidecar-injector-istio-system\u0026#34; created clusterrolebinding.rbac.authorization.k8s.io \u0026#34;istio-galley-admin-role-binding-istio-system\u0026#34; created clusterrolebinding.rbac.authorization.k8s.io \u0026#34;istio-egressgateway-istio-system\u0026#34; created clusterrolebinding.rbac.authorization.k8s.io \u0026#34;istio-ingressgateway-istio-system\u0026#34; created clusterrolebinding.rbac.authorization.k8s.io \u0026#34;istio-mixer-admin-role-binding-istio-system\u0026#34; created clusterrolebinding.rbac.authorization.k8s.io \u0026#34;istio-pilot-istio-system\u0026#34; created clusterrolebinding.rbac.authorization.k8s.io \u0026#34;prometheus-istio-system\u0026#34; created clusterrolebinding.rbac.authorization.k8s.io \u0026#34;istio-citadel-istio-system\u0026#34; created clusterrolebinding.rbac.authorization.k8s.io \u0026#34;istio-sidecar-injector-admin-role-binding-istio-system\u0026#34; created service \u0026#34;istio-galley\u0026#34; created service \u0026#34;istio-egressgateway\u0026#34; created service \u0026#34;istio-ingressgateway\u0026#34; created service \u0026#34;istio-policy\u0026#34; created service \u0026#34;istio-telemetry\u0026#34; created service \u0026#34;istio-pilot\u0026#34; created service \u0026#34;prometheus\u0026#34; created service \u0026#34;istio-citadel\u0026#34; created service \u0026#34;istio-sidecar-injector\u0026#34; created deployment.extensions \u0026#34;istio-galley\u0026#34; created deployment.extensions \u0026#34;istio-egressgateway\u0026#34; created deployment.extensions \u0026#34;istio-ingressgateway\u0026#34; created deployment.extensions \u0026#34;istio-policy\u0026#34; created deployment.extensions \u0026#34;istio-telemetry\u0026#34; created deployment.extensions \u0026#34;istio-pilot\u0026#34; created deployment.extensions \u0026#34;prometheus\u0026#34; created deployment.extensions \u0026#34;istio-citadel\u0026#34; created deployment.extensions \u0026#34;istio-sidecar-injector\u0026#34; created gateway.networking.istio.io \u0026#34;istio-autogenerated-k8s-ingress\u0026#34; created horizontalpodautoscaler.autoscaling \u0026#34;istio-egressgateway\u0026#34; created horizontalpodautoscaler.autoscaling \u0026#34;istio-ingressgateway\u0026#34; created horizontalpodautoscaler.autoscaling \u0026#34;istio-policy\u0026#34; created horizontalpodautoscaler.autoscaling \u0026#34;istio-telemetry\u0026#34; created horizontalpodautoscaler.autoscaling \u0026#34;istio-pilot\u0026#34; created mutatingwebhookconfiguration.admissionregistration.k8s.io \u0026#34;istio-sidecar-injector\u0026#34; created attributemanifest.config.istio.io \u0026#34;istioproxy\u0026#34; created attributemanifest.config.istio.io \u0026#34;kubernetes\u0026#34; created stdio.config.istio.io \u0026#34;handler\u0026#34; created logentry.config.istio.io \u0026#34;accesslog\u0026#34; created logentry.config.istio.io \u0026#34;tcpaccesslog\u0026#34; created rule.config.istio.io \u0026#34;stdio\u0026#34; created rule.config.istio.io \u0026#34;stdiotcp\u0026#34; created metric.config.istio.io \u0026#34;requestcount\u0026#34; created metric.config.istio.io \u0026#34;requestduration\u0026#34; created metric.config.istio.io \u0026#34;requestsize\u0026#34; created metric.config.istio.io \u0026#34;responsesize\u0026#34; created metric.config.istio.io \u0026#34;tcpbytesent\u0026#34; created metric.config.istio.io \u0026#34;tcpbytereceived\u0026#34; created prometheus.config.istio.io \u0026#34;handler\u0026#34; created rule.config.istio.io \u0026#34;promhttp\u0026#34; created rule.config.istio.io \u0026#34;promtcp\u0026#34; created kubernetesenv.config.istio.io \u0026#34;handler\u0026#34; created rule.config.istio.io \u0026#34;kubeattrgenrulerule\u0026#34; created rule.config.istio.io \u0026#34;tcpkubeattrgenrulerule\u0026#34; created kubernetes.config.istio.io \u0026#34;attributes\u0026#34; created destinationrule.networking.istio.io \u0026#34;istio-policy\u0026#34; created destinationrule.networking.istio.io \u0026#34;istio-telemetry\u0026#34; created WOAH, What did just happen?, a lot of new resources were created, basically we just generated the manifest from the helm chart and applied that to our cluster.\nSo lets see what\u0026rsquo;s running and what that means: $ kubectl get pods -n istio-system NAME READY STATUS RESTARTS AGE istio-citadel-856f994c58-l96p8 1/1 Running 0 3m istio-cleanup-secrets-xqqj4 0/1 Completed 0 3m istio-egressgateway-5649fcf57-7zwkh 1/1 Running 0 3m istio-galley-7665f65c9c-tzn7d 1/1 Running 0 3m istio-ingressgateway-6755b9bbf6-bh84r 1/1 Running 0 3m istio-pilot-56855d999b-c4cp5 2/2 Running 0 3m istio-policy-6fcb6d655f-9544z 2/2 Running 0 3m istio-sidecar-injector-768c79f7bf-th8zh 1/1 Running 0 3m istio-telemetry-664d896cf5-jdcwv 2/2 Running 0 3m prometheus-76b7745b64-f8jxn 1/1 Running 0 3m A few minutes later, almost everything is up, but what\u0026rsquo;s all that? Istio has several components, see the following overview extracted from github.\nEnvoy: Sidecar proxies per microservice to handle ingress/egress traffic between services in the cluster and from a service to external services. The proxies form a secure microservice mesh providing a rich set of functions like discovery, rich layer-7 routing, circuit breakers, policy enforcement and telemetry recording/reporting functions. Note: The service mesh is not an overlay network. It simplifies and enhances how microservices in an application talk to each other over the network provided by the underlying platform.\nMixer: Central component that is leveraged by the proxies and microservices to enforce policies such as authorization, rate limits, quotas, authentication, request tracing and telemetry collection.\nPilot: A component responsible for configuring the proxies at runtime.\nCitadel: A centralized component responsible for certificate issuance and rotation.\nNode Agent: A per-node component responsible for certificate issuance and rotation.\nGalley: Central component for validating, ingesting, aggregating, transforming and distributing config within Istio.\nOk so, a lot of new things were installed but how do I know it\u0026rsquo;s working? let\u0026rsquo;s deploy a test application and check it: $ export PATH=\u0026#34;$PATH:~/istio-1.0.5/bin\u0026#34; istio-1.0.5/samples/bookinfo $ kubectl apply -f \u0026lt;(istioctl kube-inject -f platform/kube/bookinfo.yaml) service \u0026#34;details\u0026#34; created deployment.extensions \u0026#34;details-v1\u0026#34; created service \u0026#34;ratings\u0026#34; created deployment.extensions \u0026#34;ratings-v1\u0026#34; created service \u0026#34;reviews\u0026#34; created deployment.extensions \u0026#34;reviews-v1\u0026#34; created deployment.extensions \u0026#34;reviews-v2\u0026#34; created deployment.extensions \u0026#34;reviews-v3\u0026#34; created service \u0026#34;productpage\u0026#34; created deployment.extensions \u0026#34;productpage-v1\u0026#34; created That command not only deployed the application but injected the Istio sidecar to each pod: $ kubectl get pods NAME READY STATUS RESTARTS AGE details-v1-8bd954dbb-zhrqq 2/2 Running 0 2m productpage-v1-849c786f96-kpfx9 2/2 Running 0 2m ratings-v1-68d648d6fd-w68qb 2/2 Running 0 2m reviews-v1-b4c984bdc-9s6j5 2/2 Running 0 2m reviews-v2-575446d5db-r6kwc 2/2 Running 0 2m reviews-v3-74458c4889-kr4wb 2/2 Running 0 2m As we can see each pod has 2 containers in it, the app container and istio-proxy. You can also configure automatic sidecar injection.\nAlso all services are running: $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE details ClusterIP 10.245.134.179 \u0026lt;none\u0026gt; 9080/TCP 3m kubernetes ClusterIP 10.245.0.1 \u0026lt;none\u0026gt; 443/TCP 3d productpage ClusterIP 10.245.32.221 \u0026lt;none\u0026gt; 9080/TCP 3m ratings ClusterIP 10.245.159.112 \u0026lt;none\u0026gt; 9080/TCP 3m reviews ClusterIP 10.245.77.125 \u0026lt;none\u0026gt; 9080/TCP 3m\nBut how do I access the app? istio-1.0.5/samples/bookinfo $ kubectl apply -f networking/bookinfo-gateway.yaml gateway.networking.istio.io \u0026#34;bookinfo-gateway\u0026#34; created virtualservice.networking.istio.io \u0026#34;bookinfo\u0026#34; created In Istio a Gateway configures a load balancer for HTTP/TCP traffic, most commonly operating at the edge of the mesh to enable ingress traffic for an application (L4-L6).\nAfter that we need to set some environment variables to fetch the LB ip, port, etc. $ export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;) $ export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=\u0026#39;{.spec.ports[?(@.name==\u0026#34;http2\u0026#34;)].port}\u0026#39;) $ export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=\u0026#39;{.spec.ports[?(@.name==\u0026#34;https\u0026#34;)].port}\u0026#39;) $ export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT curl -o /dev/null -s -w \u0026#34;%{http_code}\\n\u0026#34; http://${GATEWAY_URL}/productpage If the latest curl returns 200 then we\u0026rsquo;re good, you can also browse the app open http://${GATEWAY_URL}/productpage and you will see something like the following image:  Also you can use Grafana to check some metrics about the service usage, etc. (You don\u0026rsquo;t have to worry about prometheus since it\u0026rsquo;s enabled by default). Spin up the port-forward so we don\u0026rsquo;t have to expose grafana: to the world with: kubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath='{.items[0].metadata.name}') 3000:3000, and then open http://localhost:3000.\nAs a general advice check all the settings that Istio offers try the ones that you think that could be useful for your project and always measure and compare.\nNotes  Do mind that pilot pod requires at least 4 Gbs of memory, so you will need at least one node with that amount of memory. You can check the load balancer status under: Manage -\u0026gt; Networking -\u0026gt; Load balancers. And if everything is okay your LB will say Healthy. Grafana is not enabled by default but we do enable it via helm with --set grafana.enabled=true, if you want to check all the possible options go here, if you are using more than two --set options I would recommend creating a values.yaml file and use that instead. Istio is a big beast and should be treated carefully, there is a lot more to learn and test out. We only scratched the surface here.  Upcoming posts  More examples using Istio. Linkerd. Maybe some Golang fun. Serverless or kubeless, that\u0026rsquo;s the question.  Errata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":27,"section":"blog","summary":"Introduction This time we will see how to get started with Istio and why do we need to use a service mesh.\nIn this example I will be using Digital Ocean (that\u0026rsquo;s my referral link), note that I do not have any association with Digital Ocean but they give you $100 to test their products for 60 days, if you spend $25 I get another $25.\nIstio So\u0026hellip; You might be wondering some of those questions: why Istio?","tags":["istio","service-mesh","kubernetes"],"title":"Why do I need a service mesh?","uri":"https://techsquad.rocks/blog/why_do_i_need_a_service_mesh/","year":"2019"},{"content":"Skaffold This time we will see how to get started with Skaffold, it seems a relatively mature project, and it does a lot more than some of the previous explored alternatives: Skaffold is a command line tool that facilitates continuous development for Kubernetes applications. You can iterate on your application source code locally then deploy to local or remote Kubernetes clusters. Skaffold handles the workflow for building, pushing and deploying your application. It also provides building blocks and describe customizations for a CI/CD pipeline. (Extracted from github)\nIn this example I will be using Digital Ocean (that\u0026rsquo;s my referral link), note that I do not have any association with Digital Ocean but they give you $100 to test their products for 60 days, if you spend $25 I get another $25, I got the idea from Pelado Nerd Spanish Youtube Channel.\nLet\u0026rsquo;s get started Once you have created your account and added your credit card you will get the $100 of free credit, then you will have to go to Manage on the left side panel and click on Kubernetes, then create your cluster with the amount of nodes that you consider necessary but remember to power them off or delete these resources so you don\u0026rsquo;t waste the free credit or your credit card itself. Once you have created your cluster and downloaded the kubectl config you\u0026rsquo;re ready to go.\nWe will be working with the chat bot again you can see the original article here, and the repo here.\nLet\u0026rsquo;s tell our kubectl to use our recently downloaded config: $ export KUBECONFIG=/home/kainlite/Downloads/k8s-1-13-1-do-2-nyc1-1546545313076-kubeconfig.yaml $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME crazy-wozniak-8306 Ready \u0026lt;none\u0026gt; 6h v1.13.1 178.128.154.205 Debian GNU/Linux 9 (stretch) 4.9.0-8-amd64 docker://18.9.0 crazy-wozniak-830t Ready \u0026lt;none\u0026gt; 6h v1.13.1 167.99.224.115 Debian GNU/Linux 9 (stretch) 4.9.0-8-amd64 docker://18.9.0 Your config might have a slightly different name, but it should be similar. We can see in the output a lot of information about our nodes (workers).\nBut let\u0026rsquo;s cut to the chase, we are here for Skaffold: curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/v0.20.0/skaffold-linux-amd64 \u0026amp;\u0026amp; chmod +x skaffold \u0026amp;\u0026amp; sudo mv skaffold /usr/local/bin You can install the binary using the provided line (linux) or downloading it from the releases page.\nOnce installed we can see the examples, I will be using the getting-started example: apiVersion: skaffold/v1beta2 kind: Config build: artifacts: - image: kainlite/echobot deploy: kubectl: manifests: - k8s-* With very litle YAML we can accomplish a lot.\nWe need a manifest file that matches that pattern so skaffold can deploy/re-deploy our application, so let\u0026rsquo;s generate one with kubectl run echobot --image=kainlite/echobot --dry-run -o yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: labels: run: echobot name: echobot spec: replicas: 1 selector: matchLabels: run: echobot strategy: {} template: metadata: labels: run: echobot spec: containers: - image: kainlite/echobot name: echobot env: - name: SLACK_API_TOKEN value: really_long_token livenessProbe: exec: command: - \u0026#39;/bin/sh\u0026#39; - \u0026#39;-c\u0026#39; - \u0026#39;/app/health_check.sh\u0026#39; The above command can be used to generate any kind of k8s resource :), I stripped it a bit, because there were fields that I didn\u0026rsquo;t want in and added some that we need for it to work.\nThen the only thing left to do is testing that everything works properly: $ skaffold build Starting build... Building [kainlite/echobot]... Sending build context to Docker daemon 66.56kB Step 1/12 : FROM golang:1.11.2-alpine as builder ---\u0026gt; 57915f96905a Step 2/12 : WORKDIR /app ---\u0026gt; Using cache ---\u0026gt; e04488a7f16b Step 3/12 : RUN adduser -D -g \u0026#39;app\u0026#39; app \u0026amp;\u0026amp; chown -R app:app /app \u0026amp;\u0026amp; apk add git \u0026amp;\u0026amp; apk add gcc musl-dev ---\u0026gt; Running in 1339601fff6f fetch http://dl-cdn.alpinelinux.org/alpine/v3.8/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.8/community/x86_64/APKINDEX.tar.gz (1/6) Installing nghttp2-libs (1.32.0-r0) (2/6) Installing libssh2 (1.8.0-r3) (3/6) Installing libcurl (7.61.1-r1) (4/6) Installing expat (2.2.5-r0) (5/6) Installing pcre2 (10.31-r0) (6/6) Installing git (2.18.1-r0) Executing busybox-1.28.4-r1.trigger OK: 19 MiB in 20 packages (1/12) Installing binutils (2.30-r5) (2/12) Installing gmp (6.1.2-r1) (3/12) Installing isl (0.18-r0) (4/12) Installing libgomp (6.4.0-r9) (5/12) Installing libatomic (6.4.0-r9) (6/12) Installing pkgconf (1.5.3-r0) (7/12) Installing libgcc (6.4.0-r9) (8/12) Installing mpfr3 (3.1.5-r1) (9/12) Installing mpc1 (1.0.3-r1) (10/12) Installing libstdc++ (6.4.0-r9) (11/12) Installing gcc (6.4.0-r9) (12/12) Installing musl-dev (1.1.19-r10) Executing busybox-1.28.4-r1.trigger OK: 113 MiB in 32 packages ---\u0026gt; 0e7a97e577dc Step 4/12 : ADD . /app/ ---\u0026gt; 72cfd4dea99b Step 5/12 : RUN go get -d -v ./... \u0026amp;\u0026amp; go build -o main . \u0026amp;\u0026amp; chown -R app:app /app /home/app ---\u0026gt; Running in 4482bfd3e8f7 go: finding github.com/gorilla/websocket v1.4.0 go: finding github.com/nlopes/slack v0.4.0 go: finding github.com/pkg/errors v0.8.0 go: downloading github.com/nlopes/slack v0.4.0 go: downloading github.com/pkg/errors v0.8.0 go: downloading github.com/gorilla/websocket v1.4.0 ---\u0026gt; 8ea604c7fb37 Step 6/12 : FROM golang:1.11.2-alpine ---\u0026gt; 57915f96905a Step 7/12 : WORKDIR /app ---\u0026gt; Using cache ---\u0026gt; e04488a7f16b Step 8/12 : RUN adduser -D -g \u0026#39;app\u0026#39; app \u0026amp;\u0026amp; chown -R app:app /app ---\u0026gt; Using cache ---\u0026gt; 33b206dba7e4 Step 9/12 : COPY --from=builder --chown=app /app/health_check.sh /app/health_check.sh ---\u0026gt; Using cache ---\u0026gt; 34d3cd1a5bb0 Step 10/12 : COPY --from=builder --chown=app /app/main /app/main ---\u0026gt; Using cache ---\u0026gt; 0c3d838b25dc Step 11/12 : USER app ---\u0026gt; Using cache ---\u0026gt; 95c2bf90800c Step 12/12 : CMD [\u0026#34;/app/main\u0026#34;] ---\u0026gt; Using cache ---\u0026gt; 3541257ff16c Successfully built 3541257ff16c Successfully tagged 1fca8a8c999a8cd9b943456b70d90807:latest The push refers to repository [docker.io/kainlite/echobot] ee06a8f42495: Preparing 12468476a0ef: Preparing ec122f36b39d: Preparing e94f3271cc73: Preparing 93391cb9fd4b: Preparing cb9d0f9550f6: Preparing 93448d8c2605: Preparing c54f8a17910a: Preparing df64d3292fd6: Preparing cb9d0f9550f6: Waiting c54f8a17910a: Waiting 93448d8c2605: Waiting e94f3271cc73: Layer already exists 93391cb9fd4b: Layer already exists 12468476a0ef: Layer already exists ec122f36b39d: Layer already exists ee06a8f42495: Layer already exists 93448d8c2605: Layer already exists cb9d0f9550f6: Layer already exists df64d3292fd6: Layer already exists c54f8a17910a: Layer already exists fc03e3d-dirty-3541257: digest: sha256:99c6d3d5b226a1947e8f96c0a5f963c8e499848d271f121ad50551046a0dc7ca size: 2197 Build complete in 48.642618413s Starting test... Test complete in 9.15µs kainlite/echobot -\u0026gt; kainlite/echobot:fc03e3d-dirty-3541257 As we can see skaffold build not only did the docker build but also tagged and pushed the image to docker hub, which is really nice and really useful to build a CI/CD system with it.\nBut wait, we need to deploy that to our cluster, right on: $ skaffold deploy Starting build... Building [kainlite/echobot]... Sending build context to Docker daemon 66.56kB Step 1/12 : FROM golang:1.11.2-alpine as builder ---\u0026gt; 57915f96905a Step 2/12 : WORKDIR /app ---\u0026gt; Using cache ---\u0026gt; e04488a7f16b Step 3/12 : RUN adduser -D -g \u0026#39;app\u0026#39; app \u0026amp;\u0026amp; chown -R app:app /app \u0026amp;\u0026amp; apk add git \u0026amp;\u0026amp; apk add gcc musl-dev ---\u0026gt; Using cache ---\u0026gt; 0e7a97e577dc Step 4/12 : ADD . /app/ ---\u0026gt; Using cache ---\u0026gt; 72cfd4dea99b Step 5/12 : RUN go get -d -v ./... \u0026amp;\u0026amp; go build -o main . \u0026amp;\u0026amp; chown -R app:app /app /home/app ---\u0026gt; Using cache ---\u0026gt; 8ea604c7fb37 Step 6/12 : FROM golang:1.11.2-alpine ---\u0026gt; 57915f96905a Step 7/12 : WORKDIR /app ---\u0026gt; Using cache ---\u0026gt; e04488a7f16b Step 8/12 : RUN adduser -D -g \u0026#39;app\u0026#39; app \u0026amp;\u0026amp; chown -R app:app /app ---\u0026gt; Using cache ---\u0026gt; 33b206dba7e4 Step 9/12 : COPY --from=builder --chown=app /app/health_check.sh /app/health_check.sh ---\u0026gt; Using cache ---\u0026gt; 34d3cd1a5bb0 Step 10/12 : COPY --from=builder --chown=app /app/main /app/main ---\u0026gt; Using cache ---\u0026gt; 0c3d838b25dc Step 11/12 : USER app ---\u0026gt; Using cache ---\u0026gt; 95c2bf90800c Step 12/12 : CMD [\u0026#34;/app/main\u0026#34;] ---\u0026gt; Using cache ---\u0026gt; 3541257ff16c Successfully built 3541257ff16c Successfully tagged 510226574761304cc9d64a343d5bdbff:latest The push refers to repository [docker.io/kainlite/echobot] ee06a8f42495: Preparing 12468476a0ef: Preparing ec122f36b39d: Preparing e94f3271cc73: Preparing 93391cb9fd4b: Preparing cb9d0f9550f6: Preparing 93448d8c2605: Preparing c54f8a17910a: Preparing df64d3292fd6: Preparing cb9d0f9550f6: Waiting 93448d8c2605: Waiting c54f8a17910a: Waiting df64d3292fd6: Waiting 12468476a0ef: Layer already exists e94f3271cc73: Layer already exists cb9d0f9550f6: Layer already exists ec122f36b39d: Layer already exists 93391cb9fd4b: Layer already exists ee06a8f42495: Layer already exists c54f8a17910a: Layer already exists df64d3292fd6: Layer already exists 93448d8c2605: Mounted from library/golang fc03e3d-dirty-3541257: digest: sha256:99c6d3d5b226a1947e8f96c0a5f963c8e499848d271f121ad50551046a0dc7ca size: 2197 Build complete in 15.136865292s Starting test... Test complete in 17.912µs Starting deploy... kubectl client version: 1.10 kubectl version 1.12.0 or greater is recommended for use with skaffold deployment.extensions \u0026#34;echobot\u0026#34; configured Deploy complete in 5.676513226s Deploy does a lot like with gitkube, it build the image, pushes it to the registry and then makes the deployment to the cluster, as you can see in there skaffold relies on kubectl and I have an old version of it.\nAfter a few seconds we can see that our deployment has been triggered and we have a new pod being created for it. $ kubectl get pods NAME READY STATUS RESTARTS AGE echobot-57fdcccf76-4qwvq 0/1 ContainerCreating 0 5s echobot-6fcd78658c-njvpx 0/1 Terminating 0 9m Skaffold also has another nice option that it\u0026rsquo;s called dev it watches the folder for changes and re-deploys the app so you can focus on code.\nLet\u0026rsquo;s clean up and call it a day: $ skaffold delete Cleaning up... deployment.extensions \u0026#34;echobot\u0026#34; deleted Cleanup complete in 3.833219278s\nNotes I really liked the workflow that skaffold provides, I hope that I can use it some more in the near future. And remember to shutdown the kubernetes cluster if you are using Digital Ocean so you don\u0026rsquo;t get charged by surprise later on.\nErrata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":28,"section":"blog","summary":"Skaffold This time we will see how to get started with Skaffold, it seems a relatively mature project, and it does a lot more than some of the previous explored alternatives: Skaffold is a command line tool that facilitates continuous development for Kubernetes applications. You can iterate on your application source code locally then deploy to local or remote Kubernetes clusters. Skaffold handles the workflow for building, pushing and deploying your application.","tags":["skaffold","kubernetes","deployment-tools"],"title":"Getting started with skaffold","uri":"https://techsquad.rocks/blog/getting_started_with_skaffold/","year":"2019"},{"content":"Gitkube This time we will see how to get started with Gitkube, it\u0026rsquo;s a young project but it seems to work fine and it has an interesting approach compared to other alternatives, since it only relies on git and kubectl, other than that it\u0026rsquo;s just a CRD and a controller, so you end up with 2 pods in kube-system one for the controller and the other for gitkubed, gitkubed is in charge of cloning your repos and also build the docker images, it seems that the idea behind gitkube is for the daily use in a dev/test environment where you need to try your changes quickly and without hassle. You can find more examples here, also be sure to check their page and documentation if you like it or want to learn more.\nIn the examples I will be using minikube or you can check out this repo that has a good overview of minikube, once installed and started (minikube start) that command will download and configure the local environment, if you have been following the previous posts you already have minikube installed and working, but in this post be sure to use minikube tunnel if you configure gitkube with a load balancer (or if you configure any service type as load balancer):\nLet\u0026rsquo;s get started We\u0026rsquo;re going to deploy or re-deploy our echo bot one more time but this time using gitkube. You can find the chat bot: article here, and the repo: here\nFirst of all we need to install the gitkube binary in our machine and then the CRD in our kubernetes cluster: $ kubectl create -f https://storage.googleapis.com/gitkube/gitkube-setup-stable.yaml customresourcedefinition.apiextensions.k8s.io \u0026#34;remotes.gitkube.sh\u0026#34; created serviceaccount \u0026#34;gitkube\u0026#34; created clusterrolebinding.rbac.authorization.k8s.io \u0026#34;gitkube\u0026#34; created configmap \u0026#34;gitkube-ci-conf\u0026#34; created deployment.extensions \u0026#34;gitkubed\u0026#34; created deployment.extensions \u0026#34;gitkube-controller\u0026#34; created $ kubectl --namespace kube-system expose deployment gitkubed --type=LoadBalancer --name=gitkubed service \u0026#34;gitkubed\u0026#34; exposed Note that there are 2 ways to install gitkube into our cluster, using the manifests as displayed there or using the gitkube binary and doing gitkube install.\nTo install the gitkube binary, the easiest way is to do: curl https://raw.githubusercontent.com/hasura/gitkube/master/gimme.sh | sudo bash This will download and copy the binary into: /usr/local/bin, as a general rule I recommend reading whatever you are going to pipe into bash in your terminal to avoid potential dangers of the internet.\nThen we need to generate (and then create it in the cluster) a file called remote.yaml (or any name you like), it\u0026rsquo;s necessary in order to tell gitkube how to deploy our application once we git push it: $ gitkube remote generate -f remote.yaml Remote name: minikube namespace: default SSH public key file: ~/.ssh/id_rsa.pub Initialisation: K8S YAML Manifests Manifests/Chart directory: Enter Choose docker registry: docker.io/kainlite Deployment name: echobot Container name: echobot Dockerfile path: Dockerfile Build context path: ./ Add another container? [y/N] Enter Add another deployment? [y/N] Enter And this will yield the following remote.yaml file that we then need to create in our cluster as it is a custom resource it might look a bit different from the default kubernetes resources.\nThe actual file remote.yaml: apiVersion: gitkube.sh/v1alpha1 kind: Remote metadata: creationTimestamp: null name: minikube namespace: default spec: authorizedKeys: - | ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA8jvVVtDSVe25p2U2tDGQyVrnv3YcWjJc6AXTUMc0YNi+QDm6s+hMTwkf2wDRD7b6Y3kmgNSqLEE0EEgOkA69c8PgypM7AwbKZ51V9XcdPd7NyLabpomNiftpUwi01DGfBr25lJV9h2MHwsI/6w1izDvQyN7fAl+aTFgx+VGg1p4FygXWeBqm0n0DfHmBI7PDXxGbuFTJHUmRVS+HPd5Bi31S9Kq6eoodBWtV2MlVnZkpF67FWt2Xo2rFKVf4pZR4N1yjZKRsvIaI5i14LvtOoOqNQ+/tPMAFAif3AhldOW06fgnddYGi/iF+CatVttwNDWmClSOek9LO72UzR4s0xQ== gabriel@kainlite deployments: - containers: - dockerfile: Dockerfile name: echobot path: ./ name: echobot manifests: helm: {} path: \u0026#34;\u0026#34; registry: credentials: secretKeyRef: key: \u0026#34;\u0026#34; secretRef: minikube-regsecret url: docker.io/kainlite status: remoteUrl: \u0026#34;\u0026#34; remoteUrlDesc: \u0026#34;\u0026#34; There are a few details to have in mind here, the deployment name because gitkube expects a deployment to be already present with that name in order to update/upgrade it, the path to the Dockerfile, or helm chart, credentials for the registry if any, I\u0026rsquo;m using a public image, so we don\u0026rsquo;t need any of that. The wizard will let you choose and customize a few options for your deployment.\nThe last step would be to finally create the resource: $ gitkube remote create -f remote.yaml INFO[0000] remote minikube created INFO[0000] waiting for remote url INFO[0000] remote url: ssh://default-minikube@10.98.213.202/~/git/default-minikube # add the remote to your git repo and push: git remote add minikube ssh://default-minikube@10.98.213.202/~/git/default-minikube git push minikube master\nAfter adding the new remote called minikube we have everything ready to go, so let\u0026rsquo;s test it and see what happens: $ git push minikube master Enumerating objects: 10, done. Counting objects: 100% (10/10), done. Delta compression using up to 8 threads Compressing objects: 100% (10/10), done. Writing objects: 100% (10/10), 1.92 KiB | 1.92 MiB/s, done. Total 10 (delta 2), reused 0 (delta 0) remote: Gitkube build system : Tue Jan 1 23:50:58 UTC 2019: Initialising remote: remote: Creating the build directory remote: Checking out \u0026#39;master:a0265bc5d0229dce0cffc985ca22ebe28532ee95\u0026#39; to \u0026#39;/home/default-minikube/build/default-minikube\u0026#39; remote: remote: 1 deployment(s) found in this repo remote: Trying to build them... remote: remote: Building Docker image for : echobot remote: remote: Building Docker image : docker.io/kainlite/default-minikube-default.echobot-echobot:a0265bc5d0229dce0cffc985ca22ebe28532ee95 remote: Sending build context to Docker daemon 7.68kB remote: Step 1/12 : FROM golang:1.11.2-alpine as builder remote: ---\u0026gt; 57915f96905a remote: Step 2/12 : WORKDIR /app remote: ---\u0026gt; Using cache remote: ---\u0026gt; 997342e65c61 remote: Step 3/12 : RUN adduser -D -g \u0026#39;app\u0026#39; app \u0026amp;\u0026amp; chown -R app:app /app \u0026amp;\u0026amp; apk add git \u0026amp;\u0026amp; apk add gcc musl-dev remote: ---\u0026gt; Using cache remote: ---\u0026gt; 7c6d8b9d1137 remote: Step 4/12 : ADD . /app/ remote: ---\u0026gt; Using cache remote: ---\u0026gt; ca751c2678c4 remote: Step 5/12 : RUN go get -d -v ./... \u0026amp;\u0026amp; go build -o main . \u0026amp;\u0026amp; chown -R app:app /app /home/app remote: ---\u0026gt; Using cache remote: ---\u0026gt; 16e44978b140 remote: Step 6/12 : FROM golang:1.11.2-alpine remote: ---\u0026gt; 57915f96905a remote: Step 7/12 : WORKDIR /app remote: ---\u0026gt; Using cache remote: ---\u0026gt; 997342e65c61 remote: Step 8/12 : RUN adduser -D -g \u0026#39;app\u0026#39; app \u0026amp;\u0026amp; chown -R app:app /app remote: ---\u0026gt; Using cache remote: ---\u0026gt; 55f48da0f9ac remote: Step 9/12 : COPY --from=builder --chown=app /app/health_check.sh /app/health_check.sh remote: ---\u0026gt; Using cache remote: ---\u0026gt; 139250fd6c77 remote: Step 10/12 : COPY --from=builder --chown=app /app/main /app/main remote: ---\u0026gt; Using cache remote: ---\u0026gt; 2f1eb9f16e9f remote: Step 11/12 : USER app remote: ---\u0026gt; Using cache remote: ---\u0026gt; a72f27dccff2 remote: Step 12/12 : CMD [\u0026#34;/app/main\u0026#34;] remote: ---\u0026gt; Using cache remote: ---\u0026gt; 034275449e08 remote: Successfully built 034275449e08 remote: Successfully tagged kainlite/default-minikube-default.echobot-echobot:a0265bc5d0229dce0cffc985ca22ebe28532ee95 remote: pushing docker.io/kainlite/default-minikube-default.echobot-echobot:a0265bc5d0229dce0cffc985ca22ebe28532ee95 to registry remote: The push refers to repository [docker.io/kainlite/default-minikube-default.echobot-echobot] remote: bba61bf193fe: Preparing remote: 3f0355bbea40: Preparing remote: 2ebcdc9e5e8f: Preparing remote: 6f1324339fd4: Preparing remote: 93391cb9fd4b: Preparing remote: cb9d0f9550f6: Preparing remote: 93448d8c2605: Preparing remote: c54f8a17910a: Preparing remote: df64d3292fd6: Preparing remote: cb9d0f9550f6: Waiting remote: 93448d8c2605: Waiting remote: c54f8a17910a: Waiting remote: df64d3292fd6: Waiting remote: 2ebcdc9e5e8f: Layer already exists remote: 6f1324339fd4: Layer already exists remote: 3f0355bbea40: Layer already exists remote: bba61bf193fe: Layer already exists remote: 93391cb9fd4b: Layer already exists remote: 93448d8c2605: Layer already exists remote: cb9d0f9550f6: Layer already exists remote: df64d3292fd6: Layer already exists remote: c54f8a17910a: Layer already exists remote: a0265bc5d0229dce0cffc985ca22ebe28532ee95: digest: sha256:3046c989fe1b1c4f700aaad875658c73ef571028f731546df38fb404ac22a9c9 size: 2198 remote: remote: Updating Kubernetes deployment: echobot remote: deployment \u0026#34;echobot\u0026#34; image updated remote: deployment \u0026#34;echobot\u0026#34; successfully rolled out remote: NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE remote: echobot 1 1 1 1 31s remote: remote: Removing build directory remote: remote: Gitkube build system : Tue Jan 1 23:51:16 UTC 2019: Finished build remote: remote: To ssh://10.98.213.202/~/git/default-minikube * [new branch] master -\u0026gt; master Quite a lot happened there, first of all gitkubed checked out the commit from the branch or HEAD that we pushed to /home/default-minikube/build/default-minikube and then started building and tagged the docker image with the corresponding SHA, after that it pushed the image to docker hub and then updated the deployment that we already had in there for the echo bot.\nThe last step would be to verify that the pod was actually updated, so we can inspect the pod configuration with kubectl describe pod echobot-654cdbfb99-g4bwv: $ kubectl describe pod echobot-654cdbfb99-g4bwv Name: echobot-654cdbfb99-g4bwv Namespace: default Priority: 0 PriorityClassName: \u0026lt;none\u0026gt; Node: minikube/10.0.2.15 Start Time: Tue, 01 Jan 2019 20:51:10 -0300 Labels: app=echobot pod-template-hash=654cdbfb99 Annotations: \u0026lt;none\u0026gt; Status: Running IP: 172.17.0.9 Controlled By: ReplicaSet/echobot-654cdbfb99 Containers: echobot: Container ID: docker://fe26ba9be6e2840c0d43a4fcbb4d79af38a00aa3a16411dee5e4af3823d44664 Image: docker.io/kainlite/default-minikube-default.echobot-echobot:a0265bc5d0229dce0cffc985ca22ebe28532ee95 Image ID: docker-pullable://kainlite/default-minikube-default.echobot-echobot@sha256:3046c989fe1b1c4f700aaad875658c73ef571028f731546df38fb404ac22a9c9 Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; State: Running Started: Tue, 01 Jan 2019 20:51:11 -0300 Ready: True Restart Count: 0 Liveness: exec [/bin/sh -c /app/health_check.sh] delay=0s timeout=1s period=10s #success=1 #failure=3 Environment: SLACK_API_TOKEN: really_long_token Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-ks4jx (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: default-token-ks4jx: Type: Secret (a volume populated by a Secret) SecretName: default-token-ks4jx Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 39m default-scheduler Successfully assigned default/echobot-654cdbfb99-g4bwv to minikube Normal Pulled 39m kubelet, minikube Container image \u0026#34;docker.io/kainlite/default-minikube-default.echobot-echobot:a0265bc5d0229dce0cffc985ca22ebe28532ee95\u0026#34; already present on machine Normal Created 39m kubelet, minikube Created container Normal Started 39m kubelet, minikube Started container As we can see the image is the one that got built from our git push and everything is working as expected.\nAnd that\u0026rsquo;s it for now, I think this tool has a lot of potential, it\u0026rsquo;s simple, nice and fast.\nErrata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":29,"section":"blog","summary":"Gitkube This time we will see how to get started with Gitkube, it\u0026rsquo;s a young project but it seems to work fine and it has an interesting approach compared to other alternatives, since it only relies on git and kubectl, other than that it\u0026rsquo;s just a CRD and a controller, so you end up with 2 pods in kube-system one for the controller and the other for gitkubed, gitkubed is in charge of cloning your repos and also build the docker images, it seems that the idea behind gitkube is for the daily use in a dev/test environment where you need to try your changes quickly and without hassle.","tags":["git","gitkube","kubernetes","deployment-tools"],"title":"Getting started with gitkube","uri":"https://techsquad.rocks/blog/getting_started_with_gitkube/","year":"2019"},{"content":"Echo bot This post was going to be about advanced ksonnet usage, but it went more about the echo bot itself, so I decided to rename it.\nTo be honest, there is no other way to get the benefits of having ksonnet if you\u0026rsquo;re not going to take advantage of the deployments as code facilities that it brings thanks to Jsonnet.\nThis time we will see how to use proper templates, it seems that the templates generated with ks are outdated at the time of this writing ksonnet version is: 0.13.1, no surprise here because it\u0026rsquo;s not a really mature tool. It does require a lot of effort in learning, hacking and reading to get things to work, but hopefully soon it will be easier, of course this is my personal opinion and I have not used it for a real project yet, but I expect it to grow and become more usable before I attempt to do something for the real world with it.\nIn the examples I will be using minikube or you can check out this repo that has a good overview of minikube, once installed and started (minikube start) that command will download and configure the local environment, if you have been following the previous posts you already have minikube installed and working:\nLet\u0026rsquo;s get started This time I\u0026rsquo;m not going to deploy another wordpress instance but a simple Slack echo bot made with go: package main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; slack \u0026#34;github.com/nlopes/slack\u0026#34; ) func main() { api := slack.New( os.Getenv(\u0026#34;SLACK_API_TOKEN\u0026#34;), ) rtm := api.NewRTM() go rtm.ManageConnection() for msg := range rtm.IncomingEvents { fmt.Print(\u0026#34;Event Received: \u0026#34;) switch ev := msg.Data.(type) { case *slack.HelloEvent: // Ignore hello  case *slack.ConnectedEvent: fmt.Println(\u0026#34;Infos:\u0026#34;, ev.Info) fmt.Println(\u0026#34;Connection counter:\u0026#34;, ev.ConnectionCount) case *slack.MessageEvent: // Only echo what it said to me  fmt.Printf(\u0026#34;Message: %v\\n\u0026#34;, ev) info := rtm.GetInfo() prefix := fmt.Sprintf(\u0026#34;\u0026lt;@%s\u0026gt; \u0026#34;, info.User.ID) if ev.User != info.User.ID \u0026amp;\u0026amp; strings.HasPrefix(ev.Text, prefix) { rtm.SendMessage(rtm.NewOutgoingMessage(ev.Text, ev.Channel)) } case *slack.PresenceChangeEvent: fmt.Printf(\u0026#34;Presence Change: %v\\n\u0026#34;, ev) case *slack.LatencyReport: fmt.Printf(\u0026#34;Current latency: %v\\n\u0026#34;, ev.Value) case *slack.RTMError: fmt.Printf(\u0026#34;Error: %s\\n\u0026#34;, ev.Error()) case *slack.InvalidAuthEvent: fmt.Printf(\u0026#34;Invalid credentials\u0026#34;) return default: // Ignore other events..  // fmt.Printf(\u0026#34;Unexpected: %v\\n\u0026#34;, msg.Data)  } } } As you can see it\u0026rsquo;s the simplest example from the readme of the Go Slack API project, it only connects to Slack and when it reads a message if it\u0026rsquo;s addressed to the bot then it echoes the message back, creating a bot and everything else is out of the scope of this article but it\u0026rsquo;s really simple, you only need to create an app in the Slack workspace, set it as a bot and grab the token (there is a lot more that you can customize but that is the most basic procedure to get started with a bot), then you just invite it to any channel that you want and start interacting with it.\nHere you can see the Dockerfile, for security we create an app user for the build and for running it, and to save space and bandwidth we only ship what we need using a multi-stage build: # BuildFROMgolang:1.11.2-alpine as builderWORKDIR/appRUN adduser -D -g \u0026#39;app\u0026#39; app \u0026amp;\u0026amp; \\  chown -R app:app /app \u0026amp;\u0026amp; \\  apk add git \u0026amp;\u0026amp; apk add gcc musl-devADD . /app/RUN go get -d -v ./... \u0026amp;\u0026amp; go build -o main . \u0026amp;\u0026amp; chown -R app:app /app /home/app# RunFROMgolang:1.11.2-alpineWORKDIR/appRUN adduser -D -g \u0026#39;app\u0026#39; app \u0026amp;\u0026amp; \\  chown -R app:app /appCOPY --from=builder --chown=app /app/health_check.sh /app/health_check.shCOPY --from=builder --chown=app /app/main /app/mainUSERappCMD [\u0026#34;/app/main\u0026#34;] There are a few more files in there, you can see the full sources here, for example health_check.sh, as our app doesn\u0026rsquo;t listen on any port we need a way to tell kubernetes how to check if our app is alive.\nOkay, enough boilerplate let\u0026rsquo;s get to business, so let\u0026rsquo;s create a new ksonnet application: $ ks init echobot INFO Using context \u0026#34;minikube\u0026#34; from kubeconfig file \u0026#34;~/.kube/config\u0026#34; INFO Creating environment \u0026#34;default\u0026#34; with namespace \u0026#34;default\u0026#34;, pointing to \u0026#34;version:v1.8.0\u0026#34; cluster at address \u0026#34;https://192.168.99.100:8443\u0026#34; INFO Generating ksonnet-lib data at path \u0026#39;~/Webs/echobot/echobot/lib/ksonnet-lib/v1.8.0\u0026#39;\nAnd now let\u0026rsquo;s grab a template and modify it accordingly to be able to create the deployment for the bot components/echobot.jsonnet: // Import KSonnet library local params = std.extVar(\u0026#39;__ksonnet/params\u0026#39;).components.demo; local k = import \u0026#39;k.libsonnet\u0026#39;; // Specify the import objects that we need local container = k.extensions.v1beta1.deployment.mixin.spec.template.spec.containersType; local depl = k.extensions.v1beta1.deployment; // Environment variables, instead of hardcoding it here we could use a param or a secret // But I will leave that as an exercise for you :) local envs = [ { name: \u0026#39;SLACK_API_TOKEN\u0026#39;, value: \u0026#39;really-long-token\u0026#39;, }, ]; local livenessProbe = { exec: { command: [ \u0026#39;/bin/sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;/app/health_check.sh\u0026#39;, ], }, }; // Define containers local containers = [ container.new(\u0026#39;echobot\u0026#39;, \u0026#39;kainlite/echobot:0.0.2\u0026#39;) { env: (envs), livenessProbe: livenessProbe, }, ]; // Define deployment with 3 replicas local deployment = depl.new(\u0026#39;echobot\u0026#39;, 1, containers, { app: \u0026#39;echobot\u0026#39; }); local resources = [deployment]; // Return list of resources. k.core.v1.list.new(resources) Note that I have uploaded that image to docker hub so you can use it to follow the example if you want, after that just replace really-long-token with your token, and then do: $ ks apply default INFO Applying deployments echobot INFO Creating non-existent deployments echobot\nAnd now if we check our deployment and pod, we should see something like this:\n And in the logs: $ kubectl get pods NAME READY STATUS RESTARTS AGE echobot-7456f7d7dd-twg4r 1/1 Running 0 53s $ kubectl logs -f echobot-7456f7d7dd-twg4r Event Received: Event Received: Infos: \u0026amp;{wss://cerberus-xxxx.lb.slack-msgs.com/websocket/1gvXP_yQCFE-Y= 0xc000468000 0xc0004482a0 [] [] [] [] []} Connection counter: 0 Event Received: Event Received: Current latency: 1.256397423s Event Received: Current latency: 1.25679313s Event Received: Current latency: 1.256788737s Event Received: Message: \u0026amp;{{message CEDGU6EA0 UEDJT5DDH \u0026lt;@UED48HD33\u0026gt; echo! 1546124966.002300 false [] [] \u0026lt;nil\u0026gt; false 0 false 1546124966.002300 \u0026lt;nil\u0026gt; [] 0 [] [] false \u0026lt;nil\u0026gt; 0 TEDJT5CTD [] false false} \u0026lt;nil\u0026gt;}\nAnd that folks is all I have for now, I hope you enjoyed this small tour of ksonnet. The source code for the bot can be found here. In a future post I might explore ksonnet and helm charts.\nUpcoming topics As promised I will be doing one post about Gitkube and Skaffold, there are a lot of deployment tools for kubernetes but those are the most promising ones to me, also after that I will start covering more topics about Docker, ContainerD, KubeADM, and Kubernetes in general.\nErrata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso, you can check the source code and changes in the generated code and the sources here\n","id":30,"section":"blog","summary":"Echo bot This post was going to be about advanced ksonnet usage, but it went more about the echo bot itself, so I decided to rename it.\nTo be honest, there is no other way to get the benefits of having ksonnet if you\u0026rsquo;re not going to take advantage of the deployments as code facilities that it brings thanks to Jsonnet.\nThis time we will see how to use proper templates, it seems that the templates generated with ks are outdated at the time of this writing ksonnet version is: 0.","tags":["go","golang","slack","ksonnet","jsonnet","kubernetes","deployment-tools"],"title":"Go echo bot","uri":"https://techsquad.rocks/blog/go_echobot/","year":"2018"},{"content":"Introduction This tutorial will show you how to create a simple application and also how to deploy it to kubernetes using ksonnet, in the examples I will be using minikube or you can check out this repo that has a good overview of minikube, once installed and started (minikube start) that command will download and configure the local environment, if you have been following the previous posts you already have minikube installed and working, before we dive into an example let\u0026rsquo;s review some terminology from ksonnet (extracted from the official documentation):\nApplication A ksonnet application represents a well-structured directory of Kubernetes manifests (this is generated using the ks init).\nEnvironment An environment consists of four elements, some of which can be pulled from your current kubeconfig context: Name, Server, Namespace, API version. The environment determines to which cluster you\u0026rsquo;re going to deploy the application.\nComponent A component can be as simple as a Kubernetes resource (a Pod, Deployment, etc), or a fully working stack for example EFK/ELK, you can generate components using ks generate.\nPrototype Prototype + Parameters = Component. Think of a prototype as a base template before you apply the parameters, to set a name, replicas, etc for the resource, you can explore some system prototypes with ks prototype.\nParameter It gives live to a component with dynamic values, you can use ks param to view or modify params, there are App params (global), Component params, and Environment params (overrides app params).\nModule Modules provide a way for you to share components across environments. More concisely, a module refers to a subdirectory in components/ containing its own params.libsonnet. To create a module ks module create \u0026lt;module name\u0026gt;.\nPart It provides a way to organize and re-use code.\nPackage A package is a set of related prototypes and associates helper libraries, it allows you to create and share packages between applications.\nRegistry It\u0026rsquo;s essentially a repository for packages, it supports the incubator registry, github, filesystem, and Helm.\nManifest The same old YAML or JSON manifest but this time written in Jsonnet, basically Jsonnet is a simple extension of JSON.\nPhew, that\u0026rsquo;s a lot of names and terminology at once, let\u0026rsquo;s get started with the terminal already.\nLet\u0026rsquo;s get started This command will generate the following folder structure ks init wordpress: INFO Using context \u0026#34;minikube\u0026#34; from kubeconfig file \u0026#34;~/.kube/config\u0026#34; INFO Creating environment \u0026#34;default\u0026#34; with namespace \u0026#34;default\u0026#34;, pointing to \u0026#34;version:v1.12.4\u0026#34; cluster at address \u0026#34;https://192.168.99.100:8443\u0026#34; INFO Generating ksonnet-lib data at path \u0026#39;~/k8s-examples/wordpress/lib/ksonnet-lib/v1.12.4\u0026#39; $ ls -l | awk \u0026#39;{ print $9 }\u0026#39; app.yaml \u0026lt;--- Defines versions, namespace, cluster address, app name, registry. components \u0026lt;--- Components by default it\u0026#39;s empty and has a params file. environments \u0026lt;--- By default there is only one environment called default. lib \u0026lt;--- Here we can find the ksonnet helpers that match the Kubernetes API with the common resources (Pods, Deployments, etc). vendor \u0026lt;--- Here is where the installed packages/apps go, it can be seen as a dependencies folder.\nLet\u0026rsquo;s generate a deployed-service and inspect it\u0026rsquo;s context: $ ks generate deployed-service wordpress \\  --image bitnami/wordpress:5.0.2 \\  --type ClusterIP INFO Writing component at \u0026#39;~/k8s-examples/wordpress/components/wordpress.jsonnet\u0026#39; At the moment of this writing the latest version of Wordpress is 5.0.2, it\u0026rsquo;s always recommended to use static version numbers instead of tags like latest (because latest can not be latest).\nLet\u0026rsquo;s see how our component looks like: local env = std.extVar(\u0026#34;__ksonnet/environments\u0026#34;); local params = std.extVar(\u0026#34;__ksonnet/params\u0026#34;).components.wordpress; [ { \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Service\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: params.name }, \u0026#34;spec\u0026#34;: { \u0026#34;ports\u0026#34;: [ { \u0026#34;port\u0026#34;: params.servicePort, \u0026#34;targetPort\u0026#34;: params.containerPort } ], \u0026#34;selector\u0026#34;: { \u0026#34;app\u0026#34;: params.name }, \u0026#34;type\u0026#34;: params.type } }, { \u0026#34;apiVersion\u0026#34;: \u0026#34;apps/v1beta2\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Deployment\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: params.name }, \u0026#34;spec\u0026#34;: { \u0026#34;replicas\u0026#34;: params.replicas, \u0026#34;selector\u0026#34;: { \u0026#34;matchLabels\u0026#34;: { \u0026#34;app\u0026#34;: params.name }, }, \u0026#34;template\u0026#34;: { \u0026#34;metadata\u0026#34;: { \u0026#34;labels\u0026#34;: { \u0026#34;app\u0026#34;: params.name } }, \u0026#34;spec\u0026#34;: { \u0026#34;containers\u0026#34;: [ { \u0026#34;image\u0026#34;: params.image, \u0026#34;name\u0026#34;: params.name, \u0026#34;ports\u0026#34;: [ { \u0026#34;containerPort\u0026#34;: params.containerPort } ] } ] } } } } ] It\u0026rsquo;s just another template for some known resources, a service and a deployment that\u0026rsquo;s where the name came from: deployed-service, but where are those params coming from?\nIf we run ks show default: --- apiVersion: v1 kind: Service metadata: labels: ksonnet.io/component: wordpress name: wordpress spec: ports: - port: 80 targetPort: 80 selector: app: wordpress type: ClusterIP --- apiVersion: apps/v1beta2 kind: Deployment metadata: labels: ksonnet.io/component: wordpress name: wordpress spec: replicas: 1 selector: matchLabels: app: wordpress template: metadata: labels: app: wordpress spec: containers: - image: bitnami/wordpress:5.0.2 name: wordpress ports: - containerPort: 80 We will see what our package will generate in YAML with some good defaults. And by default if you remember from the definitions a component needs a params file to fill the blanks in this case it is components/params.libsonnet: { global: { // User-defined global parameters; accessible to all component and environments, Ex: // replicas: 4, }, components: { // Component-level parameters, defined initially from \u0026#39;ks prototype use ...\u0026#39; // Each object below should correspond to a component in the components/ directory wordpress: { containerPort: 80, image: \u0026#34;bitnami/wordpress:5.0.2\u0026#34;, name: \u0026#34;wordpress\u0026#34;, replicas: 1, servicePort: 80, type: \u0026#34;ClusterIP\u0026#34;, }, }, } But that\u0026rsquo;s not enough to run wordpress is it?, No is not, we need a database with persistent storage for it to work properly, so we will need to generate and extend another deployed-service.\nThe next step would be to create another component: $ ks generate deployed-service mariadb \\  --image bitnami/mariadb:10.1.37 \\  --type ClusterIP INFO Writing component at \u0026#39;/home/kainlite/Webs/k8s-examples/wordpress/components/mariadb.jsonnet\u0026#39; The latest stable version of MariaDB 10.1 GA at the moment of this writting is 10.1.37.\nThen we will need to add a persistent volume and also tell Wordpress to use this MariaDB instance. How do we do that, we will need to modify a few files, like this (in order to re-use things I placed the mysql variables in the global section, for this example that will simplify things, but it might not be the best approach for a production environment): The resulting components/params.json will be: { global: { // User-defined global parameters; accessible to all component and environments, Ex: // replicas: 4, mariadbEmptyPassword: \u0026#34;no\u0026#34;, mariadbUser: \u0026#34;mywordpressuser\u0026#34;, mariadbPassword: \u0026#34;mywordpresspassword\u0026#34;, mariadbDatabase: \u0026#34;bitnami_wordpress\u0026#34;, }, components: { // Component-level parameters, defined initially from \u0026#39;ks prototype use ...\u0026#39; // Each object below should correspond to a component in the components/ directory wordpress: { containerPort: 80, image: \u0026#34;bitnami/wordpress:5.0.2\u0026#34;, name: \u0026#34;wordpress\u0026#34;, replicas: 1, servicePort: 80, type: \u0026#34;ClusterIP\u0026#34;, }, mariadb: { containerPort: 3306, image: \u0026#34;bitnami/mariadb:10.1.37\u0026#34;, name: \u0026#34;mariadb\u0026#34;, replicas: 1, servicePort: 3306, type: \u0026#34;ClusterIP\u0026#34;, }, }, }\nThe resulting components/wordpress.jsonnet will be: local env = std.extVar(\u0026#34;__ksonnet/environments\u0026#34;); local params = std.extVar(\u0026#34;__ksonnet/params\u0026#34;).components.wordpress; [ { \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Service\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: params.name }, \u0026#34;spec\u0026#34;: { \u0026#34;ports\u0026#34;: [ { \u0026#34;port\u0026#34;: params.servicePort, \u0026#34;targetPort\u0026#34;: params.containerPort } ], \u0026#34;selector\u0026#34;: { \u0026#34;app\u0026#34;: params.name }, \u0026#34;type\u0026#34;: params.type } }, { \u0026#34;apiVersion\u0026#34;: \u0026#34;apps/v1beta2\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Deployment\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: params.name }, \u0026#34;spec\u0026#34;: { \u0026#34;replicas\u0026#34;: params.replicas, \u0026#34;selector\u0026#34;: { \u0026#34;matchLabels\u0026#34;: { \u0026#34;app\u0026#34;: params.name }, }, \u0026#34;template\u0026#34;: { \u0026#34;metadata\u0026#34;: { \u0026#34;labels\u0026#34;: { \u0026#34;app\u0026#34;: params.name } }, \u0026#34;spec\u0026#34;: { \u0026#34;containers\u0026#34;: [ { \u0026#34;image\u0026#34;: params.image, \u0026#34;name\u0026#34;: params.name, \u0026#34;ports\u0026#34;: [ { \u0026#34;containerPort\u0026#34;: params.containerPort } ], \u0026#34;env\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;WORDPRESS_DATABASE_USER\u0026#34;, \u0026#34;value\u0026#34;: params.mariadbUser, }, { \u0026#34;name\u0026#34;: \u0026#34;WORDPRESS_DATABASE_PASSWORD\u0026#34;, \u0026#34;value\u0026#34;: params.mariadbPassword, }, { \u0026#34;name\u0026#34;: \u0026#34;WORDPRESS_DATABASE_NAME\u0026#34;, \u0026#34;value\u0026#34;: params.mariadbDatabase, }, { \u0026#34;name\u0026#34;: \u0026#34;WORDPRESS_HOST\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;mariadb\u0026#34;, } ] } ] } } } } ] The only thing that changed here is spec.containers.env which wasn\u0026rsquo;t present before.\nThe resulting components/mariadb.jsonnet will be: local env = std.extVar(\u0026#34;__ksonnet/environments\u0026#34;); local params = std.extVar(\u0026#34;__ksonnet/params\u0026#34;).components.mariadb; [ { \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Service\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: params.name }, \u0026#34;spec\u0026#34;: { \u0026#34;ports\u0026#34;: [ { \u0026#34;port\u0026#34;: params.servicePort, \u0026#34;targetPort\u0026#34;: params.containerPort } ], \u0026#34;selector\u0026#34;: { \u0026#34;app\u0026#34;: params.name }, \u0026#34;type\u0026#34;: params.type } }, { \u0026#34;apiVersion\u0026#34;: \u0026#34;apps/v1beta2\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Deployment\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: params.name }, \u0026#34;spec\u0026#34;: { \u0026#34;replicas\u0026#34;: params.replicas, \u0026#34;selector\u0026#34;: { \u0026#34;matchLabels\u0026#34;: { \u0026#34;app\u0026#34;: params.name }, }, \u0026#34;template\u0026#34;: { \u0026#34;metadata\u0026#34;: { \u0026#34;labels\u0026#34;: { \u0026#34;app\u0026#34;: params.name } }, \u0026#34;spec\u0026#34;: { \u0026#34;containers\u0026#34;: [ { \u0026#34;image\u0026#34;: params.image, \u0026#34;name\u0026#34;: params.name, \u0026#34;ports\u0026#34;: [ { \u0026#34;containerPort\u0026#34;: params.containerPort }, ], \u0026#34;env\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;ALLOW_EMPTY_PASSWORD\u0026#34;, \u0026#34;value\u0026#34;: params.mariadbEmptyPassword, }, { \u0026#34;name\u0026#34;: \u0026#34;MARIADB_USER\u0026#34;, \u0026#34;value\u0026#34;: params.mariadbUser, }, { \u0026#34;name\u0026#34;: \u0026#34;MARIADB_PASSWORD\u0026#34;, \u0026#34;value\u0026#34;: params.mariadbPassword, }, { \u0026#34;name\u0026#34;: \u0026#34;MARIADB_ROOT_PASSWORD\u0026#34;, \u0026#34;value\u0026#34;: params.mariadbPassword, }, { \u0026#34;name\u0026#34;: \u0026#34;MARIADB_DATABASE\u0026#34;, \u0026#34;value\u0026#34;: params.mariadbDatabase, }, ], \u0026#34;volumeMounts\u0026#34;: [ { \u0026#34;mountPath\u0026#34;: \u0026#34;/var/lib/mysql\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;mariadb\u0026#34; } ] } ], \u0026#34;volumes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;mariadb\u0026#34;, \u0026#34;hostPath\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/home/docker/mariadb-data\u0026#34; } } ] } } } } ] I know, I know, that is a lot of JSON, I trust you have a decent scroll :).\nThe only things that changed here are spec.containers.env, spec.containers.volumeMount and spec.volumes which weren\u0026rsquo;t present before, that\u0026rsquo;s all you need to make wordpress work with mariadb.\nThis post only scratched the surface of what Ksonnet and Jsonnet can do, in another post I will describe more advances features with less JSON / YAML. There are a lot of things that can be improved and we will cover those things in the next post, if you want to see all the source code for this post go here.\nLet\u0026rsquo;s clean up ks delete default: INFO Deleting services mariadb INFO Deleting deployments mariadb INFO Deleting services wordpress INFO Deleting deployments wordpress\nNotes If you want to check the wordpress installation via browser you can do minikube proxy and then look up the following URL: Wordpress (I\u0026rsquo;m using the default namespace here and the service name is wordpress, if you use ingress you don\u0026rsquo;t need to do this step)\nI\u0026rsquo;m not aware if Ksonnet supports releases and rollbacks like Helm, but it seems it could be emulated using git tags and just some git hooks.\nIf everything goes well, you should see something like this in the logs: $ kubectl logs -f wordpress-5b4d6bd47c-bdtmw Welcome to the Bitnami wordpress container Subscribe to project updates by watching https://github.com/bitnami/bitnami-docker-wordpress Submit issues and feature requests at https://github.com/bitnami/bitnami-docker-wordpress/issues nami INFO Initializing apache apache INFO ==\u0026gt; Patching httpoxy... apache INFO ==\u0026gt; Configuring dummy certificates... nami INFO apache successfully initialized nami INFO Initializing php nami INFO php successfully initialized nami INFO Initializing mysql-client nami INFO mysql-client successfully initialized nami INFO Initializing libphp nami INFO libphp successfully initialized nami INFO Initializing wordpress mysql-c INFO Trying to connect to MySQL server mysql-c INFO Found MySQL server listening at mariadb:3306 mysql-c INFO MySQL server listening and working at mariadb:3306 wordpre INFO wordpre INFO ######################################################################## wordpre INFO Installation parameters for wordpress: wordpre INFO First Name: FirstName wordpre INFO Last Name: LastName wordpre INFO Username: user wordpre INFO Password: ********** wordpre INFO Email: user@example.com wordpre INFO Blog Name: User\u0026#39;s Blog! wordpre INFO Table Prefix: wp_ wordpre INFO (Passwords are not shown for security reasons) wordpre INFO ######################################################################## wordpre INFO nami INFO wordpress successfully initialized INFO ==\u0026gt; Starting wordpress... [Thu Dec 27 04:30:59.684053 2018] [ssl:warn] [pid 116] AH01909: localhost:443:0 server certificate does NOT include an ID which matches the server name [Thu Dec 27 04:30:59.684690 2018] [ssl:warn] [pid 116] AH01909: localhost:443:0 server certificate does NOT include an ID which matches the server name [Thu Dec 27 04:30:59.738783 2018] [ssl:warn] [pid 116] AH01909: localhost:443:0 server certificate does NOT include an ID which matches the server name [Thu Dec 27 04:30:59.739701 2018] [ssl:warn] [pid 116] AH01909: localhost:443:0 server certificate does NOT include an ID which matches the server name [Thu Dec 27 04:30:59.765798 2018] [mpm_prefork:notice] [pid 116] AH00163: Apache/2.4.37 (Unix) OpenSSL/1.1.0j PHP/7.2.13 configured -- resuming normal operations [Thu Dec 27 04:30:59.765874 2018] [core:notice] [pid 116] AH00094: Command line: \u0026#39;httpd -f /bitnami/apache/conf/httpd.conf -D FOREGROUND\u0026#39; 172.17.0.1 - - [27/Dec/2018:04:31:00 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 3718 172.17.0.1 - - [27/Dec/2018:04:31:01 +0000] \u0026#34;GET /wp-includes/js/wp-embed.min.js?ver=5.0.2 HTTP/1.1\u0026#34; 200 753 172.17.0.1 - - [27/Dec/2018:04:31:01 +0000] \u0026#34;GET /wp-includes/css/dist/block-library/theme.min.css?ver=5.0.2 HTTP/1.1\u0026#34; 200 452 172.17.0.1 - - [27/Dec/2018:04:31:01 +0000] \u0026#34;GET /wp-includes/css/dist/block-library/style.min.css?ver=5.0.2 HTTP/1.1\u0026#34; 200 4281 172.17.0.1 - - [27/Dec/2018:04:31:01 +0000] \u0026#34;GET /wp-content/themes/twentynineteen/style.css?ver=1.1 HTTP/1.1\u0026#34; 200 19371 172.17.0.1 - - [27/Dec/2018:04:31:01 +0000] \u0026#34;GET /wp-content/themes/twentynineteen/print.css?ver=1.1 HTTP/1.1\u0026#34; 200 1230\nAnd that folks is all I have for now, be sure to check out the Ksonnet official documentation and ks help to know more about what ksonnet can do to help you deploy your applications to any kubernetes cluster.\nErrata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso you can check the source code and changes in the generated code and the sources here\n","id":31,"section":"blog","summary":"Introduction This tutorial will show you how to create a simple application and also how to deploy it to kubernetes using ksonnet, in the examples I will be using minikube or you can check out this repo that has a good overview of minikube, once installed and started (minikube start) that command will download and configure the local environment, if you have been following the previous posts you already have minikube installed and working, before we dive into an example let\u0026rsquo;s review some terminology from ksonnet (extracted from the official documentation):","tags":["ksonnet","jsonnet","deployment-tools","kubernetes"],"title":"Getting started with ksonnet","uri":"https://techsquad.rocks/blog/getting_started_with_ksonnet/","year":"2018"},{"content":"Deploying my apps with Helm If you are already familiar with Helm, and the different types of kubernetes workloads / resource types you might be wondering how to install apps directly to kubernetes, yes, you don\u0026rsquo;t have to re-invent the wheel for your mysql installation, or your postgres, or nginx, jenkins, You name it. Helm solves that problem with Charts, this list has the official charts maintained by the community, where the folder incubator may refer to charts that are still not compliant with the technical requirements but probably usable and the folder stable is for graduated charts. This is not the only source of charts as you can imagine, You can use any source for your charts, even just the tgz files, as we will see in this post.\nHow do I search for charts?:\n$ helm search wordpress NAME CHART VERSION APP VERSION DESCRIPTION stable/wordpress 3.3.0 4.9.8 Web publishing platform for building blogs and websites. Note that I\u0026rsquo;m not a fan of Wordpress or PHP itself, but it seems like the most common example everywhere. As we can see here it says stable/wordpress so we know that we\u0026rsquo;re using the official repo in the folder stable, but what if we don\u0026rsquo;t want that chart, but someone else provides one with more features or something that You like better. Let\u0026rsquo;s use the one from Bitnami, so if we check their page you can select different kind of deployments but for it to work we need to add another external repo: helm repo add bitnami https://charts.bitnami.com/bitnami So if we search again we will now see two options (at the moment of this writing, the latest version is actually 5.0.2): $ helm search wordpress NAME CHART VERSION APP VERSION DESCRIPTION bitnami/wordpress 5.0.2 5.0.2 Web publishing platform for building blogs and websites. stable/wordpress 3.3.0 4.9.8 Web publishing platform for building blogs and websites. Let\u0026rsquo;s check the documentation of the chart to create our values.yaml file, note that in this example the stable wordpress chart it\u0026rsquo;s also maintained by Bitnami, so they have the same configuration :), this won\u0026rsquo;t always be the case but it simplifies things for us.\nOur example values.yaml will look like: wordpressBlogName: \u0026#34;Testing Helm Charts\u0026#34; persistence: size: 1Gi ingress: enabled: true We will only change the blog name by default, the persistent volume size and also enable ingress (Our app should be available through wordpress.local inside the cluster), if you are using minikube be sure to enable the ingress addon. $ minikube addons enable ingress ingress was successfully enabled\nWe can then install stable/wordpress or bitnami/wordpress, we will follow up with the one from Bitnami repo. $ helm install bitnami/wordpress \\ --set image.repository=bitnami/wordpress \\ --set image.tag=5.0.2 \\ -f values.yaml As it\u0026rsquo;s a common good practice to use specific versions we will do it here, it\u0026rsquo;s better to do it this way because you can easily move between known versions and also avoid unknown states, this can happen by misunderstanding what latest means, follow the example.\nYou should see something like: NAME: plucking-condor LAST DEPLOYED: Mon Dec 24 13:06:38 2018 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE plucking-condor-wordpress-84845db8b5-hkqhc 0/1 ContainerCreating 0 0s plucking-condor-mariadb-0 0/1 Pending 0 0s ==\u0026gt; v1/Secret NAME AGE plucking-condor-mariadb 0s plucking-condor-wordpress 0s ==\u0026gt; v1/ConfigMap plucking-condor-mariadb 0s plucking-condor-mariadb-tests 0s ==\u0026gt; v1/PersistentVolumeClaim plucking-condor-wordpress 0s ==\u0026gt; v1/Service plucking-condor-mariadb 0s plucking-condor-wordpress 0s ==\u0026gt; v1beta1/Deployment plucking-condor-wordpress 0s ==\u0026gt; v1beta1/StatefulSet plucking-condor-mariadb 0s ==\u0026gt; v1beta1/Ingress wordpress.local-plucking-condor 0s NOTES: 1. Get the WordPress URL: You should be able to access your new WordPress installation through http://wordpress.local/admin 2. Login with the following credentials to see your blog echo Username: user echo Password: $(kubectl get secret --namespace default plucking-condor-wordpress -o jsonpath=\u0026#34;{.data.wordpress-password}\u0026#34; | base64 --decode) Depending on the cluster provider or installation itself, you might need to replace the persistence.storageClass to match what your cluster has, note that in the values file is represented like JSON with dot notation but in your values.yaml you need to stick to YAML format and indent storageClass under persistence as usual, the kubernetes API parses and uses JSON but YAML seems more human friendly.\nAt this point we should a working wordpress installation, also move between versions, but be aware that the application is in charge of the database schema and updating it to match what the new version needs, this can also be troublesome rolling back or when downgrading, so if you use persistent data ALWAYS have a working backup, because when things go south, you will want to quickly go back to a known state, also note that I said \u0026ldquo;working backup\u0026rdquo;, yes, test that the backup works and that You can restore it somewhere else before doing anything destructive or that can has repercussions, this will bring you peace of mind and better ways to organize yourself while upgrading, etc.\nNow let\u0026rsquo;s check that all resources are indeed working and that we can use our recently installed app. $ kubectl get all NAME READY STATUS RESTARTS AGE pod/plucking-condor-mariadb-0 1/1 Running 0 12m pod/plucking-condor-wordpress-84845db8b5-hkqhc 1/1 Running 0 12m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 37h service/plucking-condor-mariadb ClusterIP 10.106.219.59 \u0026lt;none\u0026gt; 3306/TCP 12m service/plucking-condor-wordpress LoadBalancer 10.100.239.163 10.100.239.163 80:31764/TCP,443:32308/TCP 12m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.apps/plucking-condor-wordpress 1 1 1 1 12m NAME DESIRED CURRENT READY AGE replicaset.apps/plucking-condor-wordpress-84845db8b5 1 1 1 12m NAME DESIRED CURRENT AGE statefulset.apps/plucking-condor-mariadb 1 1 12m You can deploy it to a custom namespace (In this case I deployed it to the default namespace), the only change for that would be to set the parameter --namespace in the helm install line.\nIf you use minikube then ingress will expose a nodeport that we can find using minikube service list then using the browser or curl to navigate our freshly installed wordpress. $ minikube service list |-------------|---------------------------|--------------------------------| | NAMESPACE | NAME | URL | |-------------|---------------------------|--------------------------------| | default | kubernetes | No node port | | default | plucking-condor-mariadb | No node port | | default | plucking-condor-wordpress | http://192.168.99.100:31764 | | | | http://192.168.99.100:32308 | | kube-system | default-http-backend | http://192.168.99.100:30001 | | kube-system | kube-dns | No node port | | kube-system | kubernetes-dashboard | No node port | | kube-system | tiller-deploy | No node port | |-------------|---------------------------|--------------------------------| In the cloud or on premises this will indeed be different and you should have a publicly available installation using your own domain name (In this case http is at: http://192.168.99.100:31764 and https at: http://192.168.99.100:32308, and http://192.168.99.100:30001 is the default backend for the ingress controller), your ips can be different but the basics are the same.\nSample screenshot:\n Notes As long as we have the persistent volume our data should be preserved in this case the PV is used for tha database, but we could add another volume to preserve images, etc.\nClean everything up: helm del --purge plucking-condor\nThat\u0026rsquo;s all I have for now, I will be adding more content next week.\nDon\u0026rsquo;t Repeat Yourself DRY is a good design goal and part of the art of a good template is knowing when to add a new template and when to update or use an existing one. While helm and go helps with that, there is no perfect tool so we will explore other options in the following posts, explore what the community provides and what seems like a suitable tool for you. Happy Helming!.\nUpcoming topics The following posts will be about package managers, development deployment tools, etc. It\u0026rsquo;s hard to put all the tools in a category, but they are trying to solve similar problems in different ways, and we will be exploring the ones that seem more promising to me, if you would like me to cover any other tool/project/whatever, just send me a message :)\n Getting started with Ksonnet and friends. Getting started with Skaffold. Getting started with Gitkube.  Errata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso you can check the source code and changes in the generated code and the sources here\n","id":32,"section":"blog","summary":"Deploying my apps with Helm If you are already familiar with Helm, and the different types of kubernetes workloads / resource types you might be wondering how to install apps directly to kubernetes, yes, you don\u0026rsquo;t have to re-invent the wheel for your mysql installation, or your postgres, or nginx, jenkins, You name it. Helm solves that problem with Charts, this list has the official charts maintained by the community, where the folder incubator may refer to charts that are still not compliant with the technical requirements but probably usable and the folder stable is for graduated charts.","tags":["helm","deployment-tools","kubernetes"],"title":"Deploying my apps with Helm","uri":"https://techsquad.rocks/blog/deploying_my_apps_with_helm/","year":"2018"},{"content":"Introduction This tutorial will show you how to create a simple chart and also how to deploy it to kubernetes using Helm, in the examples I will be using minikube or you can check out this repo that has a good overview of minikube, once installed and started (minikube start) that command will download and configure the local environment, you can follow with the following example:\nCreate the chart: helm create hello-world Always use valid DNS names if you are going to have services, otherwise you will have issues later on.\nInspect the contents, as you will notice every resource is just a kubernetes resource with some placeholders and basic logic to get something more reusable: $ cd hello-world charts \u0026lt;--- Dependencies, charts that your chart depends on. Chart.yaml \u0026lt;--- Metadata mostly, defines the version of your chart, etc. templates \u0026lt;--- Here is where the magic happens. values.yaml \u0026lt;--- Default values file (this is used to replace in the templates at runtime) Note: the following link explains the basics of dependencies, your chart can have as many dependencies as you need, the only thing that you need to do is add or install the other charts as dependencies.\nThe file values.yaml by default will look like the following snippet: replicaCount: 1 image: repository: nginx tag: stable pullPolicy: IfNotPresent nameOverride: \u0026#34;\u0026#34; fullnameOverride: \u0026#34;\u0026#34; service: type: ClusterIP port: 80 ingress: enabled: false annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \u0026#34;true\u0026#34; path: / hosts: - chart-example.local tls: [] # - secretName: chart-example-tls # hosts: # - chart-example.local resources: {} nodeSelector: {} tolerations: [] affinity: {}\nThe next step would be to check the templates folder: deployment.yaml \u0026lt;--- Standard kubernetes deployment with go templates variables. _helpers.tpl \u0026lt;--- This file defines some common variables. ingress.yaml \u0026lt;--- Ingress route, etc. NOTES.txt \u0026lt;--- Once deployed this file will display the details of our deployment, usually login data, how to connect, etc. service.yaml \u0026lt;--- The service that we will use internally and/or via ingress to reach our deployed service. Go templates basics, if you need a refresher or a crash course in go templates, also always be sure to check Helm\u0026rsquo;s own documentation and also some tips and tricks.\nLet\u0026rsquo;s check the deployment file: apiVersion: apps/v1beta2 kind: Deployment metadata: name: {{ include \u0026#34;hello-world.fullname\u0026#34; . }} labels: app.kubernetes.io/name: {{ include \u0026#34;hello-world.name\u0026#34; . }} helm.sh/chart: {{ include \u0026#34;hello-world.chart\u0026#34; . }} app.kubernetes.io/instance: {{ .Release.Name }} app.kubernetes.io/managed-by: {{ .Release.Service }} spec: replicas: {{ .Values.replicaCount }} selector: matchLabels: app.kubernetes.io/name: {{ include \u0026#34;hello-world.name\u0026#34; . }} app.kubernetes.io/instance: {{ .Release.Name }} template: metadata: labels: app.kubernetes.io/name: {{ include \u0026#34;hello-world.name\u0026#34; . }} app.kubernetes.io/instance: {{ .Release.Name }} spec: containers: - name: {{ .Chart.Name }} image: \u0026#34;{{ .Values.image.repository }}:{{ .Values.image.tag }}\u0026#34; imagePullPolicy: {{ .Values.image.pullPolicy }} ports: - name: http containerPort: 80 protocol: TCP livenessProbe: httpGet: path: / port: http readinessProbe: httpGet: path: / port: http resources: {{ toYaml .Values.resources | indent 12 }} {{- with .Values.nodeSelector }} nodeSelector: {{ toYaml . | indent 8 }} {{- end }} {{- with .Values.affinity }} affinity: {{ toYaml . | indent 8 }} {{- end }} {{- with .Values.tolerations }} tolerations: {{ toYaml . | indent 8 }} {{- end }} As you can see everything will get replaced by what you define in the values.yaml file and everything is under .Values unless you define a local variable or some other variable using helpers for example.\nLet\u0026rsquo;s check the service file: apiVersion: v1 kind: Service metadata: name: {{ include \u0026#34;hello-world.fullname\u0026#34; . }} labels: app.kubernetes.io/name: {{ include \u0026#34;hello-world.name\u0026#34; . }} helm.sh/chart: {{ include \u0026#34;hello-world.chart\u0026#34; . }} app.kubernetes.io/instance: {{ .Release.Name }} app.kubernetes.io/managed-by: {{ .Release.Service }} spec: type: {{ .Values.service.type }} ports: - port: {{ .Values.service.port }} targetPort: http protocol: TCP name: http selector: app.kubernetes.io/name: {{ include \u0026#34;hello-world.name\u0026#34; . }} app.kubernetes.io/instance: {{ .Release.Name }}\nLet\u0026rsquo;s check the ingress file: {{- if .Values.ingress.enabled -}} {{- $fullName := include \u0026#34;hello-world.fullname\u0026#34; . -}} {{- $ingressPath := .Values.ingress.path -}} apiVersion: extensions/v1beta1 kind: Ingress metadata: name: {{ $fullName }} labels: app.kubernetes.io/name: {{ include \u0026#34;hello-world.name\u0026#34; . }} helm.sh/chart: {{ include \u0026#34;hello-world.chart\u0026#34; . }} app.kubernetes.io/instance: {{ .Release.Name }} app.kubernetes.io/managed-by: {{ .Release.Service }} {{- with .Values.ingress.annotations }} annotations: {{ toYaml . | indent 4 }} {{- end }} spec: {{- if .Values.ingress.tls }} tls: {{- range .Values.ingress.tls }} - hosts: {{- range .hosts }} - {{ . | quote }} {{- end }} secretName: {{ .secretName }} {{- end }} {{- end }} rules: {{- range .Values.ingress.hosts }} - host: {{ . | quote }} http: paths: - path: {{ $ingressPath }} backend: serviceName: {{ $fullName }} servicePort: http {{- end }} {{- end }} The ingress file is one of the most interesting ones in my humble opinion because it has a if else example and also local variables ($fullName for example), also iterates over a possible slice of dns record names (hosts), and the same if you have certs for them (a good way to get let\u0026rsquo;s encrypt certificates automatically is using cert-manager, in the next post I will expand on this example adding a basic web app with mysql and ssl/tls).\nAfter checking that everything is up to our needs the only thing missing is to finally deploy it to kubernetes (But first let\u0026rsquo;s install tiller): $ helm init $HELM_HOME has been configured at /home/gabriel/.helm. Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster. Please note: by default, Tiller is deployed with an insecure \u0026#39;allow unauthenticated users\u0026#39; policy. To prevent this, run `helm init` with the --tiller-tls-verify flag. For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation Happy Helming! Note that many of the complains that Helm receives are because of the admin-y capabilities that Tiller has. A good note on the security issues that Tiller can suffer and some possible mitigation alternatives can be found on the Bitnami page, this mostly applies to multi-tenant clusters. And also be sure to check Securing Helm\nDeploy our chart: $ helm install --name my-nginx -f values.yaml . NAME: my-nginx LAST DEPLOYED: Sun Dec 23 00:30:11 2018 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1/Service NAME AGE my-nginx-hello-world 0s ==\u0026gt; v1beta2/Deployment my-nginx-hello-world 0s ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE my-nginx-hello-world-6f948db8d5-s76zl 0/1 Pending 0 0s NOTES: 1. Get the application URL by running these commands: export POD_NAME=$(kubectl get pods --namespace default -l \u0026#34;app.kubernetes.io/name=hello-world,app.kubernetes.io/instance=my-nginx\u0026#34; -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) echo \u0026#34;Visit http://127.0.0.1:8080 to use your application\u0026#34; kubectl port-forward $POD_NAME 8080:80 Our deployment was successful and we can see that our pod is waiting to be scheduled.\nLet\u0026rsquo;s check that our service is there: $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 1h my-nginx-hello-world ClusterIP 10.111.222.70 \u0026lt;none\u0026gt; 80/TCP 5m\nAnd now we can test that everything is okay by running another pod in interactive mode, for example: $ kubectl run -i --tty alpine --image=alpine -- sh If you don\u0026#39;t see a command prompt, try pressing enter. / # apk add curl fetch http://dl-cdn.alpinelinux.org/alpine/v3.8/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.8/community/x86_64/APKINDEX.tar.gz (1/5) Installing ca-certificates (20171114-r3) (2/5) Installing nghttp2-libs (1.32.0-r0) (3/5) Installing libssh2 (1.8.0-r3) (4/5) Installing libcurl (7.61.1-r1) (5/5) Installing curl (7.61.1-r1) Executing busybox-1.28.4-r2.trigger Executing ca-certificates-20171114-r3.trigger OK: 6 MiB in 18 packages / # curl -v my-nginx-hello-world * Rebuilt URL to: my-nginx-hello-world/ * Trying 10.111.222.70... * TCP_NODELAY set * Connected to my-nginx-hello-world (10.111.222.70) port 80 (#0) \u0026gt; GET / HTTP/1.1 \u0026gt; Host: my-nginx-hello-world \u0026gt; User-Agent: curl/7.61.1 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: nginx/1.14.2 \u0026lt; Date: Sun, 23 Dec 2018 03:45:31 GMT \u0026lt; Content-Type: text/html \u0026lt; Content-Length: 612 \u0026lt; Last-Modified: Tue, 04 Dec 2018 14:44:49 GMT \u0026lt; Connection: keep-alive \u0026lt; ETag: \u0026#34;5c0692e1-264\u0026#34; \u0026lt; Accept-Ranges: bytes \u0026lt; \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; * Connection #0 to host my-nginx-hello-world left intact And voila we see our nginx deployed there and accessible via service name to our other pods (this is fantastic for microservices).\nOur current deployment can be checked like this: $ helm ls NAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE my-nginx 1 Sun Dec 23 00:30:11 2018 DEPLOYED hello-world-0.1.0 1.0 default\nThe last example would be to upgrade our deployment, lets change the tag in the values.yaml file from stable to mainline and update also the metadata file (Chart.yaml) to let Helm know that this is a new version of our chart. $ helm upgrade my-nginx . -f values.yaml Release \u0026#34;my-nginx\u0026#34; has been upgraded. Happy Helming! LAST DEPLOYED: Sun Dec 23 00:55:22 2018 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE my-nginx-hello-world-6f948db8d5-s76zl 1/1 Running 0 25m my-nginx-hello-world-c5cdcc95c-shgc6 0/1 ContainerCreating 0 0s ==\u0026gt; v1/Service NAME AGE my-nginx-hello-world 25m ==\u0026gt; v1beta2/Deployment my-nginx-hello-world 25m NOTES: 1. Get the application URL by running these commands: export POD_NAME=$(kubectl get pods --namespace default -l \u0026#34;app.kubernetes.io/name=hello-world,app.kubernetes.io/instance=my-nginx\u0026#34; -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) echo \u0026#34;Visit http://127.0.0.1:8080 to use your application\u0026#34; kubectl port-forward $POD_NAME 8080:80 Note that I always specify the -f values.yaml just for explicitness.\nIt seems that our upgrade went well, let\u0026rsquo;s see what Helm sees $ helm ls NAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE my-nginx 2 Sun Dec 23 00:55:22 2018 DEPLOYED hello-world-0.1.1 1.0 default\nBut before we go let\u0026rsquo;s validate that it did deployed the nginx version that we wanted to have: $ kubectl exec my-nginx-hello-world-c5cdcc95c-shgc6 -- /usr/sbin/nginx -v nginx version: nginx/1.15.7 At the moment of this writing mainline is 1.15.7, we could rollback to the previous version by doing: $ helm rollback my-nginx 1 Rollback was a success! Happy Helming! Basically this command needs a deployment name my-nginx and the revision number to rollback to in this case 1.\nLet\u0026rsquo;s check the versions again: $ kubectl exec my-nginx-hello-world-6f948db8d5-bsml2 -- /usr/sbin/nginx -v nginx version: nginx/1.14.2\nLet\u0026rsquo;s clean up: $ helm del --purge my-nginx release \u0026#34;my-nginx\u0026#34; deleted\nIf you need to see what will be sent to the kubernetes API then you can use the following command (sometimes it\u0026rsquo;s really useful for debugging or to inject a sidecar using pipes): $ helm template . -name my-nginx -f values.yaml # Source: hello-world/templates/service.yaml apiVersion: v1 kind: Service metadata: name: ame-hello-world\nAnd that folks is all I have for now, be sure to check own Helm Documentation and helm help to know more about what helm can do to help you deploy your applications to any kubernetes cluster.\nDon\u0026rsquo;t Repeat Yourself DRY is a good design goal and part of the art of a good template is knowing when to add a new template and when to update an existing one. While you\u0026rsquo;re figuring that out, accept that you\u0026rsquo;ll be doing some refactoring. Helm and go makes that easy and fast.\nUpcoming topics The following posts will be about package managers, development deployment tools, etc. It\u0026rsquo;s hard to put all the tools in a category, but they are trying to solve similar problems in different ways, and we will be exploring the ones that seem more promising to me, if you would like me to cover any other tool/project/whatever, just send me a message :)\n Expand on helm, search and install community charts. Getting started with Ksonnet and friends Getting started with Skaffold. Getting started with Gitkube.  Errata If you spot any error or have any suggestion, please send me a message so it gets fixed.\nAlso you can check the source code and changes in the generated code and the sources here\n","id":33,"section":"blog","summary":"Introduction This tutorial will show you how to create a simple chart and also how to deploy it to kubernetes using Helm, in the examples I will be using minikube or you can check out this repo that has a good overview of minikube, once installed and started (minikube start) that command will download and configure the local environment, you can follow with the following example:\nCreate the chart: helm create hello-world Always use valid DNS names if you are going to have services, otherwise you will have issues later on.","tags":["helm","deployment-tools","kubernetes"],"title":"Getting started with Helm","uri":"https://techsquad.rocks/blog/getting_started_with_helm/","year":"2018"}],"tags":[{"title":"aws","uri":"https://techsquad.rocks/tags/aws/"},{"title":"cognito","uri":"https://techsquad.rocks/tags/cognito/"},{"title":"continuous-delivery","uri":"https://techsquad.rocks/tags/continuous-delivery/"},{"title":"continuous-integration","uri":"https://techsquad.rocks/tags/continuous-integration/"},{"title":"deployment-tools","uri":"https://techsquad.rocks/tags/deployment-tools/"},{"title":"development","uri":"https://techsquad.rocks/tags/development/"},{"title":"docker","uri":"https://techsquad.rocks/tags/docker/"},{"title":"elm","uri":"https://techsquad.rocks/tags/elm/"},{"title":"gcp","uri":"https://techsquad.rocks/tags/gcp/"},{"title":"git","uri":"https://techsquad.rocks/tags/git/"},{"title":"gitkube","uri":"https://techsquad.rocks/tags/gitkube/"},{"title":"gitlab","uri":"https://techsquad.rocks/tags/gitlab/"},{"title":"go","uri":"https://techsquad.rocks/tags/go/"},{"title":"golang","uri":"https://techsquad.rocks/tags/golang/"},{"title":"grpc","uri":"https://techsquad.rocks/tags/grpc/"},{"title":"helm","uri":"https://techsquad.rocks/tags/helm/"},{"title":"istio","uri":"https://techsquad.rocks/tags/istio/"},{"title":"jsonnet","uri":"https://techsquad.rocks/tags/jsonnet/"},{"title":"kind","uri":"https://techsquad.rocks/tags/kind/"},{"title":"kops","uri":"https://techsquad.rocks/tags/kops/"},{"title":"ksonnet","uri":"https://techsquad.rocks/tags/ksonnet/"},{"title":"kubebuilder","uri":"https://techsquad.rocks/tags/kubebuilder/"},{"title":"kubernetes","uri":"https://techsquad.rocks/tags/kubernetes/"},{"title":"kustomize","uri":"https://techsquad.rocks/tags/kustomize/"},{"title":"lambda","uri":"https://techsquad.rocks/tags/lambda/"},{"title":"linux","uri":"https://techsquad.rocks/tags/linux/"},{"title":"networking","uri":"https://techsquad.rocks/tags/networking/"},{"title":"openssh","uri":"https://techsquad.rocks/tags/openssh/"},{"title":"routing","uri":"https://techsquad.rocks/tags/routing/"},{"title":"security","uri":"https://techsquad.rocks/tags/security/"},{"title":"serverless","uri":"https://techsquad.rocks/tags/serverless/"},{"title":"service-mesh","uri":"https://techsquad.rocks/tags/service-mesh/"},{"title":"skaffold","uri":"https://techsquad.rocks/tags/skaffold/"},{"title":"slack","uri":"https://techsquad.rocks/tags/slack/"},{"title":"terraform","uri":"https://techsquad.rocks/tags/terraform/"},{"title":"tips-and-tricks","uri":"https://techsquad.rocks/tags/tips-and-tricks/"},{"title":"tmux","uri":"https://techsquad.rocks/tags/tmux/"},{"title":"travis-ci","uri":"https://techsquad.rocks/tags/travis-ci/"},{"title":"urxvt","uri":"https://techsquad.rocks/tags/urxvt/"},{"title":"vault","uri":"https://techsquad.rocks/tags/vault/"},{"title":"vim","uri":"https://techsquad.rocks/tags/vim/"}]}